{
 "metadata": {
  "name": "",
  "signature": "sha256:6c7abef425ea61636e48fd5e49c6d446814b264620aec0dcc4e57a3842bd7e35"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Participle: data collection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hebrew participle, active and passive, is the object of study in this notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hebrew participle is seen to function in a nominal way, as well as in a verbal way. \n",
      "Sometimes the participle even functions as a finite verb.\n",
      "\n",
      "There are indications that the usage of participles in classical Hebrew indicates a shift in syntactic patterns, either due to temporal evolution or due to geographical distribution.\n",
      "\n",
      "In order to look for the syntactic variation that is centered around the participle, we first have to collect the data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Desiderata"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(A) We want a list of participles and per occurrence we want to record:\n",
      "\n",
      "* the root consonants of the lexeme\n",
      "* the verbal stem (qal, piel, hiphil, etc)\n",
      "\n",
      "(B) For all lexemes found in this way, we also want to know\n",
      "\n",
      "* how often do they occur as verb and as noun or other part of speech type\n",
      "* for the verb occurrences: how often do the different verbal stems occur, and with what tense: perfect, imperfect, participium active, participium passive, or other\n",
      "\n",
      "(C) We want a list of participle-like forms:\n",
      "\n",
      "* forms not marked as verb, but with the vowel pattern of a particple, eg. QOHEN\n",
      "\n",
      "(D) We want the desired quantities per book"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Starting up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import re\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.6\n",
        "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
        "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('etcbc4', '--', 'participle', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        otype\n",
      "        book chapter verse label\n",
      "        g_word g_lex lex\n",
      "        sp vs vt\n",
      "    ''','''\n",
      "    '''),\n",
      "    \"primary\": False,\n",
      "})\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.29s LOGFILE=/Users/dirk/Dropbox/laf-fabric-output/etcbc4/participle/__log__participle.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.29s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK participle AT 2014-11-12T10-02-48\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Get all verb forms with their relevant properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We collect a list with all verb occurrences and store their lexeme, verbal stem, verbal tense and the current book.\n",
      "\n",
      "At the same time we also collect a list of all non-verb words and store their lexeme and part-of-speech."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cur_book = None\n",
      "cur_chapter = None\n",
      "cur_verse = None\n",
      "gword_occs = []\n",
      "vtenses = set()\n",
      "vstems = set()\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        cur_book = F.book.v(n)\n",
      "    elif otype == 'chapter': cur_chapter = F.chapter.v(n)\n",
      "    elif otype == 'verse': cur_verse = F.verse.v(n)\n",
      "    elif otype == 'word': \n",
      "        psp = F.sp.v(n)\n",
      "        vt = F.vt.v(n)\n",
      "        vtenses.add(vt)\n",
      "        vs = F.vs.v(n)\n",
      "        vstems.add(vs)\n",
      "        glex = F.g_lex.v(n)\n",
      "        lex = F.lex.v(n).rstrip('/[=')\n",
      "        gword = F.g_word.v(n)\n",
      "        gword_occs.append((gword, glex, lex, psp, vs, vt, cur_book, cur_chapter, cur_verse))\n",
      "\n",
      "of = outfile('gwords.csv')\n",
      "for x in gword_occs:\n",
      "    of.write(\"{}\\n\".format('\\t'.join(x)))\n",
      "of.close()\n",
      "\n",
      "vtenses_list = sorted(vtenses)\n",
      "vstems_list = sorted(vstems)\n",
      "gverb_occs = [x for x in gword_occs if x[3] == 'verb']\n",
      "\n",
      "sys.stderr.write('{:>7} words\\n'.format(len(gword_occs)))\n",
      "sys.stderr.write('{:>7} verbs\\n'.format(len(gverb_occs)))\n",
      "sys.stderr.write('{:>7} verbal stems\\n'.format(len(vstems)))\n",
      "sys.stderr.write('{:>7} verbal tenses\\n'.format(len(vtenses)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 426555 words\n",
        "  73659 verbs\n",
        "     23 verbal stems\n",
        "      9 verbal tenses\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Produce derived data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now filter all occurrences where the part of speech is verb.\n",
      "\n",
      "First we generate a file with columns for verbal stem and tense filled with keywords indicating stem and tense.\n",
      "\n",
      "After that a file with columns for each possible value for stem and tense, filled with 1 or 0, indicating whether the occurrence has that stem or tense."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "of = outfile('gverbs.csv')\n",
      "of.write('word\\tglex\\tlex\\tpsp\\tstem\\ttense\\tbook\\tchapter\\tverse\\n')\n",
      "for x in gverb_occs:\n",
      "    of.write(\"{}\\n\".format('\\t'.join(x)))\n",
      "of.close()\n",
      "\n",
      "of = outfile('gverbs_bin.csv')\n",
      "of.write('word\\tglex\\tlex\\t{}\\t{}\\tbook\\tchapter\\tverse\\n'.format('\\t'.join(vstems_list), '\\t'.join(vtenses_list)))\n",
      "for (gword, glex, lex, psp, vs, vt, book, chapter, verse) in gverb_occs:\n",
      "    these_stems = '\\t'.join('1' if vs == x else '0' for x in vstems_list)\n",
      "    these_tenses = '\\t'.join('1' if vt == x else '0' for x in vtenses_list)\n",
      "    of.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n",
      "        gword, glex, lex, these_stems, these_tenses, book, chapter, verse\n",
      "    ))\n",
      "of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we generate verb_profiles: overviews how often verbs and their stems and tenses occur in the bible and the individual books."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zero_profile = tuple(0 for x in vtenses_list) + (0, )\n",
      "verb_profiles = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: list(zero_profile))))\n",
      "for (gword, glex, lex, psp, vs, vt, book, chapter, verse) in gverb_occs:\n",
      "    thiscount = [1 if vt == x else 0 for x in vtenses_list] + [1]\n",
      "    verb_profiles[lex][vs][book] = list(map(sum, zip(verb_profiles[lex][vs][book], thiscount)))\n",
      "sys.stderr.write('{:>7} verb profiles\\n'.format(len(verb_profiles)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "   1381 verb profiles\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "of_bs = outfile('verb_profiles_book_stem.csv')\n",
      "of_s = outfile('verb_profiles_stem.csv')\n",
      "of_b = outfile('verb_profiles_book.csv')\n",
      "of_ = outfile('verb_profiles.csv')\n",
      "of = outfile('verb_profile.csv')\n",
      "\n",
      "of_bs.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "    'lexeme', 'stem', 'book', \n",
      "    '\\t'.join('#' + x for x in vtenses_list),\n",
      "    '#total',\n",
      "    '\\t'.join('%' + x for x in vtenses_list), \n",
      "    '%total',\n",
      "))\n",
      "of_s.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "    'lexeme', 'stem', \n",
      "    '\\t'.join('#' + x for x in vtenses_list), \n",
      "    '#total',\n",
      "    '\\t'.join('%' + x for x in vtenses_list), \n",
      "    '%total',\n",
      "))\n",
      "of_b.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "    'lexeme', 'book', \n",
      "    '\\t'.join('#' + x for x in vtenses_list), \n",
      "    '#total',\n",
      "    '\\t'.join('%' + x for x in vtenses_list), \n",
      "    '%total',\n",
      "))\n",
      "of_.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "    'lexeme', \n",
      "    '\\t'.join('#' + x for x in vtenses_list), \n",
      "    '#total',\n",
      "    '\\t'.join('%' + x for x in vtenses_list), \n",
      "    '%total',\n",
      "))\n",
      "of.write('{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "    '\\t'.join('#' + x for x in vtenses_list), \n",
      "    '#total',\n",
      "    '\\t'.join('%' + x for x in vtenses_list), \n",
      "    '%total',\n",
      "))\n",
      "total = list(zero_profile)\n",
      "for lex in sorted(verb_profiles):\n",
      "    l_total = list(zero_profile)\n",
      "    b_total = collections.defaultdict(lambda: list(zero_profile))\n",
      "    for vs in sorted(verb_profiles[lex]):\n",
      "        s_total = list(zero_profile)\n",
      "        for book in sorted(verb_profiles[lex][vs]):\n",
      "            these_counts = verb_profiles[lex][vs][book]\n",
      "            these_percs = (round(100 * x / these_counts[-1]) for x in these_counts)\n",
      "            s_total = list(map(sum, zip(these_counts, s_total)))\n",
      "            b_total[book] = list(map(sum, zip(these_counts, b_total[book])))\n",
      "            of_bs.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "                lex, vs, book, \n",
      "                '\\t'.join(str(x) for x in these_counts), \n",
      "                '\\t'.join(str(x) for x in these_percs),\n",
      "            ))\n",
      "        s_perc = (round(100 * x / s_total[-1]) for x in s_total)\n",
      "        of_s.write('{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "            lex, vs, \n",
      "            '\\t'.join(str(x) for x in s_total), \n",
      "            '\\t'.join(str(x) for x in s_perc),\n",
      "        ))\n",
      "        l_total = list(map(sum, zip(l_total, s_total)))\n",
      "    for book in sorted(b_total):\n",
      "        b_perc = (round(100 * x / b_total[book][-1]) for x in b_total[book])\n",
      "        of_b.write('{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "            lex, book, \n",
      "            '\\t'.join(str(x) for x in b_total[book]), \n",
      "            '\\t'.join(str(x) for x in b_perc),\n",
      "        ))\n",
      "    l_perc = (round(100 * x / l_total[-1]) for x in l_total)\n",
      "    of_.write('{}\\t{}\\t{}\\n'.format(\n",
      "        lex, \n",
      "        '\\t'.join(str(x) for x in l_total), \n",
      "        '\\t'.join(str(x) for x in l_perc)\n",
      "    ))\n",
      "    total = list(map(sum, zip(total, l_total)))\n",
      "perc = (round(100 * x / total[-1]) for x in total)\n",
      "of.write('{}\\t{}\\n'.format(\n",
      "    '\\t'.join(str(x) for x in total), \n",
      "    '\\t'.join(str(x) for x in perc)\n",
      "))\n",
      "of_bs.close()\n",
      "of_b.close()\n",
      "of_s.close()\n",
      "of_.close()\n",
      "of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fetch other participia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Precise search for most participle like patterns, following the paradigm tables from [Blakley](http://blakleycreative.com/jtb/HebrewGrammar.htm): [strong verb](http://blakleycreative.com/jtb/Text/Paradigms_StrongVerbConsolidation.pdf) and/or Lettinga"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HC = '[>BGDHWZXVJKLMNS<PYQRFCTkmnpy]'\n",
      "strip_dia_pat = re.compile(r'[,0-9]+')\n",
      "\n",
      "ptc_pats_src = {\n",
      "    'qal1': {\n",
      "        'ms': '{B}{HC}O{HC};{HC}{E}',\n",
      "        'mpa': '{B}{HC}O{HC}:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}{HC}O{HC}:{HC}EJ{E}',\n",
      "        'fs1': '{B}{HC}O{HC}E{HC}ET{E}',\n",
      "        'fs2': '{B}{HC}O{HC};{HC}@H{E}',\n",
      "        'fs3': '{B}{HC}O{HC}:{HC}@H{E}',\n",
      "        'fp': '{B}{HC}O{HC}:{HC}OWT{E}',\n",
      "    },\n",
      "    'qal2': {\n",
      "        'ms': '{B}{HC}@{HC};{HC}{E}',\n",
      "        'mpa': '{B}{HC}@{HC}:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}{HC}@{HC}:{HC}EJ{E}',\n",
      "        'fs': '{B}{HC}:{HC};{HC}@H{E}',\n",
      "        'fp': '{B}{HC}:{HC}:{HC}OWT{E}',\n",
      "    },\n",
      "    'qal3': {\n",
      "        'ms': '{B}{HC}@{HC}O{HC}{E}',\n",
      "        'mpa': '{B}{HC}@{HC}:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}{HC}@{HC}:{HC}EJ{E}',\n",
      "        'fs': '{B}{HC}:{HC}O{HC}@H{E}',\n",
      "        'fp': '{B}{HC}:{HC}:{HC}OWT{E}',\n",
      "    },\n",
      "    'nifal': {\n",
      "        'ms': '{B}N.{HC}:{HC}\\.?@{HC}{E}',\n",
      "        'mpa': '{B}N.{HC}:{HC}\\.?:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}N.{HC}:{HC}\\.?:{HC}EJ{E}',\n",
      "        'fs1': '{B}N.{HC}E{HC}\\.?E{HC}ET{E}',\n",
      "        'fs2': '{B}N.{HC}@{HC}\\.?@{HC}H{E}',\n",
      "        'fp': '{B}N.{HC}@{HC}\\.?@{HC}OWT{E}',\n",
      "    },\n",
      "    'piel': {\n",
      "        'ms': '{B}M:{HC}A{HC}\\.;{HC}{E}',\n",
      "        'mpa': '{B}M:{HC}A{HC}\\.:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}M:{HC}A{HC}\\.:{HC}EJ{E}',\n",
      "        'fs1': '{B}M:{HC}A{HC}\\.E{HC}ET{E}',\n",
      "        'fs2': '{B}M:{HC}A{HC}\\.:{HC}H{E}',\n",
      "        'fp': '{B}M:{HC}A{HC}\\.:{HC}OWT{E}',\n",
      "    },\n",
      "    'hifil': {\n",
      "        'ms': '{B}MA{HC}:{HC}\\.?IJ{HC}{E}',\n",
      "        'mpa': '{B}MA{HC}:{HC}\\.?IJ{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}MA{HC}:{HC}\\.?IJ{HC}EJ{E}',\n",
      "        'fs1': '{B}MA{HC}:{HC}\\.E{HC}ET{E}',\n",
      "        'fs2': '{B}MA{HC}:{HC}\\.?IJ{HC}H{E}',\n",
      "        'fp': '{B}MA{HC}:{HC}\\.?IJ{HC}OWT{E}',\n",
      "    },\n",
      "    'hitpael': {\n",
      "        'ms': '{B}MIT:{HC}\\.?A{HC}\\.;{HC}{E}',\n",
      "        'mpa': '{B}MIT:{HC}\\.?A{HC}\\.:{HC}IJ[Mm]{E}',\n",
      "        'mpc': '{B}MIT:{HC}\\.?A{HC}\\.:{HC}EJ{E}',\n",
      "        'fs1': '{B}MIT:{HC}\\.?A{HC}\\.E{HC}ET{E}',\n",
      "        'fs2': '{B}MIT:{HC}\\.?A{HC}\\.:{HC}H{E}',\n",
      "        'fp': '{B}MIT:{HC}\\.?A{HC}\\.:{HC}OWT{E}',\n",
      "    },\n",
      "}\n",
      "\n",
      "ptc_pats = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))\n",
      "\n",
      "for stem in ptc_pats_src:\n",
      "    for pat in ptc_pats_src[stem]:\n",
      "        ptc_pats[stem][pat]['wrd'] = re.compile(ptc_pats_src[stem][pat].format(HC=HC, B='^', E=''))\n",
      "        ptc_pats[stem][pat]['lex'] = re.compile(ptc_pats_src[stem][pat].format(HC=HC, B='', E='$'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "px_occs = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: [])))\n",
      "px_ = collections.defaultdict(lambda: [])\n",
      "\n",
      "for (gword, glex, lex, psp, vs, vt, book, chapter, verse) in gword_occs:\n",
      "    if psp == 'verb' and vt == 'ptca': continue\n",
      "    pspx = psp if psp != 'verb' else vs+'.'+vt\n",
      "    gwordx = strip_dia_pat.sub('', gword)\n",
      "    summarize = psp != 'verb' and psp != 'nmpr'\n",
      "    for stem in ptc_pats:\n",
      "        for pat in ptc_pats[stem]:\n",
      "            if ptc_pats[stem][pat]['wrd'].search(gwordx) != None:\n",
      "                px_occs[stem][pspx]['wrd'].append((gword, glex, lex, psp, vs, vt, book, chapter, verse))\n",
      "                if summarize:\n",
      "                    px_['wrd'].append((gword, glex, lex, psp, vs, vt, book, chapter, verse))                    \n",
      "            if ptc_pats[stem][pat]['lex'].search(glex) != None:\n",
      "                px_occs[stem][pspx]['lex'].append((gword, glex, lex, psp, vs, vt, book, chapter, verse))\n",
      "                if summarize:\n",
      "                    px_['lex'].append((gword, glex, lex, psp, vs, vt, book, chapter, verse))                    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for stem in sorted(ptc_pats):\n",
      "    data = px_occs.get(stem, {})\n",
      "    for psp in data:\n",
      "        for kind in data[psp]:\n",
      "            thisdata = data[psp][kind]\n",
      "            sys.stderr.write('{:>7} {}-participium-like occurrences marked as {} ({}-based)\\n'.format(len(thisdata), stem, psp, kind))\n",
      "            of = outfile('ptcx_occs_{}_{}_{}.csv'.format(stem, psp, kind))\n",
      "            for x in sorted(thisdata, key=lambda y: (y[2], y[1], y[0], y[6], y[7], y[8])):\n",
      "                of.write(\"{}\\n\".format('\\t'.join(x)))\n",
      "            of.close()  \n",
      "\n",
      "for kind in px_:\n",
      "    thisdata = px_[kind]\n",
      "    sys.stderr.write('{:>7} {}-participium-like occurrences marked as {} ({}-based)\\n'.format(\n",
      "        len(thisdata), 'total', 'non-verb, non-proper-noun', kind\n",
      "    ))\n",
      "    of = outfile('ptcx_occs_{}_{}.csv'.format('total', kind))\n",
      "    for x in sorted(thisdata, key=lambda y: (y[2], y[1], y[0], y[6], y[7], y[8])):\n",
      "        of.write(\"{}\\n\".format('\\t'.join(x)))\n",
      "    of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "     41 hifil-participium-like occurrences marked as subs (lex-based)\n",
        "     56 hifil-participium-like occurrences marked as subs (wrd-based)\n",
        "      1 hifil-participium-like occurrences marked as nmpr (lex-based)\n",
        "      8 hifil-participium-like occurrences marked as nmpr (wrd-based)\n",
        "      1 nifal-participium-like occurrences marked as qal.impv (wrd-based)\n",
        "      4 nifal-participium-like occurrences marked as prps (lex-based)\n",
        "      3 nifal-participium-like occurrences marked as hit.impf (wrd-based)\n",
        "      1 nifal-participium-like occurrences marked as nit.perf (wrd-based)\n",
        "     56 nifal-participium-like occurrences marked as qal.impf (wrd-based)\n",
        "      1 nifal-participium-like occurrences marked as piel.perf (wrd-based)\n",
        "      3 nifal-participium-like occurrences marked as qal.infc (wrd-based)\n",
        "    257 nifal-participium-like occurrences marked as nif.perf (wrd-based)\n",
        "     52 nifal-participium-like occurrences marked as qal.perf (wrd-based)\n",
        "      1 nifal-participium-like occurrences marked as subs (lex-based)\n",
        "    150 nifal-participium-like occurrences marked as subs (wrd-based)\n",
        "     32 nifal-participium-like occurrences marked as nmpr (lex-based)\n",
        "     50 nifal-participium-like occurrences marked as nmpr (wrd-based)\n",
        "     12 piel-participium-like occurrences marked as subs (wrd-based)\n",
        "      5 qal1-participium-like occurrences marked as hif.perf (wrd-based)\n",
        "      2 qal1-participium-like occurrences marked as hit.wayq (lex-based)\n",
        "      2 qal1-participium-like occurrences marked as piel.wayq (lex-based)\n",
        "      2 qal1-participium-like occurrences marked as piel.perf (lex-based)\n",
        "      3 qal1-participium-like occurrences marked as piel.perf (wrd-based)\n",
        "      1 qal1-participium-like occurrences marked as hif.infa (wrd-based)\n",
        "      8 qal1-participium-like occurrences marked as piel.impf (lex-based)\n",
        "      3 qal1-participium-like occurrences marked as piel.infc (lex-based)\n",
        "      3 qal1-participium-like occurrences marked as piel.infc (wrd-based)\n",
        "      1 qal1-participium-like occurrences marked as prep (wrd-based)\n",
        "     22 qal1-participium-like occurrences marked as qal.wayq (wrd-based)\n",
        "      1 qal1-participium-like occurrences marked as hif.impv (wrd-based)\n",
        "      1 qal1-participium-like occurrences marked as adjv (lex-based)\n",
        "      2 qal1-participium-like occurrences marked as adjv (wrd-based)\n",
        "      3 qal1-participium-like occurrences marked as qal.impv (wrd-based)\n",
        "      9 qal1-participium-like occurrences marked as qal.impf (wrd-based)\n",
        "     14 qal1-participium-like occurrences marked as hif.impf (wrd-based)\n",
        "     57 qal1-participium-like occurrences marked as subs (lex-based)\n",
        "    162 qal1-participium-like occurrences marked as subs (wrd-based)\n",
        "     44 qal1-participium-like occurrences marked as nmpr (lex-based)\n",
        "     55 qal1-participium-like occurrences marked as nmpr (wrd-based)\n",
        "      1 qal2-participium-like occurrences marked as haf.impf (wrd-based)\n",
        "      7 qal2-participium-like occurrences marked as conj (lex-based)\n",
        "      7 qal2-participium-like occurrences marked as conj (wrd-based)\n",
        "     38 qal2-participium-like occurrences marked as hif.infa (wrd-based)\n",
        "     69 qal2-participium-like occurrences marked as hif.impv (wrd-based)\n",
        "      8 qal2-participium-like occurrences marked as nif.perf (wrd-based)\n",
        "    249 qal2-participium-like occurrences marked as qal.perf (lex-based)\n",
        "    259 qal2-participium-like occurrences marked as qal.perf (wrd-based)\n",
        "     16 qal2-participium-like occurrences marked as piel.wayq (lex-based)\n",
        "    109 qal2-participium-like occurrences marked as prep (lex-based)\n",
        "    113 qal2-participium-like occurrences marked as prep (wrd-based)\n",
        "     34 qal2-participium-like occurrences marked as piel.infc (lex-based)\n",
        "     47 qal2-participium-like occurrences marked as piel.infc (wrd-based)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "      9 qal2-participium-like occurrences marked as nif.wayq (lex-based)\n",
        "    257 qal2-participium-like occurrences marked as adjv (lex-based)\n",
        "    356 qal2-participium-like occurrences marked as adjv (wrd-based)\n",
        "     48 qal2-participium-like occurrences marked as intj (lex-based)\n",
        "     48 qal2-participium-like occurrences marked as intj (wrd-based)\n",
        "     32 qal2-participium-like occurrences marked as qal.impf (wrd-based)\n",
        "      1 qal2-participium-like occurrences marked as piel.impv (lex-based)\n",
        "      4 qal2-participium-like occurrences marked as piel.impv (wrd-based)\n",
        "    273 qal2-participium-like occurrences marked as subs (lex-based)\n",
        "    937 qal2-participium-like occurrences marked as subs (wrd-based)\n",
        "   2781 qal2-participium-like occurrences marked as nmpr (lex-based)\n",
        "    218 qal2-participium-like occurrences marked as nmpr (wrd-based)\n",
        "      2 qal2-participium-like occurrences marked as hif.wayq (wrd-based)\n",
        "      3 qal2-participium-like occurrences marked as qal.impv (wrd-based)\n",
        "    205 qal2-participium-like occurrences marked as advb (lex-based)\n",
        "    205 qal2-participium-like occurrences marked as advb (wrd-based)\n",
        "      1 qal2-participium-like occurrences marked as pael.impf (lex-based)\n",
        "      1 qal2-participium-like occurrences marked as peal.perf (wrd-based)\n",
        "     22 qal2-participium-like occurrences marked as piel.impf (lex-based)\n",
        "     25 qal2-participium-like occurrences marked as hif.infc (wrd-based)\n",
        "      4 qal2-participium-like occurrences marked as nif.impv (lex-based)\n",
        "     15 qal2-participium-like occurrences marked as nif.infc (lex-based)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "      2 qal2-participium-like occurrences marked as nif.infc (wrd-based)\n",
        "     43 qal2-participium-like occurrences marked as hif.impf (wrd-based)\n",
        "    119 qal2-participium-like occurrences marked as nif.impf (lex-based)\n",
        "      3 qal2-participium-like occurrences marked as piel.infa (lex-based)\n",
        "      8 qal2-participium-like occurrences marked as piel.infa (wrd-based)\n",
        "      1 qal3-participium-like occurrences marked as pual.ptcp (wrd-based)\n",
        "    162 qal3-participium-like occurrences marked as qal.infa (lex-based)\n",
        "    364 qal3-participium-like occurrences marked as qal.infa (wrd-based)\n",
        "     37 qal3-participium-like occurrences marked as nif.perf (wrd-based)\n",
        "     12 qal3-participium-like occurrences marked as qal.perf (lex-based)\n",
        "     33 qal3-participium-like occurrences marked as qal.perf (wrd-based)\n",
        "      6 qal3-participium-like occurrences marked as piel.infc (wrd-based)\n",
        "     19 qal3-participium-like occurrences marked as prep (lex-based)\n",
        "     83 qal3-participium-like occurrences marked as prep (wrd-based)\n",
        "      8 qal3-participium-like occurrences marked as qal.wayq (wrd-based)\n",
        "     82 qal3-participium-like occurrences marked as adjv (lex-based)\n",
        "    379 qal3-participium-like occurrences marked as adjv (wrd-based)\n",
        "    405 qal3-participium-like occurrences marked as qal.impf (wrd-based)\n",
        "      4 qal3-participium-like occurrences marked as nif.infc (wrd-based)\n",
        "      5 qal3-participium-like occurrences marked as nif.infa (lex-based)\n",
        "      2 qal3-participium-like occurrences marked as nif.infa (wrd-based)\n",
        "    157 qal3-participium-like occurrences marked as subs (lex-based)\n",
        "   1660 qal3-participium-like occurrences marked as subs (wrd-based)\n",
        "      1 qal3-participium-like occurrences marked as qal.infc (wrd-based)\n",
        "    359 qal3-participium-like occurrences marked as prps (wrd-based)\n",
        "     99 qal3-participium-like occurrences marked as nmpr (lex-based)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    304 qal3-participium-like occurrences marked as nmpr (wrd-based)\n",
        "   1261 total-participium-like occurrences marked as non-verb, non-proper-noun (lex-based)\n",
        "   4530 total-participium-like occurrences marked as non-verb, non-proper-noun (wrd-based)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Check special cases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RoZeN (Judges 5:3, Jes 40:23, Hab 1:10, Ps 2:2, Prov 8:15, Prov 31:4)\n",
      "\n",
      "KoReM (2Kon 25:12, Jes 61:5, Joel 1:11, 2Chr 26:10)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_pats = {\n",
      "    'RZN': re.compile('.*?R.*?Z.*?[Nn]'),\n",
      "    'KRM': re.compile('.*?K.*?R.*?[Mm]'),\n",
      "    'KHN': re.compile('.*?K.*?H.*?N'),\n",
      "    'JGB': re.compile('.*?J.*?G.*?B'),\n",
      "    'NQD': re.compile('.*?N.*?Q.*?D'),\n",
      "}\n",
      "cur_book = None\n",
      "cur_chapter = None\n",
      "cur_verse = None\n",
      "found = []\n",
      "for (gword, glex, lex, psp, vs, vt, book, chapter, verse) in gword_occs:\n",
      "    for pat in search_pats:\n",
      "        if search_pats[pat].search(lex) != None or search_pats[pat].search(glex) != None or search_pats[pat].search(gword) != None:\n",
      "            found.append((gword, glex, lex, psp, vs, vt, book, chapter, verse))\n",
      "\n",
      "of = outfile('specials.csv')\n",
      "for x in sorted(found, key=lambda y: (y[2], y[1], y[0], y[6], y[7], y[8])):\n",
      "    of.write(\"{}\\n\".format('\\t'.join(x)))\n",
      "of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}