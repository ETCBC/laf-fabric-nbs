{
 "metadata": {
  "name": "",
  "signature": "sha256:8089c1b8d8cc65b0809ffa77c6114c1ed4c10b3dbfd8b2ad4be5da8e269cad08"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
      "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Participle: data collection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hebrew participle, active and passive, is the object of study in this notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hebrew participle is seen to function in a nominal way, as well as in a verbal way. \n",
      "Sometimes the participle even functions as a finite verb.\n",
      "\n",
      "There are indications that the usage of participles in classical Hebrew indicates a shift in syntactic patterns, either due to temporal evolution or due to geographical distribution.\n",
      "\n",
      "In order to look for the syntactic variation that is centered around the participle, we first have to collect the data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Desiderata"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(A) We want a list of participles and per occurrence we want to record:\n",
      "\n",
      "* the root consonants of the lexeme\n",
      "* the verbal stem (qal, piel, hiphil, etc)\n",
      "\n",
      "(B) For all lexemes found in this way, we also want to know\n",
      "\n",
      "* how often do they occur as verb and as noun or other part of speech type\n",
      "* for the verb occurrences: how often do the different verbal stems occur, and with what tense: perfect, imperfect, participium active, participium passive, or other\n",
      "\n",
      "(C) We want a list of particple-like forms:\n",
      "\n",
      "* forms not marked as verb, but with the vowel pattern of a particple, eg. QOHEN\n",
      "\n",
      "(D) We want the desired quantities per book"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Starting up"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import re\n",
      "from laf.fabric import LafFabric\n",
      "from etcbc.preprocess import prepare\n",
      "fabric = LafFabric()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s This is LAF-Fabric 4.4.1\n",
        "http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fabric.load('etcbc4', '--', 'participle', {\n",
      "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
      "    \"features\": ('''\n",
      "        otype\n",
      "        book chapter verse label\n",
      "        g_word g_word_utf8 lex_utf8 g_lex\n",
      "        vt vs\n",
      "        sp pdp\n",
      "        prs vbe\n",
      "        g_prs\n",
      "    ''','''\n",
      "    '''),\n",
      "    \"primary\": False,\n",
      "})\n",
      "exec(fabric.localnames.format(var='fabric'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOADING API: please wait ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s INFO: USING DATA COMPILED AT: 2014-07-23T09-31-37\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.98s INFO: DATA LOADED FROM SOURCE etcbc4 AND ANNOX -- FOR TASK participle AT 2014-08-28T10-13-43\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Get all verb forms with their relevant properties"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We collect a list with all verb occurrences and store their lexeme, verbal stem, verbal tense and the current book.\n",
      "\n",
      "At the same time we also collect a list of all non-verb words and store their lexeme and part-of-speech."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cur_book = None\n",
      "cur_chapter = None\n",
      "cur_verse = None\n",
      "books = []\n",
      "lexeme_occs = []\n",
      "glexeme_occs = []\n",
      "lexemes = collections.defaultdict(lambda: set())\n",
      "glexemes = collections.defaultdict(lambda: set())\n",
      "lexemes_sub = collections.defaultdict(lambda: set())\n",
      "glexeme_map = {}\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        cur_book = F.book.v(n)\n",
      "        books.append(cur_book)\n",
      "    elif otype == 'chapter': cur_chapter = F.chapter.v(n)\n",
      "    elif otype == 'verse': cur_verse = F.verse.v(n)\n",
      "    elif otype == 'word': \n",
      "        psp = F.sp.v(n)\n",
      "        lex = F.lex_utf8.v(n).rstrip('/[=')\n",
      "        glex = F.g_lex.v(n)\n",
      "        gword = F.g_word_utf8.v(n)\n",
      "        glexeme_map[glex] = lex\n",
      "        stem = ''\n",
      "        tense = ''\n",
      "        if psp == 'verb':\n",
      "            stem = F.vs.v(n)\n",
      "            tense = F.vt.v(n)\n",
      "        lexeme_occs.append((lex, glex, gword, psp, stem, tense, cur_book))\n",
      "        lexemes[lex].add(psp)\n",
      "        glexemes[glex].add(psp)\n",
      "        ll = len(lex)\n",
      "        for i in range(ll):\n",
      "            for j in range(i, ll):\n",
      "                slex = lex[i:j+1]\n",
      "                if ' ' not in slex: lexemes_sub[slex].add(lex)\n",
      "\n",
      "of = outfile('lexemes.csv')\n",
      "for x in sorted(lexemes):\n",
      "    of.write(\"{}\\t{}\\n\".format(x, '\\t'.join(sorted(lexemes[x]))))\n",
      "of.close()\n",
      "\n",
      "of = outfile('glexemes.csv')\n",
      "for x in sorted(glexemes):\n",
      "    of.write(\"{}\\t{}\\n\".format(x, '\\t'.join(sorted(glexemes[x]))))\n",
      "of.close()\n",
      "\n",
      "of = outfile('lexeme_occs.csv')\n",
      "for item in lexeme_occs:\n",
      "    of.write('{}\\n'.format('\\t'.join(item)))\n",
      "of.close()\n",
      "\n",
      "of = outfile('lexemes_sub.csv')\n",
      "for x in sorted(lexemes_sub):\n",
      "    of.write(\"{}\\t{}\\n\".format(x, '\\t'.join(sorted(lexemes_sub[x]))))\n",
      "of.close()\n",
      "sys.stderr.write('{:>7} lexemes\\n'.format(len(lexemes)))\n",
      "sys.stderr.write('{:>7} glexemes\\n'.format(len(glexemes)))\n",
      "sys.stderr.write('{:>7} lexeme occurrences\\n'.format(len(lexeme_occs)))\n",
      "sys.stderr.write('{:>7} lexeme substrings\\n'.format(len(lexemes_sub)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "   6457 lexemes\n",
        "  21159 glexemes\n",
        " 426555 lexeme occurrences\n",
        "  12934 lexeme substrings\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Produce derived data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A verb profile is the percentage of participle forms (active and passive separately), imperfecta, perfecta and other forms relative the total number of forms of that verb lexeme in that stem. So profiles are sets of 5-tuples, one 5-tuple for each stem in which the verb occurs.\n",
      "Verb profiles can be computed with respect to the whole corpus, or with respect to smaller chunks, such as books.\n",
      "\n",
      "We are going to compute the verb profiles for all verb lexemes relative the individual bible books."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "verb_profiles = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda:[0,0,0,0,0,0])))\n",
      "for (lex, psp, stem, tense, book) in lexeme_occs:\n",
      "    if psp != 'verb': continue\n",
      "    ipf = 1 if tense == 'impf' else 0\n",
      "    pf = 1 if tense == 'perf' else 0\n",
      "    ptca = 1 if tense == 'ptca' else 0\n",
      "    ptcp = 1 if tense =='ptcp' else 0\n",
      "    other = 1 if ipf + pf + ptca + ptcp == 0 else 0\n",
      "    verb_profiles[lex][book][stem] = list(map(sum, zip(verb_profiles[lex][book][stem], [1, ptca, ptcp, ipf, pf, other])))\n",
      "sys.stderr.write('{:>7} verb profiles\\n'.format(len(verb_profiles)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "   1381 verb profiles\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vl_file = outfile('verb_profiles.csv')\n",
      "vl_file_c = outfile('verb_profiles_coarse.csv')\n",
      "verb_summary = []\n",
      "for lex in sorted(verb_profiles):\n",
      "    vl_file.write('{}\\n'.format(lex))\n",
      "    vl_file_c.write('{}\\n'.format(lex))\n",
      "    sdata = [0, 0, 0, 0, 0, 0]\n",
      "    for book in sorted(verb_profiles[lex]):\n",
      "        vl_file.write('\\t{}\\n'.format(book))\n",
      "        cdata = [0, 0, 0, 0, 0, 0]\n",
      "        for stem in sorted(verb_profiles[lex][book]):\n",
      "            data = verb_profiles[lex][book][stem]\n",
      "            cdata = list(map(sum, zip(cdata, data)))\n",
      "            sdata = list(map(sum, zip(sdata, data)))\n",
      "            vl_file.write('\\t\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "                stem,\n",
      "                data[0],\n",
      "                round(data[1] * 100 / data[0]),\n",
      "                round(data[2] * 100 / data[0]),\n",
      "                round(data[3] * 100 / data[0]),\n",
      "                round(data[4] * 100 / data[0]),\n",
      "                round(data[5] * 100 / data[0]),                \n",
      "            ))\n",
      "        vl_file_c.write('\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
      "            book,\n",
      "            data[0],\n",
      "            round(cdata[1] * 100 / cdata[0]),\n",
      "            round(cdata[2] * 100 / cdata[0]),\n",
      "            round(cdata[3] * 100 / cdata[0]),\n",
      "            round(cdata[4] * 100 / cdata[0]),\n",
      "            round(cdata[5] * 100 / cdata[0]),                \n",
      "        ))\n",
      "    verb_summary.append((\n",
      "        lex,\n",
      "        data[0],\n",
      "        round(sdata[1] * 100 / sdata[0]),\n",
      "        round(sdata[2] * 100 / sdata[0]),\n",
      "        round(sdata[3] * 100 / sdata[0]),\n",
      "        round(sdata[4] * 100 / sdata[0]),\n",
      "        round(sdata[5] * 100 / sdata[0]),                \n",
      "    ))\n",
      "vl_file.close()\n",
      "vl_file_c.close()\n",
      "vl_file_s = outfile('verb_profiles_summary.csv')\n",
      "vl_file_s.write('{}\\n'.format('\\t'.join(('lexeme', 'total', '%ptca', '%ptcp', '%ipf', '%pf', '%other'))))\n",
      "for x in sorted(verb_summary, key=lambda x: (-x[1], -x[2], -x[3], x[0])):\n",
      "    vl_file_s.write('{}\\n'.format('\\t'.join(str(z) for z in x)))\n",
      "vl_file_s.close()\n",
      "sys.stderr.write('{:>7} in verb summary\\n'.format(len(verb_summary)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "   1381 in verb summary\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fetch other lexemes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fetch all non-verb lexemes which are also a verb or that has a substring that is a verb.\n",
      "\n",
      "The verb lexemes are precisely the keys of the verb_profiles dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nv_lexemes = collections.defaultdict(lambda: set())\n",
      "for slex in lexemes_sub:\n",
      "    if slex not in lexemes: continue\n",
      "    if 'verb' not in lexemes[slex]: continue\n",
      "    for lex in lexemes_sub[slex]:\n",
      "        if 'verb' in lexemes[lex] and len(lexemes[lex]) == 1: continue\n",
      "        nv_lexemes[lex].add(slex)\n",
      "of = outfile('other_lexemes.csv')\n",
      "for lex in sorted(nv_lexemes):\n",
      "    of.write('{}\\t{}\\n'.format(lex, '\\t'.join(nv_lexemes[lex])))\n",
      "of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternatively: fetch lexemes with a participle like vowel pattern (coarsely)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HC = '>BGDHWZXVJKLMNS<PYQRFCTkmnpy'\n",
      "ptc_pat_sg = re.compile(r'OW?[' + HC + r'];J?')\n",
      "ptc_pat_pl = re.compile(r'OW?[' + HC + r'][:;AEIOU]*[' + HC + r']IJ?[Mm]')\n",
      "\n",
      "p_lexemes = set()\n",
      "for glex in glexemes:\n",
      "    if ptc_pat_sg.search(glex) != None or ptc_pat_pl.search(glex) != None:\n",
      "        psps = glexemes[glex]\n",
      "        if 'verb' in psps and len(psps) == 1: continue\n",
      "        p_lexemes.add(glex)\n",
      "sys.stderr.write('{:>7} in participium like lexemes\\n'.format(len(p_lexemes)))\n",
      "\n",
      "of = outfile('ptc_lexemes.csv')\n",
      "for glex in sorted(p_lexemes):\n",
      "    of.write('{}\\t{}\\t{}\\n'.format(glex, glexeme_map[glex], ','.join(sorted(glexemes[glex]))))\n",
      "of.close()  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    106 in participium like lexemes\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Precise search for most participle like patterns, following the paradigm tables from [Blakley](http://blakleycreative.com/jtb/HebrewGrammar.htm): [strong verb](http://blakleycreative.com/jtb/Text/Paradigms_StrongVerbConsolidation.pdf) and/or Lettinga"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HC = '[>BGDHWZXVJKLMNS<PYQRFCTkmnpy]'\n",
      "ptc_pats = {\n",
      "    'qal1': {\n",
      "        'ms': re.compile(r'{HC}O{HC};{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'{HC}O{HC}:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'{HC}O{HC}:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs1': re.compile(r'{HC}O{HC}E{HC}ET$'.format(HC=HC)),\n",
      "        'fs2': re.compile(r'{HC}O{HC};{HC}@H$'.format(HC=HC)),\n",
      "        'fs3': re.compile(r'{HC}O{HC}:{HC}@H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'{HC}O{HC}:{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'qal2': {\n",
      "        'ms': re.compile(r'{HC}@{HC};{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'{HC}@{HC}:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'{HC}@{HC}:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs': re.compile(r'{HC}:{HC};{HC}@H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'{HC}:{HC}:{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'qal3': {\n",
      "        'ms': re.compile(r'{HC}@{HC}O{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'{HC}@{HC}:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'{HC}@{HC}:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs': re.compile(r'{HC}:{HC}O{HC}@H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'{HC}:{HC}:{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'nifal': {\n",
      "        'ms': re.compile(r'N.{HC}:{HC}\\.?@{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'N.{HC}:{HC}\\.?:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'N.{HC}:{HC}\\.?:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs1': re.compile(r'N.{HC}E{HC}\\.?E{HC}ET$'.format(HC=HC)),\n",
      "        'fs2': re.compile(r'N.{HC}@{HC}\\.?@{HC}H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'N.{HC}@{HC}\\.?@{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'piel': {\n",
      "        'ms': re.compile(r'M:{HC}A{HC}\\.;{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'M:{HC}A{HC}\\.:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'M:{HC}A{HC}\\.:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs1': re.compile(r'M:{HC}A{HC}\\.E{HC}ET$'.format(HC=HC)),\n",
      "        'fs2': re.compile(r'M:{HC}A{HC}\\.:{HC}H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'M:{HC}A{HC}\\.:{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'hifil': {\n",
      "        'ms': re.compile(r'MA{HC}:{HC}\\.?IJ?{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'MA{HC}:{HC}\\.?IJ?{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'MA{HC}:{HC}\\.?IJ?{HC}EJ$'.format(HC=HC)),\n",
      "        'fs1': re.compile(r'MA{HC}:{HC}\\.E{HC}ET$'.format(HC=HC)),\n",
      "        'fs2': re.compile(r'MA{HC}:{HC}\\.?IJ?{HC}H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'MA{HC}:{HC}\\.?IJ?{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "    'hitpael': {\n",
      "        'ms': re.compile(r'MIT:{HC}\\.?A{HC}\\.;{HC}$'.format(HC=HC)),\n",
      "        'mpa': re.compile(r'MIT:{HC}\\.?A{HC}\\.:{HC}IJ[Mm]$'.format(HC=HC)),\n",
      "        'mpc': re.compile(r'MIT:{HC}\\.?A{HC}\\.:{HC}EJ$'.format(HC=HC)),\n",
      "        'fs1': re.compile(r'MIT:{HC}\\.?A{HC}\\.E{HC}ET$'.format(HC=HC)),\n",
      "        'fs2': re.compile(r'MIT:{HC}\\.?A{HC}\\.:{HC}H$'.format(HC=HC)),\n",
      "        'fp': re.compile(r'MIT:{HC}\\.?A{HC}\\.:{HC}OWT$'.format(HC=HC)),\n",
      "    },\n",
      "}\n",
      "px_lexemes = collections.defaultdict(lambda: set())\n",
      "for glex in glexemes:\n",
      "    for stem in ptc_pats:\n",
      "        for pat in ptc_pats[stem]:\n",
      "            if ptc_pats[stem][pat].search(glex) != None:\n",
      "                psps = glexemes[glex]\n",
      "                if 'verb' in psps and len(psps) == 1: continue\n",
      "                if 'nmpr' in psps and len(psps) == 1: continue\n",
      "                if 'nmpr' in psps and 'verb' in psps and len(psps) == 2: continue\n",
      "                px_lexemes[stem].add(glex)\n",
      "\n",
      "for stem in sorted(ptc_pats):\n",
      "    data = px_lexemes.get(stem, {})\n",
      "    sys.stderr.write('{:>7} in {}-participium-like lexemes\\n'.format(len(data), stem))\n",
      "    of = outfile('ptcx_lexemes_{}.csv'.format(stem))\n",
      "    for glex in sorted(px_lexemes[stem]):\n",
      "        of.write('{}\\t{}\\t{}\\n'.format(glex, glexeme_map[glex], ','.join(sorted(glexemes[glex]))))\n",
      "    of.close()  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "      7 in hifil-participium-like lexemes\n",
        "      0 in hitpael-participium-like lexemes\n",
        "      3 in nifal-participium-like lexemes\n",
        "      0 in piel-participium-like lexemes\n",
        "     17 in qal1-participium-like lexemes\n",
        "     65 in qal2-participium-like lexemes\n",
        "     33 in qal3-participium-like lexemes\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "RoZeN (Judges 5:3, Jes 40:23, Hab 1:10, Ps 2:2, Prov 8:15, Prov 31:4)\n",
      "KoReM (2Kon 25:12, Jes 61:5, Joel 1:11, 2Chr 26:10)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_pats = {\n",
      "    'RZN': re.compile('.*?R.*?Z.*?[Nn]'),\n",
      "    'KRM': re.compile('.*?K.*?R.*?[Mm]'),\n",
      "}\n",
      "cur_book = None\n",
      "cur_chapter = None\n",
      "cur_verse = None\n",
      "found = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(lambda: [])))\n",
      "for n in NN():\n",
      "    otype = F.otype.v(n)\n",
      "    if otype == 'book':\n",
      "        cur_book = F.book.v(n)\n",
      "    elif otype == 'chapter': cur_chapter = F.chapter.v(n)\n",
      "    elif otype == 'verse': cur_verse = F.verse.v(n)\n",
      "    elif otype == 'word':\n",
      "        glex = F.g_lex.v(n)\n",
      "        gword = F.g_word.v(n)\n",
      "        for pat in search_pats:\n",
      "            if search_pats[pat].search(glex) != None or search_pats[pat].search(gword) != None:\n",
      "                found[pat][glex][gword].append((cur_book, cur_chapter, cur_verse))\n",
      "\n",
      "of = outfile('specials.csv')\n",
      "for pat in sorted(search_pats):\n",
      "    sys.stderr.write('{}: {} found\\n'.format(pat, len(found[pat]) if pat in found else 0))\n",
      "    if pat in found:\n",
      "        for glex in sorted(found[pat]):\n",
      "            for gword in sorted(found[pat][glex]):\n",
      "                of.write('{}\\t{:<10}\\t{:<20}\\t{}\\n'.format(pat, glex, gword, ', '.join('{} {}:{}'.format(*x) for x in found[pat][glex][gword])))\n",
      "of.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "KRM: 98 found\n",
        "RZN: 13 found\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}