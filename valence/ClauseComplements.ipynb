{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook collects the complements to the verb in each clause.\n",
    "\n",
    "The purpose is to create a spreadsheet in which each row corresponds to a clause.\n",
    "The first column is filled with the lexeme of the verb phrase of the clause, the next columns correspond to the various complements of the verb phrase in that clause.\n",
    "\n",
    "<img align=\"right\"src=\"images/Complements.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firing up the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.4.6\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.lib import Transcription, monad_set\n",
    "from etcbc.trees import Tree\n",
    "\n",
    "fabric = LafFabric()\n",
    "tr = Transcription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.01s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  7.89s INFO: DATA LOADED FROM SOURCE etcbc4s AND ANNOX -- FOR TASK clausecomplements AT 2015-02-18T13-14-44\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('etcbc4b', '--', 'clausecomplements', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads\n",
    "        book chapter verse\n",
    "        det sp lex function\n",
    "        g_cons g_word trailer_utf8\n",
    "    ''','''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Construction\n",
    "\n",
    "We construct the trees for each clause, and we only put clause, phrase and word nodes in the trees.\n",
    "After the construction, each clause has phrase children, which are all phrases that are contained (as monad set) in the clause, and likewise every phrase has word children, which are all words contained in that phrase.\n",
    "\n",
    "We do not have phrases inside phrases. All phrases occur at the same level, but they are ordered by the canonical ordering: the phrase that contains the first monad that is not contained in the other phrase comes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API with EXTRAs: please wait ... \n",
      "  0.02s INFO: USING DATA COMPILED AT: 2014-10-23T15-58-52\n",
      "  1.37s INFO: DATA LOADED FROM SOURCE etcbc4s AND ANNOX -- FOR TASK clausecomplements AT 2015-02-18T13-15-18\n",
      "  0.00s Start computing parent and children relations for objects of type clause, phrase, word\n",
      "  1.32s 100000 nodes\n",
      "  2.71s 200000 nodes\n",
      "  4.02s 300000 nodes\n",
      "  5.42s 400000 nodes\n",
      "  6.71s 500000 nodes\n",
      "  7.99s 600000 nodes\n",
      "  9.41s 700000 nodes\n",
      "    10s 768200 nodes: 680207 have parents and 341633 have children\n",
      "    10s Ready for processing\n"
     ]
    }
   ],
   "source": [
    "tree_types = ('clause', 'phrase', 'word')\n",
    "(root_type, leaf_type, clause_type) = (tree_types[0], tree_types[-1], 'clause')\n",
    "\n",
    "tree = Tree(API, otypes=tree_types, \n",
    "    clause_type=clause_type,\n",
    "    ccr_feature=None,\n",
    "    pt_feature=None,\n",
    "    pos_feature='sp',\n",
    "    mother_feature =None,\n",
    ")\n",
    "results = tree.relations()\n",
    "parent = results['eparent']\n",
    "children = results['echildren']\n",
    "msg(\"Ready for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a passage index for the clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    15s Making passage index ...\n",
      "    17s Passage index created for 87993 clauses\n"
     ]
    }
   ],
   "source": [
    "msg(\"Making passage index ...\")\n",
    "cur_book = None\n",
    "cur_chapter = None\n",
    "cur_verse = None\n",
    "clause_passage = {}\n",
    "for n in NN():\n",
    "    otype = F.otype.v(n)\n",
    "    if otype == 'book': cur_book = F.book.v(n)\n",
    "    elif otype == 'chapter': cur_chapter = F.chapter.v(n)\n",
    "    elif otype == 'verse': cur_verse = F.verse.v(n)\n",
    "    elif otype == 'clause': clause_passage[n] = (cur_book, cur_chapter, cur_verse)\n",
    "nclauses = len(clause_passage)\n",
    "clause_order = sorted(clause_passage)\n",
    "msg(\"Passage index created for {} clauses\".format(nclauses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an index of the transcriptions of clauses and phrases\n",
    "\n",
    "We store the transcribed texts for words, phrases and clauses.\n",
    "In phrases, we separate the first word from the rest by means of an % instead of a space.\n",
    "This makes it easier to implement some logic based on the first word of a phrase.\n",
    "We do this in the consonantal transcriptions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    23s Making transcription index ...\n",
      "    28s Transcription index created for 768200 nodes\n"
     ]
    }
   ],
   "source": [
    "msg(\"Making transcription index ...\")\n",
    "node_transcr = {}\n",
    "node_transcr_c = {}\n",
    "\n",
    "for clause in clause_order:\n",
    "    clause_transcr = ''\n",
    "    clause_transcr_c = ''\n",
    "    for phrase in children[clause]:\n",
    "        phrase_transcr = ''\n",
    "        phrase_transcr_c = ''\n",
    "        phword_sep = '%'\n",
    "        for word in children[phrase]:\n",
    "            word_transcr = F.g_word.v(word) + tr.from_hebrew(F.trailer_utf8.v(word)).replace('_',' ').replace('\\n',' ')\n",
    "            word_transcr_c = F.g_cons.v(word)\n",
    "            node_transcr[word] = word_transcr.rstrip(' ')\n",
    "            node_transcr_c[word] = word_transcr_c\n",
    "            phrase_transcr += word_transcr\n",
    "            phrase_transcr_c += word_transcr_c + phword_sep\n",
    "            phword_sep = ' '\n",
    "            clause_transcr += word_transcr\n",
    "            clause_transcr_c += word_transcr_c + ' '\n",
    "        node_transcr[phrase] = phrase_transcr.rstrip(' ')\n",
    "        node_transcr_c[phrase] = phrase_transcr_c.rstrip(' %')\n",
    "    node_transcr[clause] = clause_transcr.rstrip(' ')\n",
    "    node_transcr_c[clause] = clause_transcr_c.rstrip(' ')\n",
    "msg(\"Transcription index created for {} nodes\".format(len(node_transcr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore phrase features\n",
    "\n",
    "We are interested in the phrase function and the phrase determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    33s Exploring phrase features ...\n",
      "    36s End exploring phrase features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adju                   9700 x\n",
      "Cmpl                  29602 x\n",
      "Conj                  46168 x\n",
      "EPPr                      4 x\n",
      "ExsS                     14 x\n",
      "Exst                    144 x\n",
      "Frnt                   1069 x\n",
      "IntS                    250 x\n",
      "Intj                   1625 x\n",
      "Loca                   2811 x\n",
      "ModS                     36 x\n",
      "Modi                   3823 x\n",
      "NCoS                    101 x\n",
      "NCop                    605 x\n",
      "Nega                   6054 x\n",
      "Objc                  22211 x\n",
      "PrAd                    138 x\n",
      "PrcS                      8 x\n",
      "PreC                  18779 x\n",
      "PreO                   5509 x\n",
      "PreS                    780 x\n",
      "Pred                  57055 x\n",
      "PtcO                    162 x\n",
      "Ques                   1268 x\n",
      "Rela                   6379 x\n",
      "Subj                  31031 x\n",
      "Supp                    298 x\n",
      "Time                   3787 x\n",
      "Unkn                   2639 x\n",
      "Voct                   1590 x\n"
     ]
    }
   ],
   "source": [
    "msg(\"Exploring phrase features ...\")\n",
    "phrase_functions = collections.defaultdict(lambda: 0)\n",
    "phrase_det = collections.defaultdict(lambda: set())\n",
    "for p in NN():\n",
    "    otype = F.otype.v(p)\n",
    "    if otype == 'phrase':\n",
    "        phrase_functions[F.function.v(p)] += 1\n",
    "        phrase_det[node_transcr_c[p]].add('d' if F.det.v(p) == 'det' else 'u' if F.det.v(p) == 'und' else 'n')\n",
    "\n",
    "for value in sorted(phrase_functions):\n",
    "    print(\"{:<20} {:>6d} x\".format(value, phrase_functions[value]))\n",
    "\n",
    "phrase_det_code = {}\n",
    "for (p, dets) in phrase_det.items():\n",
    "    d = len(dets)\n",
    "    phrase_det_code[p] = '{};\"{}\"'.format(d, ''.join(sorted(dets)))\n",
    "of = outfile('phrase_det.csv')\n",
    "for p in sorted(phrase_det_code):\n",
    "    dets = phrase_det_code[p]\n",
    "    of.write('\"{}\";{}\\n'.format(p, dets))\n",
    "of.close()\n",
    "msg(\"End exploring phrase features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in phrases with a particular function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pick up the complements(s)\n",
    "\n",
    "Also collect the lexemes of the verbs in the verb phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    44s Picking up complements ...\n",
      "    45s 87993 clauses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71638 clauses with  1 predicate\n",
      " 2098 clauses with  2 predicates\n",
      "32212 clauses with  1 subject\n",
      "25789 clauses with  1 object\n",
      " 1045 clauses with  2 objects\n",
      "    1 clauses with  3 objects\n",
      "27974 clauses with  1 complement\n",
      "  799 clauses with  2 complements\n",
      "   10 clauses with  3 complements\n",
      "12015 clauses with  1 adjunct\n",
      "  850 clauses with  2 adjuncts\n",
      "   69 clauses with  3 adjuncts\n",
      "    8 clauses with  4 adjuncts\n",
      "    1 clauses with  5 adjuncts\n",
      " 5776 clauses with  1 situation\n",
      "  336 clauses with  2 situations\n",
      "   43 clauses with  3 situations\n",
      "    4 clauses with  4 situations\n",
      "    1 clauses with  5 situations\n",
      "    8 clauses with  1 rest\n",
      "51815 clauses with  1 unclassified\n",
      " 6573 clauses with  2 unclassifieds\n",
      "  498 clauses with  3 unclassifieds\n",
      "  140 clauses with  4 unclassifieds\n",
      "   53 clauses with  5 unclassifieds\n",
      "   17 clauses with  6 unclassifieds\n",
      "   16 clauses with  7 unclassifieds\n",
      "    4 clauses with  8 unclassifieds\n",
      "    1 clauses with  9 unclassifieds\n",
      "    1 clauses with 10 unclassifieds\n",
      "There are at most 2 predicates in a clause\n",
      "There are at most 1 subjects in a clause\n",
      "There are at most 3 objects in a clause\n",
      "There are at most 3 complements in a clause\n",
      "There are at most 5 adjuncts in a clause\n",
      "There are at most 5 situations in a clause\n",
      "There are at most 1 rests in a clause\n",
      "There are at most 10 unclassifieds in a clause\n"
     ]
    }
   ],
   "source": [
    "msg(\"Picking up complements ...\")\n",
    "comptype_spec = '''\n",
    "predicate    : Pred PreO PreS PtcO PreC\n",
    "subject      : Subj PreS ExsS IntS ModS NCoS\n",
    "object       : Objc PreO PtcO\n",
    "complement   : Cmpl\n",
    "adjunct      : Adju PrAd Supp Modi\n",
    "situation    : Loca Time\n",
    "rest         : PrcS\n",
    "unclassified : *\n",
    "'''\n",
    "\n",
    "comptype_order = []\n",
    "comptype_inv = {}\n",
    "comptype = {}\n",
    "\n",
    "lines = comptype_spec.split('\\n')\n",
    "l = 0\n",
    "for line in lines:\n",
    "    if line.strip() == '': continue\n",
    "    (ctstr, funcstr) = line.split(':')\n",
    "    ctype = ctstr.strip()\n",
    "    functions = funcstr.strip().split()\n",
    "    comptype_order.append(ctype)\n",
    "    comptype_inv[ctype] = l\n",
    "    for func in functions:\n",
    "        comptype[func] = l\n",
    "    l += 1\n",
    "\n",
    "vp_lexemes = {}\n",
    "clause_phrases = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "\n",
    "for clause in clause_order:\n",
    "    phrases = children[clause]\n",
    "    verbs = []\n",
    "    for p in phrases:\n",
    "        cpt = comptype.get(F.function.v(p), comptype_inv['unclassified'])\n",
    "        clause_phrases[cpt][clause].append(p)\n",
    "        if cpt == comptype_inv['predicate']:\n",
    "            for word in children[p]:\n",
    "                if F.sp.v(word) == 'verb':\n",
    "                    verbs.append(F.lex.v(word).rstrip('[/='))\n",
    "    vp_lexemes[clause] = ' '.join(verbs)\n",
    "\n",
    "\n",
    "phrase_distribution = collections.defaultdict(lambda: collections.defaultdict(lambda:0))\n",
    "for ctp in sorted(clause_phrases):\n",
    "    for clause in clause_phrases[ctp]:\n",
    "        phrase_distribution[ctp][len(clause_phrases[ctp][clause])] += 1\n",
    "    \n",
    "msg(\"{} clauses\".format(\n",
    "    nclauses, \n",
    "))\n",
    "\n",
    "maxnphrases = {}\n",
    "for ctp in sorted(phrase_distribution):\n",
    "    for n in sorted(phrase_distribution[ctp]):\n",
    "        maxnphrases[ctp] = n\n",
    "        print(\"{:>5} clauses with {:>2} {}{}\".format(\n",
    "            phrase_distribution[ctp][n], \n",
    "            n, \n",
    "            comptype_order[ctp], \n",
    "            's' if n != 1 else '',\n",
    "        ))\n",
    "for ctp in sorted(maxnphrases):\n",
    "    print(\"There are at most {} {}s in a clause\".format(maxnphrases[ctp], comptype_order[ctp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the data\n",
    "\n",
    "We output everything as a tab separated file.\n",
    "If a complement type occurs multiple times in a clause, they will occupy separate columns.\n",
    "We know the maximum number of times that each complement type actually occurs.\n",
    "\n",
    "There are also statistics columns, they contain the number of occurrences for each complement type in that clause.\n",
    "Handy for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 06s 87993 clauses\n"
     ]
    }
   ],
   "source": [
    "sep = ';'\n",
    "of = outfile('clauses.csv')\n",
    "header = '\"passage\"' + sep + '\"verblex\"' + sep\n",
    "for cpt in range(len(comptype_order) - 1):\n",
    "    header += '\"#{}\"{}'.format(comptype_order[cpt], sep)\n",
    "for cpt in range(len(comptype_order) - 1):\n",
    "    for i in range(maxnphrases[cpt]):\n",
    "        header += '\"{}{}\"{}'.format(comptype_order[cpt], i + 1, sep)\n",
    "header += '\"clause\"\\n'\n",
    "of.write(header)\n",
    "\n",
    "for clause in clause_order:\n",
    "    passage = '{} {}:{}'.format(*clause_passage[clause])\n",
    "    verb_lex = vp_lexemes[clause]\n",
    "    complements = []\n",
    "    stats = []\n",
    "    for cpt in range(len(comptype_order) - 1):\n",
    "        phrases = clause_phrases[cpt][clause]\n",
    "        nphrases = len(phrases)\n",
    "        stats.append(str(nphrases))\n",
    "        for i in range(maxnphrases[cpt]):\n",
    "            det = F.det.v(phrases[i]) if i < nphrases else ''\n",
    "            detstr = '1' if det == 'det' else '0'\n",
    "            complements.append(detstr + ' ' + node_transcr_c[phrases[i]] if i < nphrases else '')\n",
    "\n",
    "    of.write(('\"{}\"' + sep + '\"{}\"' + sep + '{}' + sep + '\"{}\"' + sep + '\"{}\"\\n').format(\n",
    "        passage, \n",
    "        verb_lex, \n",
    "        sep.join(stats),\n",
    "        ('\"' + sep + '\"').join(complements),\n",
    "        node_transcr_c[clause],\n",
    "    ))\n",
    "of.close()\n",
    "msg(\"{} clauses\".format(nclauses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
