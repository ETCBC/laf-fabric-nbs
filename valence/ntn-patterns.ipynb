{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://emdros.org\" target=\"_blank\"><img align=\"left\" src=\"files/images/Emdros-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-xsmall.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# נָתַן and locatives\n",
    "\n",
    "We try to mark all those occurrences of נָתַן + object + complement.\n",
    "What we want to mark is whether the complement is a proper indirect object or a locative.\n",
    "\n",
    "This is part of Janet's project of creating a valence dictionary and using a flowchart to arrive at the meaning of a verb in its context of complements.\n",
    "\n",
    "First we use an MQL query to get all occurrences of נָתַן with an object and a complement.\n",
    "Then we apply a few heuristics to detect those cases where the complement is a locative or an indirect object.\n",
    "\n",
    "The query is also on SHEBANQ, a version by [Dirk](http://shebanq.ancient-data.org/hebrew/query?id=558) and a version by [Janet](http://shebanq.ancient-data.org/hebrew/query?id=560)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.0\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import subprocess\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.10s INFO: USING DATA COMPILED AT: 2015-05-04T13-46-20\n",
      "  0.10s DETAIL: COMPILING a: UP TO DATE\n",
      "  0.10s INFO: USING DATA COMPILED AT: 2015-05-04T14-07-34\n",
      "  0.11s DETAIL: keep main: G.node_anchor_min\n",
      "  0.11s DETAIL: keep main: G.node_anchor_max\n",
      "  0.11s DETAIL: keep main: G.node_sort\n",
      "  0.12s DETAIL: keep main: G.node_sort_inv\n",
      "  0.12s DETAIL: keep main: G.edges_from\n",
      "  0.12s DETAIL: keep main: G.edges_to\n",
      "  0.12s DETAIL: keep main: F.etcbc4_db_monads [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_db_oid [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_ft_function [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_ft_lex [node] \n",
      "  0.12s DETAIL: keep main: F.etcbc4_ft_number [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_ft_prs [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_ft_sp [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_lex_nametype [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_sft_book [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_sft_label [node] \n",
      "  0.13s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  0.13s DETAIL: keep annox: F.etcbc4_db_monads [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_db_oid [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_db_otype [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_function [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_lex [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_number [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_prs [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_ft_sp [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_lex_nametype [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_sft_book [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_sft_chapter [node] \n",
      "  0.14s DETAIL: keep annox: F.etcbc4_sft_label [node] \n",
      "  0.15s DETAIL: keep annox: F.etcbc4_sft_verse [node] \n",
      "  0.15s DETAIL: load main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  0.38s DETAIL: load annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  0.39s DETAIL: prep prep: G.node_sort\n",
      "  0.46s DETAIL: prep prep: G.node_sort_inv\n",
      "  1.11s DETAIL: prep prep: L.node_up\n",
      "  7.22s DETAIL: prep prep: L.node_down\n",
      "    17s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK ntn AT 2015-05-28T10-20-03\n"
     ]
    }
   ],
   "source": [
    "version = '4b'\n",
    "API = fabric.load('etcbc{}'.format(version), 'lexicon', 'ntn', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads\n",
    "        function\n",
    "        g_word_utf8 trailer_utf8\n",
    "        lex prs sp nametype\n",
    "        book chapter verse label number\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "    \"primary\": False,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each result, we write out a line of information.\n",
    "Here is a description of the columns.\n",
    "\n",
    "* ``order`` in what order the **P**redicate, **O**bject, and **C**omplement have been encountered\n",
    "* ``verb`` the verb occurrence in vocalised Hebrew\n",
    "* ``object`` the text of the complete (direct) object in Hebrew\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``locativity`` a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``# topo``, and ``# prep_b``\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` plus proper name occur in the complement\n",
    "* ``indirect object`` a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "* ``complement text`` the text of the complete complement as a sequence of transcribed, consonantal lexemes\n",
    "* clause atom number of the clause_atom containing the predicate with NTN\n",
    "* ``clause text`` the text of the complete clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP   :  22 results\n",
      "CPO  :  59 results\n",
      "OCP  :  17 results\n",
      "OP   :  32 results\n",
      "OPC  : 139 results\n",
      "P    :  60 results\n",
      "PC   : 351 results\n",
      "PCO  : 372 results\n",
      "PO   : 200 results\n",
      "POC  : 364 results\n",
      "    0 = error   : 790 results\n",
      "    1 = warning : 736 results\n",
      "    2 = good    :   1 results\n",
      "    4 = good    :  57 results\n",
      "    9 = good    :   9 results\n",
      "   16 = special :   5 results\n",
      "   25 = special :   1 results\n",
      "   36 = special :   2 results\n",
      "   49 = special :   1 results\n",
      "  156 = special :   6 results\n",
      "  169 = special :   5 results\n",
      "  196 = special :   2 results\n",
      "  676 = special :   1 results\n",
      "I : 497 results\n",
      "  : 781 results\n",
      "L : 322 results\n",
      "LI:  16 results\n",
      "Total: 1616 results in 87900 clauses\n"
     ]
    }
   ],
   "source": [
    "locative_lexemes = {\n",
    "    '>RY/',\n",
    "    'BJT/',\n",
    "    'DRK/',\n",
    "    'HR/',\n",
    "    'JM/',\n",
    "    'JRDN/',\n",
    "    'JRWCLM/',\n",
    "    'JFR>L/',\n",
    "    'MDBR/',\n",
    "    'MW<D/',\n",
    "    'MZBX/',\n",
    "    'MYRJM/',\n",
    "    'MQWM/',\n",
    "    'SBJB/',\n",
    "    '<JR/',\n",
    "    'FDH/',\n",
    "    'CM',\n",
    "    'CMJM/',\n",
    "    'CMC/',\n",
    "    'C<R/',\n",
    "}\n",
    "no_prs = {'absent', 'n/a'}\n",
    "\n",
    "statclass = {\n",
    "    'o': 'info',\n",
    "    '+': 'good',\n",
    "    '-': 'error',\n",
    "    '?': 'warning',\n",
    "    '!': 'special',\n",
    "    '*': 'note',\n",
    "}\n",
    "statsym = dict((x[1], x[0]) for x in statclass.items())\n",
    "\n",
    "def cert_status(cert):\n",
    "    if cert == 0: return 'error'\n",
    "    elif cert == 1: return 'warning'\n",
    "    elif cert <= 10: return 'good'\n",
    "    else: return 'special'\n",
    "\n",
    "tsvfile = outfile('ntn.csv')\n",
    "notefile = outfile('ntn-note.csv')\n",
    "nresults = 0\n",
    "nclauses = 0\n",
    "orders = collections.Counter()\n",
    "certs = collections.Counter()\n",
    "tsvfile.write('book\\tchapter\\tverse\\torder\\tverb\\tobject\\tloc\\tloc\\tloc\\tloc\\tind\\tind\\tind\\tcomplement text\\tca_num\\tclause text\\n')\n",
    "tsvfile.write('book\\tchapter\\tverse\\torder\\tverb\\tobject\\t# loc lexemes\\t# topo\\t# prep_b\\tlocativity\\t# prep_l\\t# L prop\\tindirect object\\tcomplement text\\tca_num\\tclause text\\n')\n",
    "pclass = collections.Counter()\n",
    "pclass['LI'] = 0\n",
    "notefile.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "    'version', 'book', 'chapter', 'verse', 'clause_atom', 'is_shared', 'is_published', 'status', 'keywords', 'ntext',\n",
    "))\n",
    "keywords = 'ntn-loca'\n",
    "is_shared = 'T'\n",
    "is_published = ''\n",
    "status = statsym['info']\n",
    "ntext_fmt = 'locative versus indirect object: L={} I={}; {}'\n",
    "\n",
    "climit = 900\n",
    "\n",
    "kws = ''.join(' {} '.format(k) for k in set(keywords.strip().split()))\n",
    "\n",
    "for clause in F.otype.s('clause'):\n",
    "    nclauses += 1\n",
    "    phrases = {}\n",
    "    order = ''\n",
    "    verb = None\n",
    "    for phrase in L.d('phrase', clause):\n",
    "        pf = F.function.v(phrase)\n",
    "        if pf in {'Pred', 'Objc', 'Cmpl'}:\n",
    "            words = L.d('word', phrase)\n",
    "            if pf not in phrases:\n",
    "                order += pf[0]\n",
    "                phrases[pf] = words\n",
    "            else:\n",
    "                phrases[pf].extend(words)\n",
    "    is_ntn = False\n",
    "\n",
    "    for w in phrases.get('Pred', []):\n",
    "        if F.sp.v(w) == 'verb' and F.lex.v(w) == 'NTN[':\n",
    "            is_ntn = True\n",
    "            verb = w\n",
    "            break\n",
    "    if not is_ntn: continue\n",
    "    nresults += 1    \n",
    "    orders[order] += 1    \n",
    "\n",
    "    book = F.book.v(L.u('book', verb))\n",
    "    chapter = F.chapter.v(L.u('chapter', verb))\n",
    "    verse = F.verse.v(L.u('verse', verb))    \n",
    "    clause_atom = F.number.v(L.u('clause_atom', verb))\n",
    "    \n",
    "    verb_txt = F.g_word_utf8.v(verb)\n",
    "    obj_txt = ''.join(F.g_word_utf8.v(x)+F.trailer_utf8.v(x) for x in phrases.get('Objc', []))\n",
    "    cmpl_txt = ''.join(F.g_word_utf8.v(x)+F.trailer_utf8.v(x) for x in phrases.get('Cmpl', []))\n",
    "    if len(cmpl_txt) > climit:\n",
    "        cmpl_txt = cmpl_txt[0:climit]+'...'\n",
    "    clause_txt = ''.join(F.g_word_utf8.v(x)+F.trailer_utf8.v(x) for x in L.d('word', clause))\n",
    "\n",
    "    compl_wnodes = phrases.get('Cmpl', [])\n",
    "    compl_lexemes = [F.lex.v(w) for w in compl_wnodes]\n",
    "    compl_lset = set(compl_lexemes)\n",
    "    lex_locativity = len(locative_lexemes & compl_lset)\n",
    "    prep_b = len([x for x in compl_lexemes if x == 'B'])\n",
    "    prep_l = len([x for x in compl_wnodes if F.lex.v(x) == 'L' and F.prs.v(x) not in no_prs])\n",
    "    prep_lpr = 0\n",
    "    lwn = len(compl_wnodes)\n",
    "    for (n, wn) in enumerate(compl_wnodes):\n",
    "        if F.lex.v(wn) == 'L':\n",
    "            if n+1 < lwn:\n",
    "                if F.sp.v(compl_wnodes[n+1]) == 'nmpr':\n",
    "                    prep_lpr += 1\n",
    "    topo = len([x for x in compl_wnodes if F.nametype.v(x) == 'topo'])\n",
    "\n",
    "    loca = lex_locativity + topo + prep_b\n",
    "    indi = prep_l + prep_lpr\n",
    "\n",
    "    this_class = ''\n",
    "    this_class += 'L' if loca else ''\n",
    "    this_class += 'I' if indi else ''\n",
    "    pclass[this_class] += 1\n",
    "    \n",
    "    tsvfile.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(\n",
    "        book, \n",
    "        chapter, \n",
    "        verse,\n",
    "        order,\n",
    "        verb_txt,\n",
    "        obj_txt,\n",
    "        lex_locativity,\n",
    "        topo,\n",
    "        prep_b,\n",
    "        loca,\n",
    "        prep_l,\n",
    "        prep_lpr,\n",
    "        indi,\n",
    "        ' '.join(compl_lexemes),\n",
    "        clause_atom,\n",
    "        clause_txt,\n",
    "    ).replace('\\n', ' ')+'\\n')\n",
    "    \n",
    "    ntext = ntext_fmt.format(loca, indi, cmpl_txt)\n",
    "    certainty = abs(loca - indi) * max((loca, indi))\n",
    "    certs[certainty] += 1\n",
    "    status = statsym[cert_status(certainty)]\n",
    "    notefile.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(\n",
    "        version, book, chapter, verse, clause_atom, is_shared, is_published, status, kws, ntext,\n",
    "    ).replace('\\n', ' ')+'\\n')\n",
    "    \n",
    "tsvfile.close()\n",
    "notefile.close()\n",
    "for order in sorted(orders):\n",
    "    print(\"{:<5}: {:>3} results\".format(order, orders[order]))\n",
    "\n",
    "for cert in sorted(certs):\n",
    "    print(\"{:>5} = {:<8}: {:>3} results\".format(cert, cert_status(cert), certs[cert]))\n",
    "\n",
    "for this_class in pclass:\n",
    "    print(\"{:<2}: {:>3} results\".format(this_class, pclass[this_class]))\n",
    "print('Total: {:>3} results in {} clauses'.format(nresults, nclauses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\tchapter\tverse\torder\tverb\tobject\tloc\tloc\tloc\tloc\tind\tind\tind\tcomplement text\tca_num\tclause text\r\n",
      "book\tchapter\tverse\torder\tverb\tobject\t# loc lexemes\t# topo\t# prep_b\tlocativity\t# prep_l\t# L prop\tindirect object\tcomplement text\tca_num\tclause text\r\n",
      "Genesis\t1\t17\tPOC\tיִּתֵּ֥ן\tאֹתָ֛ם \t1\t0\t1\t2\t0\t0\t0\tB RQJ</ H CMJM/\t67\tוַיִּתֵּ֥ן אֹתָ֛ם אֱלֹהִ֖ים בִּרְקִ֣יעַ הַשָּׁמָ֑יִם \r\n",
      "Genesis\t1\t29\tPCO\tנָתַ֨תִּי\tאֶת־כָּל־עֵ֣שֶׂב ׀ וְאֶת־כָּל־הָעֵ֛ץ \t0\t0\t0\t0\t1\t0\t1\tL\t121\tהִנֵּה֩ נָתַ֨תִּי לָכֶ֜ם אֶת־כָּל־עֵ֣שֶׂב ׀ וְאֶת־כָּל־הָעֵ֛ץ \r\n",
      "Genesis\t3\t6\tPC\tתִּתֵּ֧ן\t\t0\t0\t0\t0\t0\t0\t0\tGM L >JC/\t258\tוַתִּתֵּ֧ן גַּם־לְאִישָׁ֛הּ עִמָּ֖הּ \r\n",
      "Genesis\t3\t12\tPC\tנָתַ֣תָּה\t\t0\t0\t0\t0\t0\t0\t0\t<MD/\t285\tאֲשֶׁ֣ר נָתַ֣תָּה עִמָּדִ֔י \r\n",
      "Genesis\t3\t12\tPC\tנָֽתְנָה\t\t0\t0\t0\t0\t0\t0\t0\tMN H <Y/\t286\tהִ֛וא נָֽתְנָה־לִּ֥י מִן־הָעֵ֖ץ \r\n",
      "Genesis\t4\t12\tPOC\tתֵּת\tכֹּחָ֖הּ \t0\t0\t0\t0\t1\t0\t1\tL\t385\tתֵּת־כֹּחָ֖הּ לָ֑ךְ \r\n",
      "Genesis\t9\t2\tCP\tנִתָּֽנוּ\t\t0\t0\t1\t1\t0\t0\t0\tB JD/\t762\tבְּיֶדְכֶ֥ם נִתָּֽנוּ׃\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 {my_file('ntn.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version\tbook\tchapter\tverse\tclause_atom\tis_shared\tis_published\tstatus\tkeywords\tntext\r\n",
      "4b\tGenesis\t1\t17\t67\tT\t\t+\tntn-loca\tlocative versus indirect object: L=2 I=0; בִּרְקִ֣יעַ הַשָּׁמָ֑יִם \r\n",
      "4b\tGenesis\t1\t29\t121\tT\t\t?\tntn-loca\tlocative versus indirect object: L=0 I=1; לָכֶ֜ם \r\n",
      "4b\tGenesis\t3\t6\t258\tT\t\t-\tntn-loca\tlocative versus indirect object: L=0 I=0; גַּם־לְאִישָׁ֛הּ \r\n",
      "4b\tGenesis\t3\t12\t285\tT\t\t-\tntn-loca\tlocative versus indirect object: L=0 I=0; עִמָּדִ֔י \r\n",
      "4b\tGenesis\t3\t12\t286\tT\t\t-\tntn-loca\tlocative versus indirect object: L=0 I=0; מִן־הָעֵ֖ץ \r\n",
      "4b\tGenesis\t4\t12\t385\tT\t\t?\tntn-loca\tlocative versus indirect object: L=0 I=1; לָ֑ךְ \r\n",
      "4b\tGenesis\t9\t2\t762\tT\t\t?\tntn-loca\tlocative versus indirect object: L=1 I=0; בְּיֶדְכֶ֥ם \r\n",
      "4b\tGenesis\t9\t3\t766\tT\t\t?\tntn-loca\tlocative versus indirect object: L=0 I=1; לָכֶ֖ם \r\n",
      "4b\tGenesis\t9\t13\t797\tT\t\t?\tntn-loca\tlocative versus indirect object: L=1 I=0; בֶּֽעָנָ֑ן \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 {my_file('ntn-note.csv')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the result files from my SURFdrive: [tab separated file](https://surfdrive.surf.nl/files/public.php?service=files&t=9c80db85478ae91a74cb3408a24a1b14) and a formatted [openoffice spreadsheet](https://surfdrive.surf.nl/files/public.php?service=files&t=38df9044eb5ac29a09a9daff81e1f276)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate a manual annotation file\n",
    "\n",
    "We need per note:\n",
    "1. Data version\n",
    "1. Book\n",
    "1. Chapter number\n",
    "1. Verse number\n",
    "1. Clause atom number\n",
    "1. Status\n",
    "1. Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
