{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"https://www.academic-bible.com/en/online-bibles/biblia-hebraica-stuttgartensia-bhs/read-the-bible-text/\" target=\"_blank\"><img align=\"right\" src=\"files/images/DBG-xsmall.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbal valence\n",
    "\n",
    "*Verbal valence* is a kind of signature of a verb, not unlike overloading in programming languages.\n",
    "The meaning of a verb depends on the number and kind of its complements, i.e. the linguistic entities that act as parameters for the semantic function of the verb.\n",
    "\n",
    "We will use a set of flowcharts to specify and compute the sense of a verb in specific contexts depending on the verbal valence it has in that context. The flowcharts are by Janet Dyk. Although they are not difficult to understand, it takes a good deal of ingenuity to apply them in all the real world situations that we encounter in our corpus.\n",
    "\n",
    "\n",
    "# Authors\n",
    "\n",
    "This notebook is being written by [Dirk Roorda](dirk.roorda@dans.knaw.nl) following the ideas of \n",
    "[Janet Dyk](j.w.dyk@vu.nl). Janet's ideas have been published in various ways, see the references below.\n",
    "They can be summarized as a set of flowcharts. Each flowchart describes set of rules how to choose between\n",
    "the senses of a specific verb based on the constituents in each context where it occurs.\n",
    "The role of Dirk is to turn those ideas into a working program base on the ETCBC data.\n",
    "\n",
    "# About\n",
    "\n",
    "This is an IPython notebook. It contains a working program to carry out the computations that we need for\n",
    "making use of verbal valence patterns.\n",
    "You can download this notebook and run it on your computer, provided you have\n",
    "[LAF-Fabric](http://laf-fabric.readthedocs.org/en/latest/texts/welcome.html) installed.\n",
    "An easy way to do that is describe [here](https://github.com/ETCBC/llshebanq).\n",
    "\n",
    "There is not only code in this notebook, but also extensive documentation, and a description how to view\n",
    "the results on \n",
    "[SHEBANQ](https://shebanq.ancient-data.org) as a set of *Notes*.\n",
    "See the end of the notebook for precise links.\n",
    "\n",
    "# Status\n",
    "\n",
    "**Last modified: 2015-07-09**\n",
    "\n",
    "This notebook is not yet finished. \n",
    "It turns out that the ETCBC data at present does not contain all bits and pieces that are needed to follow\n",
    "the rules in Janet's flowcharts. It is difficult to find all direct objects, especially implied ones.\n",
    "And there are many cases where the database encodes a phrase as a complement, where the flowchart expects it to be a direct object.\n",
    "\n",
    "# More about flowcharts\n",
    "\n",
    "Here is an original flowchart by Janet, the one for NTN (*give*).\n",
    "\n",
    "<img src=\"images/FlowChartNTN-orig.pdf\"/>\n",
    "\n",
    "In order to run the flowcharts, preliminary work has to be done. \n",
    "We have to \n",
    "\n",
    "* identify direct objects;\n",
    "* divide them into primary and secundary ones;\n",
    "* identify complements;\n",
    "* divide them into locatives, indirect objects, and other complements;\n",
    "* detect relativa and offer them as potential direct objects;\n",
    "* detect phrases starting with MN (*from*) and offer them as potential direct objects.\n",
    "\n",
    "It is not straightforward to extract all this information from the contexts of the verbs,\n",
    "and quite often additional explorations and checks have to be done, see the companion notebook\n",
    "[flowchart_checks](http://nbviewer.ipython.org/github/etcbc/laf-fabric-nbs/blob/master/valence/flowchart_checks.ipynb)\n",
    "\n",
    "# Generic flowchart\n",
    "\n",
    "The generic flowchart rules can be read off this diagram.\n",
    "\n",
    "<img src=\"images/Valence-Generic.pdf\"/>\n",
    "\n",
    "In fact, this part of the flowchart requires the most programming effort.\n",
    "\n",
    "\n",
    "\n",
    "# Specific flowcharts\n",
    "\n",
    "Using the generic flowchart, we state the rules for individual verbs, which can be expressed as simple\n",
    "multiple choice lists. Far below in this notebook, these rules will be applied to all clauses.\n",
    "\n",
    "As an example, this is a simplified flowchart for NTN in diagram form as we will implement it below.\n",
    "\n",
    "<img src=\"images/Valence-NTN.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart logic\n",
    "\n",
    "Here is the bare logic of the flow charts for the individual verbs.\n",
    "\n",
    "The ``senses`` data structure is a dictionary keyed by verb lexemes. \n",
    "For each verb it is keyed by *sense labels*, which is a code for the number of direct object and the nature of complements that are present in the context.\n",
    "\n",
    "Behind each sense label there is information about the meaning of the verb in such a context.\n",
    "The meaning consists of 2 or 3 pieces of information.\n",
    "\n",
    "The important part is the second one, the *sense template*, which consist of a gloss augmented with placeholders for the direct objecs and complements.\n",
    "\n",
    "* **{verb}** the verb occurrence in question\n",
    "* **{pdo}** primary direct object\n",
    "* **{sdos}** secundary direct objects\n",
    "* **{inds}** indirect objects\n",
    "* **{locs}** locatives\n",
    "* **{cpls}** complements, not marked as either indirect object or locative\n",
    "\n",
    "In case there are multiple entities, the algorithm returns them chunked as phrases/clauses.\n",
    "\n",
    "Apart from the template, there is also a *status* and an optional *account*. \n",
    "\n",
    "The status is ``!`` in normal cases, ``?`` in dubious cases, and ``-`` in erroneous cases.\n",
    "In SHEBANQ these statuses are translated into colors of the notes (blue/orange/red).\n",
    "\n",
    "The account contains information about the grounds of which the algorithm has arrived at its conclusions.\n",
    "\n",
    "* **{ilc}** the outcome of the heuristic that distinguishes locatives from indirect objects\n",
    "\n",
    "A typical case is ``NTN[`` sense ``0c``. This verbs prefers indirect objects and not locatives.\n",
    "So when the context has a complement that fails to be classified beforehand as either locative or indirect object, this is the moment that we finally decide it is an indirect object after all.\n",
    "But this is risky, so we give it status ``?`` and we tell the user that we have decided to change ``C`` into ``I`` for this complement.\n",
    "\n",
    "Likewise, sense ``0l`` is not expected to occur. When we encounter it, we conclude that our heuristic for choosing between ``L`` and ``I`` has failed here, and we overrule that decision and change ``L`` to ``I``.\n",
    "We tell the user that here we have encountered an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senses_spec = '''\n",
    "<FH\n",
    "00:!: act; take action\n",
    "0i:?: act; take action for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: act; take action at {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: do; make; perform; observe {cpls} :: {cpls} taken as direct object\n",
    "10:!: do; make; perform; observe {pdo}\n",
    "1i:?: do; make; perform; observe {pdo} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:?: do; make; perform; observe {pdo} at {locs} :: {locs} taken as locative adjunct\n",
    "1c:?: make {pdo} to be {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: make {pdo} to be {sdos}\n",
    "\n",
    "BR>\n",
    "00:-: !not encountered!\n",
    "0i:?: create for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: create at {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: create {cpls} :: {cpls} taken as direct object\n",
    "10:!: create {pdo}\n",
    "1i:?: create {pdo} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:?: create {pdo} at {locs} :: {locs} taken as locative adjunct\n",
    "1c:?: create {pdo} to be {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: create {pdo} to be {sdos}\n",
    "\n",
    "CJT\n",
    "00:-: !not encountered!\n",
    "0i:-: !not encountered!\n",
    "0l:-: !not encountered!\n",
    "0c:?: install; set up; put in place {cpls} :: {cpls} taken as direct object\n",
    "10:!: install; set up; put in place {pdo}\n",
    "1i:?: place {pdo} for the benefit of {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:!: place {pdo} ... {locs}\n",
    "1c:?: make {pdo} to be {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: make {pdo} to be {sdos}\n",
    "\n",
    "DBQ\n",
    "00:-: !not encountered!\n",
    "0i:?: cling; cleave; adhere to {inds} :: {inds} taken as locative\n",
    "0l:!: cling; cleave; adhere after/to {locs}\n",
    "0c:?: cling; cleave; adhere to {cpls} :: {cpls} taken as locative\n",
    "10:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1i:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1l:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "1c:-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "2 :-: !not encountered! :: Should {verb} be hiphil? :: ?\n",
    "\n",
    "FJM\n",
    "00:!: prepare; put in place; make ready\n",
    "0i:?: prepare; put in place; make ready for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:!: make ready; prepare {locs} (specific meaning depending on preposition)\n",
    "0c:?: prepare; put in place; institute {pdo} :: {cpls} taken as extra direct object besides {pdo}\n",
    "10:!: prepare; put in place; institute {pdo}\n",
    "1i:?: prepare; put in place; institute {pdo} for {inds} :: {inds} taken as benefactive adjunct\n",
    "1l:!: put; place {pdo} ... {locs} (specific meaning depending on preposition)\n",
    "1c:?: make {pdo} (to be (as)/to become/to do) {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: make {pdo} (to be (as)/to become/to do) {sdos}\n",
    "\n",
    "NTN\n",
    "00:!: (act of) producing; yielding; giving (in itself)\n",
    "0i:!: produce for; yield for; give to {inds}\n",
    "0l:-: !not encountered!\n",
    "0c:?: produce; yield; give {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "10:!: produce; yield; give {pdo}\n",
    "1i:!: give {pdo} to {inds}\n",
    "1l:!: place {pdo} ... {locs}\n",
    "1c:?: make {pdo} (to be (as)/to become/to do) {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: make {pdo} (to be (as)/to become/to do) {sdos}\n",
    "\n",
    "QR>\n",
    "00:!: shout; call; invoke\n",
    "0i:!: call; summon {inds}\n",
    "0l:?: call at {locs} :: {locs} taken as locative adjunct.\n",
    "0c:?: call {cpls} (content) :: {cpls} taken as direct object\n",
    "10:!: call; summon {pdo} (content or addressee)\n",
    "1i:!: summon {pdo} for {inds}\n",
    "1l:!: call out {pdo} before {locs}\n",
    "1c:?: call {pdo} (to be named) {cpls} :: {cpls} taken as extra direct object besides {pdo}\n",
    "2 :!: call {pdo} (to be named) {sdos}\n",
    "\n",
    "ZQN\n",
    "00:!: be old\n",
    "0i:?: be old for {inds} :: {inds} taken as benefactive adjunct\n",
    "0l:?: be old in {locs} :: {locs} taken as locative adjunct\n",
    "0c:?: be old ... {cpls} :: {cpls} taken as adjunct\n",
    "10:-: !not encountered!\n",
    "1i:-: !not encountered!\n",
    "1l:-: !not encountered!\n",
    "1c:-: !not encountered!\n",
    "2 :-: !not encountered!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "See the results on SHEBANQ.\n",
    "\n",
    "The complete set of results is in the note set \n",
    "[valence](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxlbmNl&tp=txt_tb1).\n",
    "You can find it on the Notes page in SHEBANQ:\n",
    "\n",
    "<img src=\"images/valnotes.png\"/>\n",
    "\n",
    "By checking the other note sets you *mute* them, so they do not show up among the lines.\n",
    "\n",
    "In order to see a note set, click on its name. You then go to pages with all verses that have a note of this set attached. \n",
    "\n",
    "<img src=\"images/notesview.png\"/>\n",
    "\n",
    "In order to see the actual notes, click the comment cloud icons. If you click the upper left one, notes are fetched for all verses on the page.\n",
    "\n",
    "<img src=\"images/withnotes.png\"/>\n",
    "\n",
    "You can also export the notes as csv, or view them in a chart.\n",
    "\n",
    "The *valence* set has the following subsets:\n",
    "\n",
    "* Unresolved results: [val_nb](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfbmI_&tp=txt_tb1);\n",
    "* Uncertain results: [val_wrn](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfd3Ju&tp=txt_tb1);\n",
    "* Erroneous results: [val_err](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfZXJy&tp=txt_tb1);\n",
    "* Promotion candidates [val_prom](https://shebanq.ancient-data.org/hebrew/note?version=4b&id=Mnx2YWxfcHJvbQ__&tp=txt_tb1)\n",
    "\n",
    "So if you follow the *valence* link you see them all, but you can also focus on the problematic cases.\n",
    "\n",
    "And if you are logged in, you can add remarks in free text. Just start typing in one of the new note boxes.\n",
    "Hint: use the keyword **val_note** for your manual notes to valence, then other users can see all relevant information about valence together.\n",
    "\n",
    "By clicking on the status symbol you can cycle through different display styles and colors for your note.\n",
    "Do not forget to save when you are done!\n",
    "\n",
    "See also the SHEBANQ help on notes:\n",
    "[general](https://shebanq.ancient-data.org/help#notes)\n",
    "[notes view](https://shebanq.ancient-data.org/help#notes_style)\n",
    "[working with notes](https://shebanq.ancient-data.org/help#working_with_notes)\n",
    "\n",
    "If you have a solid contribution to make, e.g. the outcome of an algorithm, consider\n",
    "[bulk uploading notes](https://shebanq.ancient-data.org/help#bulk_uploading_notes).\n",
    "\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext](https://surfdrive.surf.nl/files/public.php?service=files&t=d4ad6d52915e5003af45662d28d4c39d)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firing up the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.3\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: http://shebanq-doc.readthedocs.org/en/latest/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-06-29T05-30-49\n",
      "  0.01s INFO: USING DATA COMPILED AT: 2015-05-04T14-07-34\n",
      "  8.79s LOGFILE=/Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/valence/__log__valence.txt\n",
      "    24s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  0.00s LOADING API with EXTRAs: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-06-29T05-30-49\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-05-04T14-07-34\n",
      "  0.01s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK valence AT 2015-07-09T12-01-30\n",
      "  0.00s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK valence AT 2015-07-09T12-01-30\n"
     ]
    }
   ],
   "source": [
    "version = '4b'\n",
    "API = fabric.load('etcbc{}'.format(version), 'lexicon', 'valence', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads\n",
    "        function rela\n",
    "        g_word_utf8 trailer_utf8\n",
    "        lex prs uvf sp ls vs vt nametype det gloss\n",
    "        book chapter verse label number\n",
    "    ''',\n",
    "    '''\n",
    "        mother\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "    \"primary\": False,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Here we specify details of the flow chart process such as which lexemes to look for, which senses they have, etc.\n",
    "\n",
    "``locative_lexemes``: a set of lexemes that are cues for a locative interpretation of complements in which they occur.\n",
    "This set is far from exhaustive.\n",
    "\n",
    "``personal_lexemes``: a set of lexemes that have a high probability of referring to people. A pretty exhaustive list.\n",
    "Complements with these lexemes tend to be indirect objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())\n",
    "\n",
    "pronominal_suffix = {\n",
    "    'W': ('p3-sg-m', 'him'),\n",
    "    'K': ('p2-sg-m', 'you:m'),\n",
    "    'J': ('p1-sg-', 'me'),\n",
    "    'M': ('p3-pl-m', 'them:mm'),\n",
    "    'H': ('p3-sg-f', 'her'),\n",
    "    'HM': ('p3-pl-m', 'them:mm'),\n",
    "    'KM': ('p2-pl-m', 'you:mm'),\n",
    "    'NW': ('p1-pl-', 'us'),\n",
    "    'HW': ('p3-sg-m', 'him'),\n",
    "    'NJ': ('p1-sg-', 'me'),\n",
    "    'K=': ('p2-sg-f', 'you:f'),\n",
    "    'HN': ('p3-pl-f', 'them:ff'),\n",
    "    'MW': ('p3-pl-m', 'them:mm'),\n",
    "    'N': ('p3-pl-f', 'them:ff'),\n",
    "    'KN': ('p2-pl-f', 'you:ff'),\n",
    "}\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())\n",
    "\n",
    "non3_prs = set('''\n",
    "K J KM NW NJ K= KN\n",
    "'''.strip().split())\n",
    "\n",
    "to_be = set('''\n",
    "    HJH[ HWH[\n",
    "'''.strip().split())\n",
    "\n",
    "\n",
    "predicates = set('''\n",
    "Pred PreS PreO PtcO PreC\n",
    "'''.strip().split())\n",
    "\n",
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "                  \n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a verb-clause index\n",
    "\n",
    "We generate an index which gives for each verb lexeme a list of clauses that have that lexeme as the main verb.\n",
    "In the index we store the clause node together with the word node(s) that carries the main verb(s).\n",
    "\n",
    "Clauses may have multiple verbs. In many cases it is 'HJH[' (or 'HWH[') plus an other verb.\n",
    "In those cases, it is the other verb that is the main verb.\n",
    "\n",
    "Yet, there are also sentences with more than one main verb.\n",
    "In those cases, we treat both verbs separately as main verb of one and the same clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2h 31m 50s Making the verb-clause index\n",
      " 2h 31m 54s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49 multiple verb clauses of total 87900 clauses\n",
      "Genesis 4:12#40_1 NW<[ NWD[\n",
      "Genesis 4:14#45_1 NW<[ NWD[\n",
      "Genesis 4:22#70_2 LVC[ XRC=[\n",
      "Genesis 8:5#9_1 HLK[ XSR[\n",
      "Genesis 13:7#10_1 R<H[ R<H[\n",
      "Genesis 31:39#130_1 BQC[ GNB[ GNB[\n",
      "Exodus 27:16#36_1 CZR[ RQM[\n",
      "Exodus 38:18#34_1 RQM[ CZR[\n",
      "Leviticus 22:4#10_2 YR<[ ZWB[\n",
      "Deuteronomium 21:20#57_2 SRR[ MRH[\n",
      "Deuteronomium 21:20#57_4 ZLL[ SB>[\n",
      "Deuteronomium 28:29#59_1 <CQ[ GZL[\n",
      "Deuteronomium 28:33#78_1 <CQ[ RYY[\n",
      "Deuteronomium 32:36#92_3 <YR[ <ZB[\n",
      "Josua 9:21#63_1 XVB[ C>B[\n",
      "Samuel_I 18:23#79_1 RWC[ QLH=[\n",
      "Reges_I 21:21#83_3 <YR[ <ZB[\n",
      "Jesaia 14:19#50_1 HRG[ V<N=[\n",
      "Jesaia 16:2#2_1 NDD[ CLX[\n",
      "Jesaia 17:14#43_1 CSH[ BZZ[\n",
      "Jesaia 27:10#28_1 CLX[ <ZB[\n",
      "Jesaia 42:22#67_1 BZZ[ CSH[\n",
      "Jesaia 49:26#97_2 JC<[ G>L[\n",
      "Jesaia 60:15#47_1 <ZB[ FN>[\n",
      "Jesaia 60:16#50_2 JC<[ G>L[\n",
      "Jeremia 22:28#85_1 BZH[ NPY[\n",
      "Jeremia 41:5#12_1 GLX[ QR<[ GDD[\n",
      "Ezechiel 28:24#75_1 M>R[ K>B[\n",
      "Sacharia 8:10#25_1 JY>[ BW>[\n",
      "Maleachi 3:2#8_1 YRP[ KBS[\n",
      "Psalmi 24:6#8_1 DRC[ BQC[\n",
      "Psalmi 32:1#3_1 NF>[ KSH[\n",
      "Psalmi 78:8#10_8 SRR[ MRH[\n",
      "Proverbia 21:6#10_1 NDP[ BQC[\n",
      "Proverbia 25:19#35_1 R<H[ M<D[\n",
      "Proverbia 25:26#47_1 RPF[ CXT[\n",
      "Proverbia 28:15#29_1 NHM[ CQQ[\n",
      "Esther 3:5#13_2 KR<[ XWH[\n",
      "Esther 3:8#23_2 PZR[ PRD[\n",
      "Esther 8:14#39_2 BHL[ DXP[\n",
      "Esther 9:4#9_1 HLK[ GDL[\n",
      "Daniel 6:27#82_2 ZW<[ DXL[\n",
      "Nehemia 12:46#42_1 CJR[ JDH[\n",
      "Chronica_I 12:2#4_1 JMN[ FM>L[\n",
      "Chronica_I 24:6#15_1 >XZ[ >XZ[\n",
      "Chronica_II 2:8#17_1 GDL[ PL>[\n",
      "Chronica_II 5:13#27_1 XYR[ CJR[\n",
      "Chronica_II 15:5#14_1 JY>[ BW>[\n",
      "Chronica_II 28:15#65_1 NHL[ KCL[\n"
     ]
    }
   ],
   "source": [
    "msg('Making the verb-clause index')\n",
    "nclauses = 0\n",
    "multiple = []\n",
    "verb_clause = collections.defaultdict(lambda: [])\n",
    "clause_verb = collections.OrderedDict()\n",
    "\n",
    "for c in F.otype.s('clause'):\n",
    "    nclauses += 1\n",
    "    the_verbs = []\n",
    "    for p in L.d('phrase', c):\n",
    "        pf = F.function.v(p)\n",
    "        if pf in predicates:\n",
    "            for w in L.d('word', p):\n",
    "                if F.sp.v(w) == 'verb': the_verbs.append(w)\n",
    "    if len(the_verbs):\n",
    "        real_verbs = []\n",
    "        keep_to_be = len(the_verbs) == 1\n",
    "        for v in the_verbs:\n",
    "            vl = F.lex.v(v)\n",
    "            if keep_to_be or (vl not in to_be): real_verbs.append(v)\n",
    "        if len(real_verbs) > 1: multiple.append('{} {}:{}#{}_{} {}'.format(\n",
    "            F.book.v(L.u('book', v)),\n",
    "            F.chapter.v(L.u('chapter', v)),\n",
    "            F.verse.v(L.u('verse', v)),\n",
    "            F.number.v(L.u('sentence', v)),\n",
    "            F.number.v(c),\n",
    "            ' '.join(F.lex.v(x) for x in real_verbs),\n",
    "        ))\n",
    "        for v in real_verbs:\n",
    "            vl = F.lex.v(v)\n",
    "            verb_clause[vl].append((c,v))\n",
    "        if len(real_verbs):\n",
    "            clause_verb[c] = tuple(real_verbs)\n",
    "msg('Done')\n",
    "print('There are {} multiple verb clauses of total {} clauses'.format(len(multiple), nclauses))\n",
    "print('\\n'.join(multiple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the direct objects\n",
    "\n",
    "In the target clauses we will find the direct object(s).\n",
    "If there is more than one, we will compute which is the primary one.\n",
    "The others are secundary ones.\n",
    "If there is only one direct object, it may or may not be the primary direct object.\n",
    "\n",
    "An object can be a phrase or a clause. \n",
    "A clause object is never a primary direct object.\n",
    "\n",
    "## Implied objects\n",
    "\n",
    "There are many cases where there is a direct object without it being marked as such in the data.\n",
    "Those are cases where there are no objective, unambiguous signals for a direct object.\n",
    "We call them *implied objects*. Examples: \n",
    "\n",
    "* the relativum in relative clauses\n",
    "* complements starting with MN (from) or L (to)\n",
    "\n",
    "In the case of implied objects we have to guess.\n",
    "We compute the set of direct objects (and the primary direct object) in two ways:\n",
    "\n",
    "* assuming that there is no implied object\n",
    "* assuming that we have an implied object\n",
    "\n",
    "When we compute flowcharts of individual verbs, and there are candidates for implied objects present in the context, we compute the sense for all different possible cases. \n",
    "If the computed sense carries an error status, we skip the possibility.\n",
    "If we end up with multiple possibilities, we generate a highlight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the primary direct object\n",
    "\n",
    "When there are multiple direct objects, we use the rules formulated by (Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) to determine which one is the primary one. The rules are summarized in the diagram above. Here we make some remarks as how we apply them to our data.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Clauses that function as direct object are never the primary direct object. So we restrict to direct objects at the phrase level, either being complete phrases, or pronominal suffixes within phrases. The following rules express a preference for the primary direct object. In a given context, we select the direct object that is preferred by applying those rules as the primary direct object. We only apply these rules if there are at least two objects.\n",
    "If there is only one object, it is chosen as primary object if and only if it is not a clause.\n",
    "\n",
    "#### Rule 1: pronominal suffixes > preferred above marked objects > unmarked objects\n",
    "\n",
    "In a given clause, we collect all phrases with function ``PreO`` or ``PtcO``. \n",
    "If this collection is non-empty, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise, we proceed as follows.\n",
    "\n",
    "We collect all the phrases with function ``Objc``.\n",
    "If this collection is empty, there will not be a primary object.\n",
    "Otherwise, we split it up in marked and unmarked object phrases.\n",
    "\n",
    "An object phrase is *marked* if and only if it contains, somewhere, the object marker ``>T``.\n",
    "If there are marked object phrases, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 2: determined phrases > undetermined phrases\n",
    "\n",
    "We only arrive here if there are multiple ``Objc`` phrases, neither of which is marked.\n",
    "In this case, we take the textually first one (by rule 3) which has the value ``det`` for its feature ``det``, if there is one, and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 3: earlier phrases > later phrases (by textual order)\n",
    "\n",
    "This rule is implicitly applied if one of the rules before yielded more than one candidate for the primary object. Furthermore, we arrive here if the previous rules have not selected any primary direct object, while we do have more than one ``Objc`` phrase.\n",
    "\n",
    "In this case, we pick the textually first ``Objc`` phrase.\n",
    "\n",
    "#### NB: Implied object\n",
    "\n",
    "In case we reckon with an implied objects, the primary one is the implied object that comes textually first. \n",
    "All other ones are secudary ones.\n",
    "\n",
    "# Complements as Objects\n",
    "\n",
    "In some cases, a complement functions as objects, such as in [Genesis 21:13](https://shebanq.ancient-data.org/hebrew/text?nget=v&chapter=21&book=Genesis&qw=n&tp=txt_tb1&version=4b&mr=m) *I make him (into) a people*.\n",
    "\n",
    "Candidates are those complements that: \n",
    "\n",
    "* start with either preposition ``L`` or ``K`` and\n",
    "* the ``L`` or ``K`` in question does not carry a pronominal suffix\n",
    "\n",
    "Todo\n",
    "\n",
    "Extra condition:\n",
    "\n",
    "Only promote to object if it is not locative, and not a body part, more precisely:\n",
    "Exclude L+bodypart; L+locative lexeme\n",
    "\n",
    "* should also not be followed by a body part\n",
    "\n",
    "Currently, we do not have a good way to decide whether candidates for object promotion should be in fact promoted.\n",
    "So we do not build this in into the flowchart rules, but we do generate annotations that specify the promotion candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2h 32m 40s Finding direct objects and determining the primary one\n",
      " 2h 32m 42s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clauses with  3 objects                 :     6\n",
      "Clauses with  2 objects                 :  1133\n",
      "Clauses with  1 objects                 : 26138\n",
      "Clauses with  0 objects                 : 42147\n",
      "Clauses with    implied objects         : 61117\n",
      "Clauses with  M implied objects         :  3370\n",
      "Clauses with  R implied objects         :  4568\n",
      "Clauses with MM implied objects         :     3\n",
      "Clauses with RM implied objects         :   365\n",
      "Clauses with RMM implied objects        :     1\n",
      "Clauses with  3 complements as objects  :     1\n",
      "Clauses with  2 complements as objects  :    34\n",
      "Clauses with  1 complements as objects  :  3936\n",
      "Clauses with a primary object           : 27268\n",
      "Clauses with a direct object            : 27277\n",
      "Clauses with an implied object          :  8307\n",
      "Clauses with a complement as object     :  3971\n",
      "Total number of clauses                 : 69424\n"
     ]
    }
   ],
   "source": [
    "msg('Finding direct objects and determining the primary one')\n",
    "directobjects = {}\n",
    "primdirectobjects = {}\n",
    "directobjects_c = {}\n",
    "primdirectobjects_c = {}\n",
    "promotions = {}\n",
    "mobjects = collections.Counter() # count how many clauses have m objects (for each m)\n",
    "cobjects = collections.Counter() # count how many clauses have m object candidates\n",
    "cmobjects = collections.Counter() # count how many clauses have m promotion candidates\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d('word', p)\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "        \n",
    "for c in clause_verb:\n",
    "    dobjects = {}\n",
    "    dobjects_set = set()\n",
    "    dobjects_c = {}\n",
    "    dobjects_set_c = set()\n",
    "    nobjects = 0\n",
    "    nobjects_c = ''\n",
    "    prom = []\n",
    "    \n",
    "    for p in L.d('phrase', c):\n",
    "        pf = F.function.v(p)\n",
    "        if pf in objectfuncs:\n",
    "            dobjects.setdefault('p_'+pf, set()).add(p)\n",
    "            nobjects += 1\n",
    "            dobjects_set.add(p)\n",
    "        elif pf == 'Rela':\n",
    "            dobjects_c.setdefault('p_'+pf, set()).add(p)\n",
    "            nobjects_c += 'R'\n",
    "            dobjects_set_c.add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d('word', p)\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l == 'MN':\n",
    "                dobjects_c.setdefault('p_MN_'+pf, set()).add(p)\n",
    "                nobjects_c += 'M'\n",
    "                dobjects_set_c.add(p)\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                prom.append(p)\n",
    "    cobjects[nobjects_c] += 1\n",
    "    nprom = len(prom)\n",
    "    if nprom:\n",
    "        cmobjects[nprom] += 1\n",
    "        promotions[c] = prom\n",
    "\n",
    "    # find clause objects\n",
    "    for ac in L.d('clause', L.u('sentence', c)):\n",
    "        cr = F.rela.v(ac)\n",
    "        if cr in {'Objc'} and list(C.mother.v(ac))[0] == c:\n",
    "            dobjects.setdefault('c_'+cr, set()).add(p)\n",
    "            nobjects += 1\n",
    "            dobjects_set.add(p)\n",
    "    mobjects[nobjects] += 1\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    dobjects_order = sorted(dobjects_set, key=NK)\n",
    "\n",
    "    # compute the primary object\n",
    "    primary_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            theobject = list(dobjects_set)[0]\n",
    "            if F.otype.v(theobject) == 'phrase': primary_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes\n",
    "        primary_candidates = dobjects.get('p_PreO', set()) | dobjects.get('p_PtcO', set())\n",
    "        if len(primary_candidates) != 0:\n",
    "            primary_object = sorted(primary_candidates, key=NK)[0]\n",
    "            break\n",
    "        primary_candidates = dobjects.get('p_Objc', set())\n",
    "        if len(primary_candidates) != 0:\n",
    "            if len(primary_candidates) > 0:\n",
    "                primary_object = sorted(primary_candidates, key=NK)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in primary_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                primary_object = sorted(objects_marked, key=NK)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                primary_object = sorted(objects_unmarked, key=NK)[0]\n",
    "                break            \n",
    "    if primary_object != None:\n",
    "        primdirectobjects[c] = primary_object\n",
    "\n",
    "    if len(dobjects_set): directobjects[c] = dobjects_set\n",
    "    if len(dobjects_set_c): directobjects_c[c] = dobjects_set_c\n",
    "\n",
    "msg('Done') \n",
    "\n",
    "for (label, n) in sorted(mobjects.items(), key=lambda y: -y[0]):\n",
    "    print('{:<40}: {:>5}'.format('Clauses with {:>2} objects'.format(label), n))\n",
    "for (label, n) in sorted(cobjects.items(), key=lambda y: (len(y[0]), y)):\n",
    "    print('{:<40}: {:>5}'.format('Clauses with {:>2} implied objects'.format(label), n))\n",
    "for (label, n) in sorted(cmobjects.items(), key=lambda y: -y[0]):\n",
    "    print('{:<40}: {:>5}'.format('Clauses with {:>2} complements as objects'.format(label), n))\n",
    "\n",
    "print('{:<40}: {:>5}'.format('Clauses with a primary object', len(primdirectobjects)))\n",
    "print('{:<40}: {:>5}'.format('Clauses with a direct object', len(directobjects)))\n",
    "print('{:<40}: {:>5}'.format('Clauses with an implied object', len(directobjects_c)))\n",
    "print('{:<40}: {:>5}'.format('Clauses with a complement as object', sum(cmobjects.values())))\n",
    "print('{:<40}: {:>5}'.format('Total number of clauses', len(clause_verb)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complements: Indirect object or Locative?\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "## Cues for a locative complement\n",
    "\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``# h_loc`` how many H-locales are carried on words in the complement\n",
    "* ``body_part`` is 2 if the phrase starts with the preposition ``L`` followed by a body part, else 0. See remark below!\n",
    "* ``locativity`` ($loc$) a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``#topo``, ``# prep_b``, ``# h_loc`` and ``body_part``.\n",
    "\n",
    "### Note on body parts\n",
    "Often body parts are parts of idiomatic expressions.\n",
    "In those cases they do not necessarily form a locative cue.\n",
    "\n",
    "## Cues for an indirect object\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` or ``>L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` or ``>L`` plus proper name or person reference word occur in the complement\n",
    "* ``indirect object`` ($ind$) a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "\n",
    "## The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "The outcome is $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*)\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$\n",
    "\n",
    "In words:\n",
    "\n",
    "* if there are positive signals for L or I and none for the other, we choose the one for which there are positive signals;\n",
    "* if there are positive signals for both L and I, we follow the majority count, but only if the difference is at least two;\n",
    "* in all other cases we leave it at C: not necessarilty locative and not necessarily indirect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2h 33m 33s Determinig kind of complements\n",
      " 2h 33m 35s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases of kind L :  11360\n",
      "Phrases of kind C :   9700\n",
      "Phrases of kind I :   7453\n",
      "Total complements :  28513\n",
      "Total phrases     : 213193\n"
     ]
    }
   ],
   "source": [
    "msg('Determinig kind of complements')\n",
    "\n",
    "complements = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d('phrase', c):\n",
    "        nphrases += 1\n",
    "        pf = F.function.v(p)\n",
    "        if pf != 'Cmpl': continue\n",
    "        ncomplements += 1\n",
    "        words = L.d('word', p)\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements[c][ckind].append(p)\n",
    "\n",
    "msg('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    print('Phrases of kind {:<2}: {:>6}'.format(label, n))\n",
    "print('Total complements : {:>6}'.format(ncomplements))\n",
    "print('Total phrases     : {:>6}'.format(nphrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Applying the flowchart\n",
    "\n",
    "We can now apply the flowchart in a straightforward manner.\n",
    "\n",
    "We output the results as a stand-alone comma separated file, with these columns as specified in the code below.\n",
    "This file can be used to import into a spreadsheet and check results.\n",
    "\n",
    "We also provide a comma separated file that can be imported directly into SHEBANQ as a set of notes, so that the reader can check results within SHEBANQ. This has the benefit that the full context is available, and also data view can be called up easily to inspect the coding situation for each particular instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "status_rep = {\n",
    "    '*': 'note',\n",
    "    '!': 'good',\n",
    "    '?': 'warning',\n",
    "    '-': 'error',\n",
    "}\n",
    "stat_rep = {\n",
    "    '*': 'NB',\n",
    "    '!': '',\n",
    "    '?': 'wrn',\n",
    "    '-': 'err',\n",
    "}\n",
    "\n",
    "def reptext(label, phrases, num=False, txt=False, gl=False): \n",
    "    if phrases == None: return ''\n",
    "    label_rep = '{}='.format(label) if label else ''\n",
    "    phrases_rep = []\n",
    "    for p in sorted(phrases, key=NK):\n",
    "        ptext = '[{}|'.format(F.number.v(p) if num else '[')\n",
    "        if txt:\n",
    "            ptext += (''.join('{}{}'.format(\n",
    "                F.g_word_utf8.v(w),\n",
    "                F.trailer_utf8.v(w),\n",
    "            ) for w in L.d('word',p ))).replace('\\n','')\n",
    "        if gl:\n",
    "            wtexts = []\n",
    "            for w in L.d('word',p ):\n",
    "                g = F.gloss.v(w).replace('<object marker>','&')\n",
    "                prs = F.prs.v(w)\n",
    "                prs_g = pronominal_suffix.get(prs, (None, None))[1]\n",
    "                uvf = F.uvf.v(w)\n",
    "                wtext = ''\n",
    "                if uvf == 'H': ptext += 'toward '\n",
    "                wtext += g\n",
    "                wtext += ('~'+prs_g) if prs_g != None else ''\n",
    "                wtexts.append(wtext)\n",
    "            ptext += ' '.join(wtexts)\n",
    "        ptext += ']'\n",
    "        phrases_rep.append(ptext)\n",
    "    return ' '.join(phrases_rep)\n",
    "\n",
    "def ilc_info(inds, locs, cpls):\n",
    "    pinfos = []\n",
    "    for p in set(inds) | set(locs) | set(cpls):\n",
    "        (loca, indi, ckind) = complementk[p]\n",
    "        pinfos.append('[{}| L={} I={} => {}]'.format(F.number.v(p), loca, indi, ckind))\n",
    "    return ' '.join(pinfos)\n",
    "\n",
    "def flowchart(lex, verb, dos, pdo, sdos, inds, locs, cpls):\n",
    "    sense_label = None\n",
    "    n_dos = len(dos)\n",
    "    n_pdo = len(pdo)\n",
    "    n_sdos = len(sdos)\n",
    "    n_inds = len(inds)\n",
    "    n_locs = len(locs)\n",
    "    n_cpls = len(cpls)\n",
    "    na_cpls = n_inds + n_locs + n_cpls\n",
    "    ndo = ''\n",
    "    kcp = ''\n",
    "\n",
    "    if n_dos == 0: ndo = '0'\n",
    "    elif n_dos == 1: ndo = '1'\n",
    "    else: ndo = '2'\n",
    "    \n",
    "    if na_cpls == 0: kcp = '0'\n",
    "    elif n_inds: kcp = 'i'\n",
    "    elif n_locs: kcp = 'l'\n",
    "    else: kcp = 'c'\n",
    "    sense_label = ndo+kcp if ndo != '2' else '2'\n",
    "    \n",
    "    sinfo = senses.\\\n",
    "        get(lex, {lex: {'': ('-', 'no senses given for {}'.format(lex))}}).\\\n",
    "        get(sense_label, ('-', 'no sense {} given for {}'.format(sense_label, lex)))\n",
    "    status = sinfo[0]\n",
    "    sense_fmt = sinfo[1][0]\n",
    "    action_fmt = sinfo[1][1] if len(sinfo[1]) >= 2 else ''\n",
    "    action_stat = sinfo[1][2] if len(sinfo) >= 3 else status\n",
    "\n",
    "    verb_rep = reptext('', verb, num=True, gl=True)\n",
    "    pdo_rep = reptext('', pdo, num=True, gl=True)\n",
    "    sdos_rep = reptext('', sdos, num=True, gl=True)\n",
    "    inds_rep = reptext('', inds, num=True, gl=True)\n",
    "    locs_rep = reptext('', locs, num=True, gl=True)\n",
    "    cpls_rep = reptext('', cpls, num=True, gl=True)\n",
    "    ilc_rep = ''\n",
    "    if na_cpls: ilc_rep = ilc_info(inds, locs, cpls)\n",
    "    \n",
    "    sense_txt = sense_fmt.format(\n",
    "        verb=verb_rep, pdo=pdo_rep, sdos=sdos_rep, inds=inds_rep, locs=locs_rep, cpls=cpls_rep, ilc=ilc_rep,\n",
    "    )\n",
    "    action_txt = action_fmt.format(\n",
    "        verb=verb_rep, pdo=pdo_rep, sdos=sdos_rep, inds=inds_rep, locs=locs_rep, cpls=cpls_rep, ilc=ilc_rep,\n",
    "    )\n",
    "\n",
    "    return (sense_label, status, sense_txt, action_txt, action_stat)\n",
    "\n",
    "fields = '''\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    sentence#\n",
    "    clause#\n",
    "    lex\n",
    "    status\n",
    "    sense_label\n",
    "    sense\n",
    "    action_status\n",
    "    action\n",
    "    #do\n",
    "    #pdo\n",
    "    #sdos\n",
    "    #inds\n",
    "    #locs\n",
    "    #cpls\n",
    "    text\n",
    "'''.strip().split()\n",
    "\n",
    "sfields = '''\n",
    "    version\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    clause_atom\n",
    "    is_shared\n",
    "    is_published\n",
    "    status\n",
    "    keywords\n",
    "    ntext\n",
    "'''.strip().split()\n",
    "\n",
    "fields_fmt = ('{},' * (len(fields) - 1)) + '{}\\n' \n",
    "sfields_fmt = ('{}\\t' * (len(sfields) - 1)) + '{}\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the flowchart\n",
    "\n",
    "The next cell finally performs all the flowchart computations for all verbs in all contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2h 34m 53s Applying the flowchart\n",
      " 2h 34m 54s Done\n",
      " 2h 34m 54s Computed 5787 clauses with flowchart\n",
      " 2h 34m 54s Added notes for 3971 clauses with complement promotion candidates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action     notes: 2069\n",
      "prom       notes: 3971\n",
      "valence    notes: 5787\n",
      "Total      notes: 11827\n",
      "All lexemes with flowchart specification\n",
      "     Status   good   : 3677 clauses\n",
      "     Status   note   :  728 clauses\n",
      "     Status   error  :   44 clauses\n",
      "     Status   warning: 1338 clauses\n",
      "     All status      : 5787 clauses\n",
      "     Sense    00     :  562 clauses\n",
      "     Sense    00|10  :  264 clauses\n",
      "     Sense    0c     :  233 clauses\n",
      "     Sense    0c|10  :   11 clauses\n",
      "     Sense    0c|1c  :   73 clauses\n",
      "     Sense    0c|2   :    3 clauses\n",
      "     Sense    0i     :  309 clauses\n",
      "     Sense    0i|1i  :  173 clauses\n",
      "     Sense    0i|2   :    2 clauses\n",
      "     Sense    0l     :  179 clauses\n",
      "     Sense    0l|1l  :   53 clauses\n",
      "     Sense    10     : 1379 clauses\n",
      "     Sense    10|2   :   55 clauses\n",
      "     Sense    1c     :  638 clauses\n",
      "     Sense    1c|10  :    1 clauses\n",
      "     Sense    1c|2   :   33 clauses\n",
      "     Sense    1i     :  720 clauses\n",
      "     Sense    1i|2   :   30 clauses\n",
      "     Sense    1l     :  650 clauses\n",
      "     Sense    1l|2   :   21 clauses\n",
      "     Sense    2      :  389 clauses\n",
      "     Sense    2|2    :    9 clauses\n",
      "     All senses     : 5787 clauses\n",
      " \n",
      "<FH[\n",
      "     Status   good   : 1485 clauses\n",
      "     Status   note   :  409 clauses\n",
      "     Status   warning:  573 clauses\n",
      "     All status      : 2467 clauses\n",
      "     Sense    00     :  418 clauses\n",
      "     Sense    00|10  :  248 clauses\n",
      "     Sense    0c     :   77 clauses\n",
      "     Sense    0c|10  :    7 clauses\n",
      "     Sense    0c|1c  :   33 clauses\n",
      "     Sense    0c|2   :    1 clauses\n",
      "     Sense    0i     :   94 clauses\n",
      "     Sense    0i|1i  :   37 clauses\n",
      "     Sense    0l     :   43 clauses\n",
      "     Sense    0l|1l  :   31 clauses\n",
      "     Sense    10     :  939 clauses\n",
      "     Sense    10|2   :   34 clauses\n",
      "     Sense    1c     :  125 clauses\n",
      "     Sense    1c|2   :   10 clauses\n",
      "     Sense    1i     :  183 clauses\n",
      "     Sense    1i|2   :    5 clauses\n",
      "     Sense    1l     :   51 clauses\n",
      "     Sense    1l|2   :    3 clauses\n",
      "     Sense    2      :  128 clauses\n",
      "     All senses     : 2467 clauses\n",
      " \n",
      "BR>[\n",
      "     Status   good   :   34 clauses\n",
      "     Status   note   :    1 clauses\n",
      "     Status   error  :    1 clauses\n",
      "     All status      :   36 clauses\n",
      "     Sense    00     :    1 clauses\n",
      "     Sense    10     :   32 clauses\n",
      "     Sense    10|2   :    1 clauses\n",
      "     Sense    2      :    2 clauses\n",
      "     All senses     :   36 clauses\n",
      " \n",
      "CJT[\n",
      "     Status   good   :   40 clauses\n",
      "     Status   note   :    5 clauses\n",
      "     Status   error  :    3 clauses\n",
      "     Status   warning:   34 clauses\n",
      "     All status      :   82 clauses\n",
      "     Sense    00     :    1 clauses\n",
      "     Sense    0c     :    6 clauses\n",
      "     Sense    0c|10  :    1 clauses\n",
      "     Sense    0c|1c  :    2 clauses\n",
      "     Sense    0i     :    1 clauses\n",
      "     Sense    0l     :    1 clauses\n",
      "     Sense    10     :    9 clauses\n",
      "     Sense    10|2   :    1 clauses\n",
      "     Sense    1c     :   24 clauses\n",
      "     Sense    1i     :    4 clauses\n",
      "     Sense    1l     :   19 clauses\n",
      "     Sense    1l|2   :    1 clauses\n",
      "     Sense    2      :   12 clauses\n",
      "     All senses     :   82 clauses\n",
      " \n",
      "DBQ[\n",
      "     Status   good   :   25 clauses\n",
      "     Status   error  :    4 clauses\n",
      "     Status   warning:   10 clauses\n",
      "     All status      :   39 clauses\n",
      "     Sense    00     :    1 clauses\n",
      "     Sense    0c     :   10 clauses\n",
      "     Sense    0l     :   25 clauses\n",
      "     Sense    10     :    1 clauses\n",
      "     Sense    1c     :    1 clauses\n",
      "     Sense    1l     :    1 clauses\n",
      "     All senses     :   39 clauses\n",
      " \n",
      "FJM[\n",
      "     Status   good   :  306 clauses\n",
      "     Status   note   :   36 clauses\n",
      "     Status   warning:  235 clauses\n",
      "     All status      :  577 clauses\n",
      "     Sense    00     :   10 clauses\n",
      "     Sense    0c     :   29 clauses\n",
      "     Sense    0c|1c  :    2 clauses\n",
      "     Sense    0i     :    2 clauses\n",
      "     Sense    0i|1i  :    2 clauses\n",
      "     Sense    0l     :   21 clauses\n",
      "     Sense    0l|1l  :   12 clauses\n",
      "     Sense    10     :   66 clauses\n",
      "     Sense    10|2   :    3 clauses\n",
      "     Sense    1c     :  166 clauses\n",
      "     Sense    1c|10  :    1 clauses\n",
      "     Sense    1c|2   :    5 clauses\n",
      "     Sense    1i     :   38 clauses\n",
      "     Sense    1l     :  152 clauses\n",
      "     Sense    1l|2   :    4 clauses\n",
      "     Sense    2      :   57 clauses\n",
      "     Sense    2|2    :    7 clauses\n",
      "     All senses     :  577 clauses\n",
      " \n",
      "NTN[\n",
      "     Status   good   : 1281 clauses\n",
      "     Status   note   :  243 clauses\n",
      "     Status   error  :   36 clauses\n",
      "     Status   warning:  352 clauses\n",
      "     All status      : 1912 clauses\n",
      "     Sense    00     :   33 clauses\n",
      "     Sense    00|10  :   10 clauses\n",
      "     Sense    0c     :   55 clauses\n",
      "     Sense    0c|10  :    3 clauses\n",
      "     Sense    0c|1c  :   36 clauses\n",
      "     Sense    0c|2   :    2 clauses\n",
      "     Sense    0i     :   73 clauses\n",
      "     Sense    0i|1i  :  128 clauses\n",
      "     Sense    0i|2   :    2 clauses\n",
      "     Sense    0l     :   36 clauses\n",
      "     Sense    10     :  233 clauses\n",
      "     Sense    10|2   :   11 clauses\n",
      "     Sense    1c     :  297 clauses\n",
      "     Sense    1c|2   :   15 clauses\n",
      "     Sense    1i     :  454 clauses\n",
      "     Sense    1i|2   :   24 clauses\n",
      "     Sense    1l     :  395 clauses\n",
      "     Sense    1l|2   :   11 clauses\n",
      "     Sense    2      :   93 clauses\n",
      "     Sense    2|2    :    1 clauses\n",
      "     All senses     : 1912 clauses\n",
      " \n",
      "QR>[\n",
      "     Status   good   :  484 clauses\n",
      "     Status   note   :   34 clauses\n",
      "     Status   warning:  134 clauses\n",
      "     All status      :  652 clauses\n",
      "     Sense    00     :   76 clauses\n",
      "     Sense    00|10  :    6 clauses\n",
      "     Sense    0c     :   56 clauses\n",
      "     Sense    0i     :  139 clauses\n",
      "     Sense    0i|1i  :    6 clauses\n",
      "     Sense    0l     :   53 clauses\n",
      "     Sense    0l|1l  :   10 clauses\n",
      "     Sense    10     :   99 clauses\n",
      "     Sense    10|2   :    5 clauses\n",
      "     Sense    1c     :   25 clauses\n",
      "     Sense    1c|2   :    3 clauses\n",
      "     Sense    1i     :   41 clauses\n",
      "     Sense    1i|2   :    1 clauses\n",
      "     Sense    1l     :   32 clauses\n",
      "     Sense    1l|2   :    2 clauses\n",
      "     Sense    2      :   97 clauses\n",
      "     Sense    2|2    :    1 clauses\n",
      "     All senses     :  652 clauses\n",
      " \n",
      "ZQN[\n",
      "     Status   good   :   22 clauses\n",
      "     All status      :   22 clauses\n",
      "     Sense    00     :   22 clauses\n",
      "     All senses     :   22 clauses\n",
      " \n"
     ]
    }
   ],
   "source": [
    "msg('Applying the flowchart')\n",
    "\n",
    "outcome_sta = collections.Counter()\n",
    "outcome_lab = collections.Counter()\n",
    "outcome_sta_l = collections.defaultdict(lambda: collections.Counter())\n",
    "outcome_lab_l = collections.defaultdict(lambda: collections.Counter())\n",
    "\n",
    "of = outfile('valence_results.csv')\n",
    "ofs = outfile('valence_notes.csv')\n",
    "of.write('{}\\n'.format(','.join(fields)))\n",
    "ofs.write('{}\\n'.format('\\t'.join(sfields)))\n",
    "\n",
    "senses = {}\n",
    "senses_blocks = senses_spec.strip().split('\\n\\n')\n",
    "for b in senses_blocks:\n",
    "    lines = b.split('\\n')\n",
    "    verb = lines[0]+'['\n",
    "    sense_parts = [l.split(':', 2) for l in lines[1:]]\n",
    "    senses[verb] = dict(\n",
    "        (x[0].strip(), (x[1].strip(), [y.strip() for y in x[2].strip().split('::')])) for x in sense_parts\n",
    "    )\n",
    "\n",
    "nnotes = collections.Counter()\n",
    "\n",
    "for lex in senses:\n",
    "    if lex not in verb_clause:\n",
    "        msg('No verb {} in corpus'.format(lex))\n",
    "        continue\n",
    "    for (c,v) in verb_clause[lex]:\n",
    "        if F.vs.v(v) != 'qal': continue\n",
    "    \n",
    "        book = F.book.v(L.u('book', v))\n",
    "        chapter = F.chapter.v(L.u('chapter', v))\n",
    "        verse = F.verse.v(L.u('verse', v))\n",
    "        sentence_n = F.number.v(L.u('sentence', v))\n",
    "        clause_n = F.number.v(c)\n",
    "        clause_atom_n = F.number.v(L.u('clause_atom', v))\n",
    "        \n",
    "        verb = [L.u('phrase', v)]\n",
    "        dos = directobjects.get(c, set())\n",
    "        pdo = primdirectobjects.get(c, None)\n",
    "        pdo = set() if pdo == None else {pdo}\n",
    "        sdos = sorted(dos - pdo)\n",
    "        dos_can = directobjects_c.get(c, set())\n",
    "        dos_can_cpls = {p for p in dos_can if F.function.v(p) == 'Cmpl'}\n",
    "        dos_c = set()\n",
    "        pdo_c = set()\n",
    "        sdos_c = set()\n",
    "        if len(dos_can):\n",
    "            dos_can_lst = sorted(dos_can, key=NK)\n",
    "            dos_c = dos | dos_can\n",
    "            pdo_c = {dos_can_lst[0]}\n",
    "            sdos_c = sorted(dos | set(dos_can_lst[1:]), key=NK)\n",
    "\n",
    "        inds = complements.get(c, {}).get('I', [])\n",
    "        locs = complements.get(c, {}).get('L', [])\n",
    "        cpls = complements.get(c, {}).get('C', [])\n",
    "\n",
    "        (sense_label, status, sense_txt, action_txt, action_stat) = flowchart(\n",
    "            lex, verb, dos, pdo, sdos, inds, locs, cpls,\n",
    "        )\n",
    "        if len(dos_c):\n",
    "            inds_c = [p for p in inds if p not in dos_can_cpls]\n",
    "            locs_c = [p for p in locs if p not in dos_can_cpls]\n",
    "            cpls_c = [p for p in cpls if p not in dos_can_cpls]\n",
    "            (sense_label_c, status_c, sense_txt_c, action_txt_c, action_stat_c) = flowchart(\n",
    "                lex, verb, dos_c, pdo_c, sdos_c, inds_c, locs_c, cpls_c)\n",
    "            if status == '-' and status_c != '-':\n",
    "                status = status_c\n",
    "                sense_label = sense_label_c\n",
    "                sense_txt = sense_txt_c\n",
    "                action_txt = action_txt_c\n",
    "                action_stat = action_stat_c\n",
    "            elif status != '-' and status_c == '-':\n",
    "                pass # the values of the vars are OK\n",
    "            elif status != '-' and status_c != '-':\n",
    "                status = '*'\n",
    "                sense_label = sense_label+'|'+sense_label_c\n",
    "                sense_txt = '(A) '+sense_txt+' (B) '+sense_txt_c\n",
    "                action_txt = '(A) '+action_txt+' (B) '+action_txt_c\n",
    "\n",
    "        outcome_sta[status] += 1\n",
    "        outcome_sta_l[lex][status] += 1\n",
    "        outcome_lab[sense_label] += 1\n",
    "        outcome_lab_l[lex][sense_label] += 1\n",
    "        text = reptext('', L.d('phrase', c), num=True, txt=True)\n",
    "\n",
    "        of.write(fields_fmt.format(\n",
    "            book,\n",
    "            chapter,\n",
    "            verse,\n",
    "            sentence_n,\n",
    "            clause_n,\n",
    "            lex,\n",
    "            stat_rep[status],\n",
    "            sense_label,\n",
    "            sense_txt,\n",
    "            action_stat,\n",
    "            action_txt,\n",
    "            len(dos),\n",
    "            len(pdo),\n",
    "            len(sdos),\n",
    "            len(inds),\n",
    "            len(locs),\n",
    "            len(cpls),\n",
    "            text,\n",
    "        ))\n",
    "        ofs.write(sfields_fmt.format(\n",
    "            version,\n",
    "            book,\n",
    "            chapter,\n",
    "            verse,\n",
    "            clause_atom_n,\n",
    "            'T',\n",
    "            '',\n",
    "            status,\n",
    "            'valence'+(' val_{}'.format(stat_rep[status]) if status != '!' else ''),\n",
    "            '_{sl}_ [{nm}|{vb}] {st}'.format(\n",
    "                nm=F.number.v(L.u('phrase', v)),\n",
    "                vb=F.g_word_utf8.v(v),\n",
    "                st=sense_txt,\n",
    "                sl=sense_label,\n",
    "            ),\n",
    "        ))\n",
    "        nnotes['valence'] += 1\n",
    "        if action_txt != '':\n",
    "            ofs.write(sfields_fmt.format(\n",
    "                version,\n",
    "                book,\n",
    "                chapter,\n",
    "                verse,\n",
    "                clause_atom_n,\n",
    "                'T',\n",
    "                '',\n",
    "                action_stat,\n",
    "                'valence'+(' val_{}'.format(stat_rep[status]) if status != '!' else ''),\n",
    "                action_txt,\n",
    "            ))\n",
    "            nnotes['action'] += 1\n",
    "            \n",
    "# generate notes for the promotion candidates\n",
    "            \n",
    "for c in promotions:\n",
    "    w1 = L.d('word', c)[0]\n",
    "    book = F.book.v(L.u('book', w1))\n",
    "    chapter = F.chapter.v(L.u('chapter', w1))\n",
    "    verse = F.verse.v(L.u('verse', w1))\n",
    "    clause_atom_n = F.number.v(L.u('clause_atom', w1))\n",
    "    ps = reptext('', promotions[c], num=True, gl=True)\n",
    "        \n",
    "    ofs.write(sfields_fmt.format(\n",
    "        version,\n",
    "        book,\n",
    "        chapter,\n",
    "        verse,\n",
    "        clause_atom_n,\n",
    "        'T',\n",
    "        '',\n",
    "        '?',\n",
    "        'valence val_prom',\n",
    "        'Consider C => DO for {}'.format(ps),\n",
    "    ))\n",
    "    nnotes['prom'] += 1\n",
    "of.close()\n",
    "ofs.close()\n",
    "msg('Done')\n",
    "\n",
    "msg('Computed {} clauses with flowchart'.format(sum(outcome_sta.values())))\n",
    "msg('Added notes for {} clauses with complement promotion candidates'.format(len(promotions)))\n",
    "ntot = 0\n",
    "for (lab, n) in sorted(nnotes.items(), key=lambda x: x[0]):\n",
    "    ntot += n\n",
    "    print('{:<10} notes: {}'.format(lab, n))\n",
    "print('{:<10} notes: {}'.format('Total', ntot))\n",
    "\n",
    "for lex in [''] + sorted(senses):\n",
    "    print('All lexemes with flowchart specification' if lex == '' else lex)\n",
    "    src_sta = outcome_sta if lex == '' else outcome_sta_l.get(lex, {})\n",
    "    src_lab = outcome_lab if lex == '' else outcome_lab_l.get(lex, {})\n",
    "    tot = 0\n",
    "    for (x, n) in sorted(src_sta.items()):\n",
    "        tot += n\n",
    "        print('     Status   {:<7}: {:>4} clauses'.format(status_rep[x], n))\n",
    "    print('     All status      : {:>4} clauses'.format(tot))\n",
    "    tot = 0\n",
    "    for (x, n) in sorted(src_lab.items()):\n",
    "        tot += n\n",
    "        print('     Sense    {:<7}: {:>4} clauses'.format(x, n))\n",
    "    print('     All senses     : {:>4} clauses'.format(tot))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
