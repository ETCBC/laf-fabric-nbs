{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label.\n",
    "\n",
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "\n",
    "# 1. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.22\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  6.41s LOGFILE=/Users/dirk/Local/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  6.41s INFO: LOADING PREPARED data: please wait ... \n",
      "  6.41s prep prep: G.node_sort\n",
      "  6.52s prep prep: G.node_sort_inv\n",
      "  7.00s prep prep: L.node_up\n",
      "    10s prep prep: L.node_down\n",
      "    15s prep prep: V.verses\n",
      "    15s prep prep: V.books_la\n",
      "    15s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    17s INFO: LOADED PREPARED data\n",
      "    17s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-18T08-20-45\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, '--', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    24s Finding occurrences\n",
      "    25s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR> 48    occurrences\n",
      "CJT 85    occurrences\n",
      "QR> 743   occurrences\n"
     ]
    }
   ],
   "source": [
    "msg('Finding occurrences')\n",
    "occs = collections.defaultdict(list)\n",
    "for n in F.otype.s('word'):\n",
    "    lex = F.lex.v(n)\n",
    "    if lex.endswith('['):\n",
    "        lex = lex[0:-1]\n",
    "        occs[lex].append(n)\n",
    "msg('Done')\n",
    "for verb in sorted(verbs):\n",
    "    print('{} {:<5} occurrences'.format(verb, len(occs[verb])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to either a verb occurrence or to a phrase.\n",
    "\n",
    "Every row contains several identification fields:\n",
    "* node of the verb\n",
    "* node of the clause of the verb\n",
    "And in case of phrases, the node of the phrase is also added.\n",
    "Every row also contains book, chapter, verse.\n",
    "\n",
    "Since all rows have verb and clause node identifier, it is possible to sort all rows conveniently.\n",
    "\n",
    "The other fields per row are dependent on the type of row.\n",
    "* verb rows:\n",
    "  * verb lexeme\n",
    "  * verb occurrence\n",
    "  * clause text\n",
    "* phrase rows\n",
    "  * phrase node\n",
    "  * phrase text\n",
    "  * phrase function\n",
    "  * phrase function (corrected)\n",
    "  * phrase verbal valence\n",
    "  * phrase lexical characterisation\n",
    "  * phrase grammatical relation\n",
    "  * phrase semantic role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "pnode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_occurrence\n",
      "phrase_text\n",
      "function\n",
      "function_(corr)\n",
      "valence\n",
      "lexical\n",
      "grammatical\n",
      "semantical\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_occurrence\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    clause_text    \n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    phrase_text\n",
    "    function\n",
    "    function_(corr)\n",
    "    valence\n",
    "    lexical\n",
    "    grammatical\n",
    "    semantical\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = []\n",
    "for f in COMMON_FIELDS: field_names.append(f)\n",
    "for i in range(max((len(CLAUSE_FIELDS), len(PHRASE_FIELDS)))):\n",
    "    pf = PHRASE_FIELDS[i] if i < len(PHRASE_FIELDS) else '--'\n",
    "    field_names.append(pf)\n",
    "    \n",
    "fillrows = len(CLAUSE_FIELDS) - len(PHRASE_FIELDS)\n",
    "cfillrows = 0 if fillrows >= 0 else -fillrows\n",
    "pfillrows = fillrows if fillrows >= 0 else 0\n",
    "print('\\n'.join(field_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41m 23s Generated correction sheet for verb BRa_blank_etcbc4b.csv\n",
      "41m 23s Generated correction sheet for verb CJT_blank_etcbc4b.csv\n",
      "41m 23s Generated correction sheet for verb QRa_blank_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def vfile(verb, kind): return '{}_{}_{}{}.csv'.format(verb.replace('>','a').replace('<', 'o'), kind, source, version)\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vt)\n",
    "        clause_fields = (ct,)\n",
    "        rows.append(common_fields + clause_fields + (('',)*cfillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vt)\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            phrase_fields = (pt, pf) + (('',)*5)\n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "    filename = vfile(verb, 'blank')\n",
    "    row_file = outfile(filename)\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "Then we turn that data into LAF annotations. Every correction is stored in a new feature, with name `function`, label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT: Found     6 corrections in /Users/dirk/laf-fabric-data/cpl/CJT_corrected_etcbc4b.csv\n",
      "NO file /Users/dirk/laf-fabric-data/cpl/QRa_corrected_etcbc4b.csv\n",
      "NO file /Users/dirk/laf-fabric-data/cpl/BRa_corrected_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def read_corr(rootdir):\n",
    "    results = []\n",
    "    for verb in verbs:\n",
    "        filename = '{}/{}'.format(rootdir, vfile(verb, 'corrected'))\n",
    "        if not os.path.exists(filename):\n",
    "            print('NO file {}'.format(filename))\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[4*i], fields[4*i+3])\n",
    "                    pc = pc.strip()\n",
    "                    if pc != '': results.append((int(pn), pc))\n",
    "        print('{}: Found {:>5} corrections in {}'.format(verb, len(results), filename))\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement corrections', 'date': '2016-02'},\n",
    "    [\n",
    "        ('cpl', 'complements', read_corr, (\n",
    "            ('JanetDyk', 'ft', 'function'),\n",
    "        ))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Check the corrections\n",
    "\n",
    "We load the corrections into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "We perform a few basic consistency checks.\n",
    "\n",
    "# 2.1 Annox complements\n",
    "Load the API again, but note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s USING annox DATA COMPILED AT: 2016-03-11T07-47-07\n",
      "  1.04s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.04s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.04s INFO: LOADING PREPARED data: please wait ... \n",
      "  1.04s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  1.68s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.68s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  3.11s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  3.11s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  3.11s INFO: LOADED PREPARED data\n",
      "  3.11s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-14T14-21-18\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        JanetDyk:ft.function etcbc4:ft.function\n",
    "    ''',\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Checks\n",
    "\n",
    "We make sure that every correction applies to a node that corresponds to a phrase, and that no correction applies to the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    16s Checking corrections\n",
      "    17s Found 3 types of corrections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('Cmpl', 'Loca'): 3, ('Adju', 'Benf'): 2, ('Supp', 'Adju-Benf'): 1})\n",
      "Cmpl  => Loca      3 x\n",
      "Adju  => Benf      2 x\n",
      "Supp  => Adju-Benf     1 x\n",
      "NO ERRORS DETECTED\n"
     ]
    }
   ],
   "source": [
    "msg('Checking corrections')\n",
    "corr = collections.Counter()\n",
    "errors = collections.Counter()\n",
    "corrected_nodes = set()\n",
    "for n in NN():\n",
    "    c = F.JanetDyk_ft_function.v(n)\n",
    "    if c != None:\n",
    "        if F.otype.v(n) != 'phrase': errors['Correction applied to non-phrase object'] += 1\n",
    "        if n in corrected_nodes: errors['Phrase with multiple corrections'] += 1\n",
    "        o = F.etcbc4_ft_function.v(n) or ''\n",
    "        corr[(o, c)] += 1\n",
    "        corrected_nodes.add(n)\n",
    "    \n",
    "msg('Found {} types of corrections'.format(len(corr)))\n",
    "print(corr)\n",
    "for ((o, c), n) in sorted(corr.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print('{:<5} => {:<5} {:>5} x'.format(o, c, n))\n",
    "if not errors:\n",
    "    print('NO ERRORS DETECTED')\n",
    "else:\n",
    "    print('THERE ARE ERRORS:')\n",
    "    for (e, n) in sorted(errors.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print('{:>5} x {}'.format(n, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional observations\n",
    "\n",
    "## Phrases with function ``EPPr``\n",
    "\n",
    "There are 9 occurrences, of which 8 in Aramaic text.\n",
    "In all cases the phrase functions as a copula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel 2:9 HJ>  :: XDH&HJ> DTKWN \n",
      "Daniel 2:38 HW>  :: >NT&HW> R>#H DJ DHB>00\n",
      "Daniel 2:47 HW>  :: DJ >LHKWN HW> >LH >LHJN WMR> MLKJN \n",
      "Daniel 3:15 HW>  :: WMN&HW> >LH \n",
      "Daniel 4:27 HJ>  :: HL> D>&HJ> BBL RBT> \n",
      "Daniel 5:13 HW>  :: >NT&HW> DNJ>L \n",
      "Daniel 6:17 HW>  :: >LHK HW> J#JZBNK00\n",
      "Ezra 5:11 HMW  :: >NXN> HMW <BDWHJ DJ&>LH #MJ> W>R<> \n",
      "2_Chronicles 20:6 HW>  :: HL> >TH&HW> >LHJM B#MJM \n"
     ]
    }
   ],
   "source": [
    "for p in F.otype.s('phrase'):\n",
    "    if F.function.v(p) != 'EPPr': continue\n",
    "    words = L.d('word', p)\n",
    "    first_word = words[0]\n",
    "    b = L.u('book', first_word)\n",
    "    v = L.u('verse', first_word)\n",
    "    c = L.u('clause', first_word)\n",
    "    passage = '{} {}:{}'.format(\n",
    "        T.book_name(b, lang='en'), \n",
    "        F.chapter.v(v),\n",
    "        F.verse.v(v),\n",
    "    )\n",
    "    pt = T.words(L.d('word', p), fmt='ec').replace('\\n', '')\n",
    "    ct = T.words(L.d('word', c), fmt='ec').replace('\\n', '')\n",
    "    print('{} {} :: {}'.format(passage, pt, ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
