{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.6.2\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os, collections\n",
    "from copy import deepcopy\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  5.86s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  5.86s INFO: LOADING PREPARED data: please wait ... \n",
      "  5.86s prep prep: G.node_sort\n",
      "  5.97s prep prep: G.node_sort_inv\n",
      "  6.50s prep prep: L.node_up\n",
      "  9.79s prep prep: L.node_down\n",
      "    15s prep prep: V.verses\n",
      "    15s prep prep: V.books_la\n",
      "    15s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    18s INFO: LOADED PREPARED data\n",
      "    18s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-05-27T11-31-57\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, '--', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex uvf\n",
    "        function\n",
    "        chapter verse\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        msg('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this phrase is part of another phrase and does not merit its own function value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.33s {'function': {'Subj', 'PrAd', 'NCoS', 'Conj', 'Exst', 'EPPr', 'PtcO', 'Modi', 'NCop', 'Supp', 'Objc', 'BoundErr', 'ModS', 'PreS', 'Loca', 'Frnt', 'Intj', 'Ques', 'Nega', 'Voct', 'Cmpl', 'PreO', 'PrcS', 'PreC', 'ExsS', 'Adju', 'Pred', 'Time', 'IntS', 'Rela'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "inf('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.36s Finding occurrences ...\n",
      "  3.03s Done\n",
      "Total:     73679 verb occurrences in 70131 clauses\n",
      "Selected:  16209 verb occurrences in 16053 clauses\n",
      "<BR 556   occurrences\n",
      "<FH 2629  occurrences\n",
      "<LH 890   occurrences\n",
      "BR> 54    occurrences\n",
      "BW> 2570  occurrences\n",
      "CJT 85    occurrences\n",
      "CWB 1056  occurrences\n",
      "FJM 609   occurrences\n",
      "HLK 1554  occurrences\n",
      "JRD 377   occurrences\n",
      "JY> 1069  occurrences\n",
      "NF> 656   occurrences\n",
      "NPL 445   occurrences\n",
      "NTN 2017  occurrences\n",
      "NWS 159   occurrences\n",
      "PQD 303   occurrences\n",
      "QR> 883   occurrences\n",
      "SWR 297   occurrences\n"
     ]
    }
   ],
   "source": [
    "inf('Finding occurrences ...')\n",
    "occs = collections.defaultdict(list)           # dictionary of all verb occurrence nodes per verb lexeme\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of all verb occurrence nodes per clause node\n",
    "clause_verb_selected = collections.defaultdict(list) # idem but for the occurrences of selected verbs\n",
    "\n",
    "nw = 0\n",
    "nws = 0\n",
    "for n in F.otype.s('word'):\n",
    "    if F.sp.v(n) != 'verb': continue\n",
    "    nw += 1\n",
    "    lex = F.lex.v(n).rstrip('/=[')\n",
    "    occs[lex].append(n)\n",
    "    cn = L.u('clause', n)\n",
    "    clause_verb[cn].append(n)\n",
    "    if lex in verbs:\n",
    "        nws += 1\n",
    "        clause_verb_selected[cn].append(n)\n",
    "\n",
    "inf('Done')\n",
    "inf('Total:    {:>6} verb occurrences in {} clauses'.format(nw, len(clause_verb)), withtime=False)\n",
    "inf('Selected: {:>6} verb occurrences in {} clauses'.format(nws, len(clause_verb_selected)), withtime=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    inf('{} {:<5} occurrences'.format(verb, len(occs[verb])), withtime=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 15s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CJT_etcbc4b.csv\n",
      "23m 15s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/QRa_etcbc4b.csv\n",
      "23m 15s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/FJM_etcbc4b.csv\n",
      "23m 15s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NTN_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oBR_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BWa_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NPL_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NWS_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/PQD_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NFa_etcbc4b.csv\n",
      "23m 16s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oLH_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oFH_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CWB_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/SWR_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JRD_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/HLK_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JYa_etcbc4b.csv\n",
      "23m 17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BRa_etcbc4b.csv\n",
      "23m 17s 52110  phrases seen 1  time(s)\n",
      "23m 17s 181    phrases seen 2  time(s)\n",
      "23m 17s 9      phrases seen 3  time(s)\n",
      "23m 17s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = T.passage(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        row = [cln, wn, passage_label, lnx, vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            row.extend((pn, pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oBR_etcbc4b.csv\n",
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "15m 46s BR>: Found     2 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "15m 46s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 46s OK: all corrected nodes where phrase nodes\n",
      "15m 46s OK: all corrected values are legal\n",
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "15m 46s BW>: Found    57 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "15m 46s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 46s OK: all corrected nodes where phrase nodes\n",
      "15m 46s OK: all corrected values are legal\n",
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "15m 46s CJT: Found    60 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "15m 46s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 46s OK: all corrected nodes where phrase nodes\n",
      "15m 46s OK: all corrected values are legal\n",
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "15m 46s CWB: Found   110 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "15m 46s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 46s OK: all corrected nodes where phrase nodes\n",
      "15m 46s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "15m 46s HLK: Found   269 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "15m 46s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 46s OK: all corrected nodes where phrase nodes\n",
      "15m 46s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JRD_etcbc4b.csv\n",
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JYa_etcbc4b.csv\n",
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      "15m 46s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "15m 47s NTN: Found   336 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "15m 47s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 47s OK: all corrected nodes where phrase nodes\n",
      "15m 47s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15m 47s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      "15m 47s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 47s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "15m 47s QR>: Found   339 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "15m 47s OK: Corrected phrases did not receive multiple corrections\n",
      "15m 47s OK: all corrected nodes where phrase nodes\n",
      "15m 47s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15m 47s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 47s Found 339 corrections in the phrase function\n",
      "15m 47s 26090  phrases seen 1  time(s)\n",
      "15m 47s 33     phrases seen 2  time(s)\n",
      "15m 47s Total phrases seen: 26123\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            msg('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            inf('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        inf('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                msg('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            inf('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            inf('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            msg('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            inf('OK: all corrected values are legal')\n",
    "    inf('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 00s Enrich field specification OK\n",
      "valence = {adjunct, complement, core}\n",
      "grammatical = {*, copula, copula+subject, object, predication, predication+object, predication+subject, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, location, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "grammatical\n",
    "    *\n",
    "    subject\n",
    "    object\n",
    "    copula\n",
    "    copula+subject\n",
    "    predication\n",
    "    predication+subject\n",
    "    predication+object\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "if None in enrich_fields:\n",
    "    msg('Invalid enrich field specification')\n",
    "else:\n",
    "    inf('Enrich field specification OK')\n",
    "for ef in enrich_fields:\n",
    "    print('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specs = '''\n",
    "Adju\tAdjunct\tadjunct\tNA\t\t\n",
    "Cmpl\tComplement\tcomplement\t*\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula+subject\t\t\n",
    "Exst\tExistence\tcore\tcopula\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tsubject\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tsubject\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula+subject\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\tNA\n",
    "Objc\tObject\tcomplement\tobject\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tpredication+subject\t\t\n",
    "PreC\tPredicate complement\tcore\tpredication\t\t\n",
    "Pred\tPredicate\tcore\tpredication\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tpredication+object\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tpredication+subject\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tpredication+object\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\tNA\n",
    "Subj\tSubject\tcore\tsubject\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\tNA'''.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 02s Defaults OK (124 good)\n"
     ]
    }
   ],
   "source": [
    "transform = {}\n",
    "for line in specs:\n",
    "    x = line.split('\\t') \n",
    "    transform[x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "for e in error_values['function']:\n",
    "    transform[e] = dict(zip(enrich_fields, ['NA']*4))\n",
    "\n",
    "errors = 0\n",
    "good = 0\n",
    "for f in transform:\n",
    "    for e in enrich_fields:\n",
    "        val = transform[f][e]\n",
    "        if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "            msg('Defaults for `{}`: wrong `{}` value: \"{}\"'.format(f, e, val))\n",
    "            errors += 1\n",
    "        else: good += 1\n",
    "if errors:\n",
    "    msg('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    inf('Defaults OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence         grammatical          lexical         semantic       \n",
      "Adju    : adjunct         NA                                                  \n",
      "BoundErr: NA              NA                   NA              NA             \n",
      "Cmpl    : complement      *                                                   \n",
      "Conj    : NA              NA                   NA              NA             \n",
      "EPPr    : NA              copula                                              \n",
      "ExsS    : core            copula+subject                                      \n",
      "Exst    : core            copula                                              \n",
      "Frnt    : NA              NA                   NA              NA             \n",
      "IntS    : core            subject                                             \n",
      "Intj    : NA              NA                   NA              NA             \n",
      "Loca    : adjunct         NA                   location        location       \n",
      "ModS    : core            subject                                             \n",
      "Modi    : NA              NA                   NA              NA             \n",
      "NCoS    : core            copula+subject                                      \n",
      "NCop    : core            copula                                              \n",
      "Nega    : NA              NA                   NA              NA             \n",
      "Objc    : complement      object                                              \n",
      "PrAd    : adjunct         NA                                                  \n",
      "PrcS    : core            predication+subject                                 \n",
      "PreC    : core            predication                                         \n",
      "PreO    : core            predication+object                                  \n",
      "PreS    : core            predication+subject                                 \n",
      "Pred    : core            predication                                         \n",
      "PtcO    : core            predication+object                                  \n",
      "Ques    : NA              NA                   NA              NA             \n",
      "Rela    : NA              NA                   NA              NA             \n",
      "Subj    : core            subject                                             \n",
      "Supp    : adjunct         NA                                   benefactive    \n",
      "Time    : adjunct         NA                   time            time           \n",
      "Unkn    : NA              NA                   NA              NA             \n",
      "Voct    : NA              NA                   NA              NA             \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: {:<15} {:<20} {:<15} {:<15}'\n",
    "print(ltpl.format('func', *enrich_fields))\n",
    "for f in sorted(transform):\n",
    "    sfs = transform[f]\n",
    "    print(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 1 and F.lex.v(words[1] in personal_lexemes)\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases : value, which is true iff that feature has that value for the phrase in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_logic = {\n",
    "    'CJT': [\n",
    "        (\n",
    "            ('semantic', 'benefactive'), \n",
    "            ('function:Adju', has_L, is_lex_personal),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('function:Cmpl', has_H_locale),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('semantic', 'location'),\n",
    "            ('function:Cmpl', is_lex_local),\n",
    "        ),\n",
    "    ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 07s CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      "30m 07s CJT-2 if function   = Cmpl     AND has_H_locale   \n",
      "      lexical    => location       \n",
      "\n",
      "30m 07s CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      "30m 07s All 3 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    result = '{}-{} '.format(vl, i+1)\n",
    "    pref_len = len(result)\n",
    "    result += 'if {}\\n'.format(' AND '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        result += '{}{:<10} => {:<15}\\n'.format(' '* pref_len, *sfa)\n",
    "    return result\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for vl in sorted(enrich_logic):\n",
    "        for items in enrich_logic[vl]:\n",
    "            rule_index[vl].append((items[-1], items[0:-1]))\n",
    "        for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "            inf(rule_as_str(vl, i))\n",
    "            nrules += 1\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                if sf not in enrich_fields:\n",
    "                    msg('\"{}\" not a valid enrich field'.format(sf), withtime=False)\n",
    "                    errors += 1\n",
    "                elif sfval not in enrich_fields[sf]:\n",
    "                    msg('`{}`: \"{}\" not a valid enrich field value'.format(sf, sfval), withtime=False)\n",
    "                    errors += 1\n",
    "            for c in conditions:\n",
    "                if type(c) == str:\n",
    "                    x = c.split(':')\n",
    "                    if len(x) != 2:\n",
    "                        msg('Wrong feature condition {}'.format(c))\n",
    "                        errors += 1\n",
    "                    else:\n",
    "                        (feat, val) = x\n",
    "                        if feat not in legal_values:\n",
    "                            msg('Feature `{}` not in use'.format(feat))\n",
    "                            errors += 1\n",
    "                        elif val not in legal_values[feat]:\n",
    "                            msg('Feature `{}`: not a valid value \"{}\"'.format(feat, val))\n",
    "                            errors += 1\n",
    "    if errors:\n",
    "        msg('There were {} errors in {} rules'.format(errors, nrules))\n",
    "    else:\n",
    "        inf('All {} rules OK'.format(nrules))\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "applied_cases = {}\n",
    "\n",
    "def apply_logic(vl, pn, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    verb_rules = enrich_logic.get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                this_ok = F.item[feature].v(pn) == value\n",
    "            else:\n",
    "                this_ok = condition(vl, pn)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            applied_cases.setdefault((vl, i), []).append(pn)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "pnode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "link\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "phrase_text\n",
      "function\n",
      "valence\n",
      "grammatical\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    link\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    clause_text    \n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    phrase_text\n",
    "    function\n",
    "'''.strip().split() + list(enrich_fields)\n",
    "\n",
    "field_names = []\n",
    "for f in COMMON_FIELDS: field_names.append(f)\n",
    "for i in range(max((len(CLAUSE_FIELDS), len(PHRASE_FIELDS)))):\n",
    "    pf = PHRASE_FIELDS[i] if i < len(PHRASE_FIELDS) else '--'\n",
    "    field_names.append(pf)\n",
    "    \n",
    "fillrows = len(CLAUSE_FIELDS) - len(PHRASE_FIELDS)\n",
    "cfillrows = 0 if fillrows >= 0 else -fillrows\n",
    "pfillrows = fillrows if fillrows >= 0 else 0\n",
    "print('\\n'.join(field_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30m 11s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CJT_etcbc4b.csv (375 rows)\n",
      "30m 11s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/QRa_etcbc4b.csv (3662 rows)\n",
      "30m 11s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/FJM_etcbc4b.csv (2857 rows)\n",
      "30m 12s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NTN_etcbc4b.csv (9659 rows)\n",
      "30m 12s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oBR_etcbc4b.csv (2309 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BWa_etcbc4b.csv (10817 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NPL_etcbc4b.csv (1915 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NWS_etcbc4b.csv (613 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/PQD_etcbc4b.csv (1269 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NFa_etcbc4b.csv (2826 rows)\n",
      "30m 13s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oLH_etcbc4b.csv (3821 rows)\n",
      "30m 14s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oFH_etcbc4b.csv (11048 rows)\n",
      "30m 14s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CWB_etcbc4b.csv (4238 rows)\n",
      "30m 14s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/SWR_etcbc4b.csv (1259 rows)\n",
      "30m 14s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JRD_etcbc4b.csv (1553 rows)\n",
      "30m 15s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/HLK_etcbc4b.csv (5672 rows)\n",
      "30m 15s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JYa_etcbc4b.csv (4495 rows)\n",
      "30m 15s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BRa_etcbc4b.csv (219 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 15s Done\n",
      "2 rules applied\n",
      "CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      "\t   5 phrases: 615130, 630648, 712015, 794512, 797440\n",
      "\n",
      "CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      "\t   7 phrases: 606396, 619338, 630956, 654145, 776266, 789542, 797377\n",
      "\n",
      "12 applications in total\n",
      "30m 15s 52110  phrases seen 1  time(s)\n",
      "30m 15s 181    phrases seen 2  time(s)\n",
      "30m 15s 9      phrases seen 3  time(s)\n",
      "30m 15s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cl = L.u('clause', wn)\n",
    "        if cl in clauses_seen: continue\n",
    "        clauses_seen.add(cl)\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, lnx, vl, vstem, vt)\n",
    "        clause_fields = (ct,)\n",
    "        rows.append(common_fields + clause_fields + (('',)*cfillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            phrases_seen[pn] += 1\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vt)\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = pf_corr.get(pn, None) or F.function.v(pn)\n",
    "            phrase_fields = (pt, pf) + apply_logic(vl, pn, transform[pf])            \n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated enrichment sheet for verb {} ({} rows)'.format(filename, len(rows)))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "inf('Done')\n",
    "print('{} rules applied'.format(len(applied_cases)))\n",
    "totaln = 0\n",
    "for rule_id in applied_cases:\n",
    "    cases = applied_cases[rule_id]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    print('{}\\n\\t{:>4} phrases: {}\\n'.format(rule_as_str(*rule_id), n, ', '.join(str(c) for c in cases[0:10])))\n",
    "print('{} applications in total'.format(totaln))\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t90243, 93571, 29637, 284965, 289859, 136745, 257293, 289871, 154354, 154964, 9525, 257016, 284989, 93598\n",
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t26118, 26127, 146447, 187920, 197138, 272406, 95257, 184350, 398368, 289826, 201253, 24616, 78897, 401459, 100410, 32829, 100413, 198208, 5698, 200258, 100938, 24653, 141902, 112207, 186960, 24658, 196690, 28764, 34400, 298594, 248931, 132198, 162918, 12402, 5747, 146044, 396927, 153216, 134792, 151176, 188042, 97419, 426120, 257165, 136338, 21656, 162970, 200349, 214687, 24740, 257192, 158378, 100527, 25777, 160434, 214707, 4789, 4793, 272569, 139963, 90812, 249020, 38595, 113861, 138448, 8920, 282841, 19166, 20703, 26850, 43235, 145127, 8424, 8937, 170729, 397032, 254703, 154354, 200948, 426230, 176376, 79609, 165626, 206075, 208636, 27391, 269569, 106246, 157447, 26380, 149785, 170782, 211232, 126758, 26414, 27438, 246062, 109363, 172340, 249140, 398134, 64828, 26431, 16704, 4929, 168771, 154964, 132955, 393569, 47460, 157541, 47466, 100206, 37232, 269170, 23415, 410999, 23933, 24448, 78208, 133518, 25999, 191381, 12698, 19355, 24476, 170909, 18344, 157608, 267689, 244660, 256952, 8633, 63419, 167359, 175553, 138694, 110536, 175561, 108490, 111051, 143820, 37324, 192973, 264137, 5586, 99795, 11732, 170963, 20438, 218583, 269285, 25062, 110576, 26099, 184315, 256511\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t154354, 322818, 154964, 75702\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u('clause', w)\n",
    "        ps = L.d('phrase', c)\n",
    "        phs = {p for p in ps if len({w for w in L.d('word', p) if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or F.function.v(p) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        print('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])))\n",
    "        if show_results: print('\\t{}'.format(', '.join(str(x) for x in hl[f])))\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oBR_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oFH_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oLH_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BRa_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BWa_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CJT_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CWB_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/FJM_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/HLK_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JRD_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JYa_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NPL_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NTN_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NWS_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/QRa_etcbc4b.csv\n",
      "21m 12s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 12s OK: The used blank enrichment sheets have legal values\n",
      "21m 12s OK: The used blank enrichment sheets are consistent\n",
      "21m 12s OK: The used filled enrichment sheets have legal values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exodus 30:12 Conflict in filled: 632989 = K.IJ  in K.IJ TIF.@> >ET&RO>C B.:N;J&JIF:R@>;L LIP:QUD;JHEm :\n",
      "\tNA, NA, NA, location                     in verb(s) NF>\n",
      "\tNA, NA, NA, time                         in verb(s) PQD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 12s OK: all enriched nodes where phrase nodes\n",
      "21m 12s OK: all enriched nodes occurred in the blank sheet\n",
      "21m 12s OK: there are 344 manual correction/enrichment annotations\n",
      "COR                                    => 2_Kings 22:6 726393 NA,NA,NA,NA\n",
      "\tLEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "\t\tW:JIT.:NW. >OTOW L:<OF;J HAM.:L@>K@H LEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "COR                                    => Genesis 5:2 606420 complement,object,,\n",
      "\tZ@K@R W.N:Q;B@H \n",
      "\t\tZ@K@R W.N:Q;B@H B.:R@>@m \n",
      "COR                                    => Psalms 89:48 799849 complement,object,,\n",
      "\tC.@W:> \n",
      "\t\t<AL&MAH&C.@W:> B.@R@>T@ K@L&B.:N;J&>@D@m00\n",
      "\n",
      "COR                                    => Genesis 45:25 621386 NA,NA,NA,NA\n",
      "\t>EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "\t\tWAJ.@BO>W. >EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "COR                                    => Exodus 14:16 627778 complement,*,,\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tW:J@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H00\n",
      "\n",
      "COR                                    => Exodus 14:22 627852 complement,*,,\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tWAJ.@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H \n",
      "COR                                    => Leviticus 4:23 637127 NA,NA,NA,NA\n",
      "\tF:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "\t\tW:H;BIJ> >ET&Q@R:B.@NOW F:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "COR                                    => Leviticus 4:28 637188 NA,NA,NA,NA\n",
      "\tF:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H \n",
      "\t\tW:H;BIJ> Q@R:B.@NOW F:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H <AL&XAV.@>TOW \n",
      "COR                                    => Leviticus 4:32 637235 adjunct,NA,,\n",
      "\tQ@R:B.@NOW L:XAV.@>T \n",
      "\t\tW:>Im&K.EBEF J@BIJ> Q@R:B.@NOW L:XAV.@>T \n",
      "COR                                    => Leviticus 5:6 637373 adjunct,NA,,\n",
      "\tN:Q;B@H MIn&HAY.O>n K.IF:B.@H >OW&F:<IJRAT <IZ.IJm \n",
      "\t\tW:H;BIJ> >ET&>:AC@MOW LAJHW@H <AL XAV.@>TOW N:Q;B@H MIn&HAY.O>n K.IF:B.@H >OW&F:<IJRAT <IZ.IJm L:XAV.@>T \n",
      "... AND 334 ANNOTATIONS MORE\n",
      "21m 14s 49035  phrases seen 1  time(s)\n",
      "21m 14s 3193   phrases seen 2  time(s)\n",
      "21m 14s 65     phrases seen 3  time(s)\n",
      "21m 14s 7      phrases seen 4  time(s)\n",
      "21m 14s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def read_enrich(rootdir): # rootdir will not be used, data is computed from sheets\n",
    "    pf_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_phrase = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                msg('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            with open(filename) as f:\n",
    "                header = f.__next__()\n",
    "                for line in f:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    pn = int(fields[2])\n",
    "                    if pn < 0: continue\n",
    "                    phrases_seen[pn] += 1\n",
    "                    vvals = tuple(fields[-4:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][pn].append((verb, f, v))\n",
    "                    vresults[is_filled][pn] = vvals\n",
    "                    if pn in pf_enriched[is_filled]:\n",
    "                        if pn not in repeated[is_filled]:\n",
    "                            repeated[is_filled][pn] = [pf_enriched[is_filled][pn]]\n",
    "                        repeated[is_filled][pn].append((verb, vvals))\n",
    "                    else:\n",
    "                        pf_enriched[is_filled][pn] = (verb, vvals)\n",
    "                    if F.otype.v(pn) != 'phrase': \n",
    "                        non_phrase[pn].append(verb)\n",
    "            for pn in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if pn not in vresults[False]:\n",
    "                    wrong_node[pn].append(verb)\n",
    "            for pn in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                f_corr = pn in pf_corr                               # manual correction in phrase function\n",
    "                s_manual = pn in vresults[True] and vresults[False][pn] != vresults[True][pn] # real change\n",
    "                these_results = vresults[True][pn] if s_manual else vresults[False][pn]\n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((pn,)+these_results+(f_corr, s_manual))\n",
    "                results.append((pn,)+these_results+(f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                msg('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), withtime=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    msg('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), withtime=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        else:\n",
    "            inf('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        msg('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), withtime=False)\n",
    "                        for vals in overview:\n",
    "                            msg('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), withtime=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        inf('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), withtime=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        if nerrors == 0:\n",
    "            inf('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Enrichments have been applied to non-phrase nodes:')\n",
    "        for x in sorted(non_phrase)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} Node {} is not a phrase but a {}'.format(\n",
    "                non_phrase[x], T.passage(x), x, F.otype.v(x),\n",
    "            ), withtime=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        msg('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} node {}'.format(\n",
    "                non_phrase[x], T.passage(x), x,\n",
    "            ), withtime=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        inf('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_corr, s_manual) = r\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "            inf('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                T.passage(x), x, ','.join(vals), px, cx\n",
    "            ), withtime=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: inf('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        msg('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-06'},\n",
    "    [\n",
    "        (None, 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in list(enrich_fields.keys())+['f_correction', 's_manual']\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Annox complements\n",
    "We load the s into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in ``enrich_fields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.00s DETAIL: load main: X. [node]  -> \n",
      "  1.66s DETAIL: load main: X. [e]  -> \n",
      "  3.89s DETAIL: load main: G.node_anchor_min\n",
      "  3.98s DETAIL: load main: G.node_anchor_max\n",
      "  4.08s DETAIL: load main: G.node_sort\n",
      "  4.18s DETAIL: load main: G.node_sort_inv\n",
      "  4.83s DETAIL: load main: G.edges_from\n",
      "  4.96s DETAIL: load main: G.edges_to\n",
      "  5.10s LOGFILE=/Users/dirk/laf/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  5.10s PARSING ANNOTATION FILES\n",
      "  5.74s INFO: parsing complements.xml\n",
      "  8.13s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "     52300 good   annots   and     0 faulty ones\n",
      "    313800 good   features and     0 faulty ones\n",
      "     52300 distinct xml identifiers\n",
      "\n",
      "  8.13s MODELING RESULT FILES\n",
      "  8.13s INFO: CONNECTIVITY\n",
      "  8.28s WRITING RESULT FILES for a\n",
      "  8.28s DETAIL: write annox: F.JanetDyk_ft_f_correction [node] \n",
      "  8.35s DETAIL: write annox: F.JanetDyk_ft_grammatical [node] \n",
      "  8.42s DETAIL: write annox: F.JanetDyk_ft_lexical [node] \n",
      "  8.46s DETAIL: write annox: F.JanetDyk_ft_s_manual [node] \n",
      "  8.53s DETAIL: write annox: F.JanetDyk_ft_semantic [node] \n",
      "  8.57s DETAIL: write annox: F.JanetDyk_ft_valence [node] \n",
      "  8.64s DETAIL: write annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  8.65s DETAIL: write annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  8.65s DETAIL: write annox: F.etcbc4_ph_phono [node] \n",
      "  9.42s DETAIL: write annox: F.etcbc4_ph_phono_sep [node] \n",
      "  9.60s END   COMPILE a: complements\n",
      "  9.60s USING annox DATA COMPILED AT: 2016-05-27T12-45-43\n",
      "  9.60s DETAIL: keep main: G.node_anchor_min\n",
      "  9.60s DETAIL: keep main: G.node_anchor_max\n",
      "  9.60s DETAIL: keep main: G.node_sort\n",
      "  9.60s DETAIL: keep main: G.node_sort_inv\n",
      "  9.60s DETAIL: keep main: G.edges_from\n",
      "  9.60s DETAIL: keep main: G.edges_to\n",
      "  9.60s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  9.60s DETAIL: keep main: F.etcbc4_ft_lex [node] \n",
      "  9.60s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  9.60s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_g_lex [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_g_word [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ph_phono [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_ph_phono_sep [node] \n",
      "  9.61s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_db_otype [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_g_lex [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_g_word [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_lex [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_lex_utf8 [node] \n",
      "  9.61s DETAIL: clear annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_ph_phono [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_ph_phono_sep [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_sft_book [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_sft_chapter [node] \n",
      "  9.62s DETAIL: clear annox: F.etcbc4_sft_verse [node] \n",
      "  9.62s DETAIL: load main: F.JanetDyk_ft_f_correction [node] \n",
      "  9.62s DETAIL: load main: F.JanetDyk_ft_grammatical [node] \n",
      "  9.62s DETAIL: load main: F.JanetDyk_ft_lexical [node] \n",
      "  9.62s DETAIL: load main: F.JanetDyk_ft_s_manual [node] \n",
      "  9.62s DETAIL: load main: F.JanetDyk_ft_semantic [node] \n",
      "  9.63s DETAIL: load main: F.JanetDyk_ft_valence [node] \n",
      "  9.63s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "    11s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "    11s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "    11s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_f_correction [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_grammatical [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_lexical [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_s_manual [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_semantic [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_valence [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_db_oid [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_db_otype [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_function [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_lex [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_sp [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_vs [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_sft_verse [node] \n",
      "    12s INFO: LOADING PREPARED data: please wait ... \n",
      "    12s DETAIL: keep prep: G.node_sort\n",
      "    12s DETAIL: keep prep: G.node_sort_inv\n",
      "    12s DETAIL: keep prep: L.node_up\n",
      "    12s DETAIL: keep prep: L.node_down\n",
      "    12s DETAIL: keep prep: V.verses\n",
      "    12s DETAIL: keep prep: V.books_la\n",
      "    12s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    16s INFO: LOADED PREPARED data\n",
      "    16s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-05-27T12-45-49\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "        function s_manual f_correction\n",
    "    ''' + ' '.join(enrich_fields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 40s collecting phrases ...\n",
      "20m 41s  10000 phrases in  2910 clauses ...\n",
      "20m 41s  20000 phrases in  5916 clauses ...\n",
      "20m 42s  30000 phrases in  9055 clauses ...\n",
      "20m 43s  40000 phrases in 12168 clauses ...\n",
      "20m 43s  50000 phrases in 15374 clauses ...\n",
      "20m 44s  52300 phrases in 16053 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = outfile('all.csv')\n",
    "NALLFIELDS = 10\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "inf('collecting phrases ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'valence',\n",
    "    'grammatical',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'phrase text',\n",
    "    'phrase node',\n",
    "))\n",
    "i = 0\n",
    "j = 0\n",
    "c = 0\n",
    "CHUNK_SIZE = 10000\n",
    "for cn in sorted(clause_verb_selected):\n",
    "    c += 1\n",
    "    verbs = sorted(clause_verb_selected[cn])\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        T.passage(cn),\n",
    "        ' '.join(F.lex.v(verb) for verb in verbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.words(L.d('word', cn), fmt='ec').replace('\\n', ' '),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d('phrase', cn):\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            inf('{:>6} phrases in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'True' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'True' else '',\n",
    "            T.passage(pn),\n",
    "            '',\n",
    "            F.valence.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.words(L.d('word', pn), fmt='ec').replace('\\n', ' '),\n",
    "            pn,\n",
    "        ))\n",
    "f.close()\n",
    "inf('{:>6} phrases in {:>5} clauses done'.format(i, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
