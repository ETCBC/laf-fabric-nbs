{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label.\n",
    "\n",
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "\n",
    "# 1. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.23\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  5.65s LOGFILE=/Users/dirk/Local/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  5.65s INFO: LOADING PREPARED data: please wait ... \n",
      "  5.65s prep prep: G.node_sort\n",
      "  5.76s prep prep: G.node_sort_inv\n",
      "  6.29s prep prep: L.node_up\n",
      "  9.61s prep prep: L.node_down\n",
      "    15s prep prep: V.verses\n",
      "    15s prep prep: V.books_la\n",
      "    15s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    17s INFO: LOADED PREPARED data\n",
      "    17s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-23T12-11-53\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, '--', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "verbs |= motion_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function_values = {F.function.v(p) for p in F.otype.s('phrase')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adju',\n",
       " 'BoundErr',\n",
       " 'Cmpl',\n",
       " 'Conj',\n",
       " 'EPPr',\n",
       " 'ExsS',\n",
       " 'Exst',\n",
       " 'Frnt',\n",
       " 'IntS',\n",
       " 'Intj',\n",
       " 'Loca',\n",
       " 'ModS',\n",
       " 'Modi',\n",
       " 'NCoS',\n",
       " 'NCop',\n",
       " 'Nega',\n",
       " 'Objc',\n",
       " 'PrAd',\n",
       " 'PrcS',\n",
       " 'PreC',\n",
       " 'PreO',\n",
       " 'PreS',\n",
       " 'Pred',\n",
       " 'PtcO',\n",
       " 'Ques',\n",
       " 'Rela',\n",
       " 'Subj',\n",
       " 'Supp',\n",
       " 'Time',\n",
       " 'Voct'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_values = dict(\n",
    "    BoundErr='this phrase is part of another phrase and does not merit its own function value',\n",
    ")\n",
    "\n",
    "function_values |= set(error_values)\n",
    "function_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 00m 59s Finding occurrences\n",
      " 1h 01m 01s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BR 548   occurrences\n",
      "<LH 890   occurrences\n",
      "BR> 48    occurrences\n",
      "BW> 2570  occurrences\n",
      "CJT 85    occurrences\n",
      "CWB 1037  occurrences\n",
      "HLK 1554  occurrences\n",
      "JRD 377   occurrences\n",
      "JY> 1069  occurrences\n",
      "NPL 445   occurrences\n",
      "NWS 159   occurrences\n",
      "QR> 743   occurrences\n",
      "SWR 297   occurrences\n"
     ]
    }
   ],
   "source": [
    "msg('Finding occurrences')\n",
    "occs = collections.defaultdict(list)\n",
    "for n in F.otype.s('word'):\n",
    "    lex = F.lex.v(n)\n",
    "    if lex.endswith('['):\n",
    "        lex = lex[0:-1]\n",
    "        occs[lex].append(n)\n",
    "msg('Done')\n",
    "for verb in sorted(verbs):\n",
    "    print('{} {:<5} occurrences'.format(verb, len(occs[verb])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 01m 07s Generated correction sheet for verb HLK_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb CWB_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb NWS_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb JRD_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb JYa_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb oBR_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb QRa_corr_etcbc4b.csv\n",
      " 1h 01m 07s Generated correction sheet for verb CJT_corr_etcbc4b.csv\n",
      " 1h 01m 08s Generated correction sheet for verb NPL_corr_etcbc4b.csv\n",
      " 1h 01m 08s Generated correction sheet for verb BRa_corr_etcbc4b.csv\n",
      " 1h 01m 08s Generated correction sheet for verb SWR_corr_etcbc4b.csv\n",
      " 1h 01m 08s Generated correction sheet for verb oLH_corr_etcbc4b.csv\n",
      " 1h 01m 08s Generated correction sheet for verb BWa_corr_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def vfile(verb, kind): return '{}_{}_{}{}.csv'.format(verb.replace('>','a').replace('<', 'o'), kind, source, version)\n",
    "\n",
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = '{} {}:{}'.format(T.book_name(bn, lang='en'), ch, vs)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        row = [cln, wn, passage_label, lnx, vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            row.extend((pn, pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr')\n",
    "    row_file = outfile(filename)\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39m 49s OK: Corrected phrases did not receive multiple corrections\n",
      "39m 49s OK: all corrected nodes where phrase nodes\n",
      "39m 49s ERROR: Some corrections supply illegal values for phrase function!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR>: Found     2 corrections in /Users/dirk/surfdrive/laf-fabric-data/cpl/BRa_corrected_etcbc4b.csv\n",
      "CJT: Found     3 corrections in /Users/dirk/surfdrive/laf-fabric-data/cpl/CJT_corrected_etcbc4b.csv\n",
      "QR>: Found     5 corrections in /Users/dirk/surfdrive/laf-fabric-data/cpl/QRa_corrected_etcbc4b.csv\n",
      "{'spec'}\n"
     ]
    }
   ],
   "source": [
    "pf_corr = {}\n",
    "repeated = collections.defaultdict(list)\n",
    "non_phrase = set()\n",
    "illegal_fvalue = set()\n",
    "\n",
    "annox_basedir = API['data_dir']\n",
    "annox_subdir = 'cpl'\n",
    "annox_dir = '{}/{}'.format(annox_basedir, annox_subdir)\n",
    "\n",
    "def read_corr():\n",
    "    for verb in sorted(verbs):\n",
    "        filename = '{}/{}'.format(annox_dir, vfile(verb, 'corrected'))\n",
    "        if not os.path.exists(filename):\n",
    "            print('NO file {}'.format(filename))\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        print('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "    if len(repeated):\n",
    "        msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "        for x in sorted(repeated):\n",
    "            print('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "    else:\n",
    "        msg('OK: Corrected phrases did not receive multiple corrections')\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "    else:\n",
    "        msg('OK: all corrected nodes where phrase nodes')\n",
    "    if len(illegal_fvalue):\n",
    "        msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "        print(illegal_fvalue)\n",
    "    else:\n",
    "        msg('OK: all corrected values are legal')\n",
    "        \n",
    "read_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specs = '''function\tdescription\tvalence\tgrammatical\tlexical\tsemantic\n",
    "Adju\tAdjunct\tadjunct\tNA\t\t\n",
    "Cmpl\tComplement\tcomplement\t*\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula+subject\t\t\n",
    "Exst\tExistence\tcore\tcopula\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tsubject\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tsubject\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula+subject\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\tNA\n",
    "Objc\tObject\tcomplement\tobject\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tpredication+subject\t\t\n",
    "PreC\tPredicate complement\tcore\tpredication\t\t\n",
    "Pred\tPredicate\tcore\tpredication\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tpredication+object\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tpredication+subject\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tpredication+object\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\tNA\n",
    "Subj\tSubject\tcore\tsubject\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\tNA'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specfields = specs[0].split('\\t')[2:]\n",
    "transform = dict((x[0], tuple(x[2:])) for x in (y.split('\\t') for y in specs[1:]))\n",
    "for e in error_values:\n",
    "    transform[e] = ('NA',)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valence', 'grammatical', 'lexical', 'semantic']\n",
      "Adju: ('adjunct', 'NA', '', '')\n",
      "BoundErr: ('NA', 'NA', 'NA', 'NA')\n",
      "Cmpl: ('complement', '*', '', '')\n",
      "Conj: ('NA', 'NA', 'NA', 'NA')\n",
      "EPPr: ('NA', 'copula', '', '')\n",
      "ExsS: ('core', 'copula+subject', '', '')\n",
      "Exst: ('core', 'copula', '', '')\n",
      "Frnt: ('NA', 'NA', 'NA', 'NA')\n",
      "IntS: ('core', 'subject', '', '')\n",
      "Intj: ('NA', 'NA', 'NA', 'NA')\n",
      "Loca: ('adjunct', 'NA', 'location', 'location')\n",
      "ModS: ('core', 'subject', '', '')\n",
      "Modi: ('NA', 'NA', 'NA', 'NA')\n",
      "NCoS: ('core', 'copula+subject', '', '')\n",
      "NCop: ('core', 'copula', '', '')\n",
      "Nega: ('NA', 'NA', 'NA', 'NA')\n",
      "Objc: ('complement', 'object', '', '')\n",
      "PrAd: ('adjunct', 'NA', '', '')\n",
      "PrcS: ('core', 'predication+subject', '', '')\n",
      "PreC: ('core', 'predication', '', '')\n",
      "PreO: ('core', 'predication+object', '', '')\n",
      "PreS: ('core', 'predication+subject', '', '')\n",
      "Pred: ('core', 'predication', '', '')\n",
      "PtcO: ('core', 'predication+object', '', '')\n",
      "Ques: ('NA', 'NA', 'NA', 'NA')\n",
      "Rela: ('NA', 'NA', 'NA', 'NA')\n",
      "Subj: ('core', 'subject', '', '')\n",
      "Supp: ('adjunct', 'NA', '', 'benefactive')\n",
      "Time: ('adjunct', 'NA', 'time', 'time')\n",
      "Unkn: ('NA', 'NA', 'NA', 'NA')\n",
      "Voct: ('NA', 'NA', 'NA', 'NA')\n"
     ]
    }
   ],
   "source": [
    "print('{}\\n{}'.format(specfields, '\\n'.join('{}: {}'.format(*x) for x in sorted(transform.items()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "pnode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "link\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "phrase_text\n",
      "function\n",
      "valence\n",
      "grammatical\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    link\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    clause_text    \n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    phrase_text\n",
    "    function\n",
    "'''.strip().split() + specfields\n",
    "\n",
    "field_names = []\n",
    "for f in COMMON_FIELDS: field_names.append(f)\n",
    "for i in range(max((len(CLAUSE_FIELDS), len(PHRASE_FIELDS)))):\n",
    "    pf = PHRASE_FIELDS[i] if i < len(PHRASE_FIELDS) else '--'\n",
    "    field_names.append(pf)\n",
    "    \n",
    "fillrows = len(CLAUSE_FIELDS) - len(PHRASE_FIELDS)\n",
    "cfillrows = 0 if fillrows >= 0 else -fillrows\n",
    "pfillrows = fillrows if fillrows >= 0 else 0\n",
    "print('\\n'.join(field_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56m 23s Generated enrichment sheet for verb QRa_enrich_etcbc4b.csv\n",
      "56m 23s Generated enrichment sheet for verb CJT_enrich_etcbc4b.csv\n",
      "56m 23s Generated enrichment sheet for verb BRa_enrich_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cl = L.u('clause', wn)\n",
    "        if cl in clauses_seen: continue\n",
    "        clauses_seen.add(cl)\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, lnx, vl, vstem, vt)\n",
    "        clause_fields = (ct,)\n",
    "        rows.append(common_fields + clause_fields + (('',)*cfillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vt)\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = pf_corr.get(pn, None) or F.function.v(pn)\n",
    "            phrase_fields = (pt, pf) + transform[pf]\n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "    filename = vfile(verb, 'enrich')\n",
    "    row_file = outfile(filename)\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated enrichment sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    30s OK: Enriched phrases did not receive multiple enrichments\n",
      "    30s OK: all enriched nodes where phrase nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO file /Users/dirk/surfdrive/laf-fabric-data/cpl/BRa_enriched_etcbc4b.csv\n",
      "CJT: Found   291 enrichments in /Users/dirk/surfdrive/laf-fabric-data/cpl/CJT_enriched_etcbc4b.csv\n",
      "NO file /Users/dirk/surfdrive/laf-fabric-data/cpl/QRa_enriched_etcbc4b.csv\n",
      "[(605977, 'NA', 'NA', 'NA', 'NA'), (605978, 'complement', 'object', '', ''), (605979, 'core', 'predication', '', ''), (605980, 'complement', '*', '', ''), (606391, 'NA', 'NA', 'NA', 'NA'), (606392, 'core', 'predication', '', ''), (606393, 'adjunct', 'NA', '', ''), (606394, 'core', 'subject', '', ''), (606395, 'complement', 'object', '', ''), (606396, 'complement', '*', '', '')]\n"
     ]
    }
   ],
   "source": [
    "pf_enriched = set()\n",
    "repeated = collections.defaultdict(list)\n",
    "non_phrase = set()\n",
    "\n",
    "def read_enrich(rootdir):\n",
    "    results = []\n",
    "    for verb in sorted(verbs):\n",
    "        filename = '{}/{}'.format(rootdir, vfile(verb, 'enriched'))\n",
    "        if not os.path.exists(filename):\n",
    "            print('NO file {}'.format(filename))\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                pn = int(fields[2])\n",
    "                if pn < 0: continue\n",
    "                vvals = tuple(fields[-4:])\n",
    "                results.append((pn,)+vvals)\n",
    "                if pn in pf_enriched:\n",
    "                    repeated[pn] += vvals\n",
    "                else:\n",
    "                    pf_enriched.add(pn)\n",
    "                if F.otype.v(pn) != 'phrase': \n",
    "                    non_phrase.add(pn)\n",
    "\n",
    "        print('{}: Found {:>5} enrichments in {}'.format(verb, len(results), filename))\n",
    "    if len(repeated):\n",
    "        msg('ERROR: Some phrases have been enriched multiple times!')\n",
    "        for x in sorted(repeated):\n",
    "            print('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "    else:\n",
    "        msg('OK: Enriched phrases did not receive multiple enrichments')\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Enrichments have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "    else:\n",
    "        msg('OK: all enriched nodes where phrase nodes')\n",
    "    print(results[0:10])\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-03'},\n",
    "    [\n",
    "        ('cpl', 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in specfields\n",
    "        ))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Annox complements\n",
    "We load the s into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in the variable ``specfields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.00s DETAIL: load main: X. [node]  -> \n",
      "  1.44s DETAIL: load main: X. [e]  -> \n",
      "  4.00s DETAIL: load main: G.node_anchor_min\n",
      "  4.11s DETAIL: load main: G.node_anchor_max\n",
      "  4.22s DETAIL: load main: G.node_sort\n",
      "  4.33s DETAIL: load main: G.node_sort_inv\n",
      "  4.90s DETAIL: load main: G.edges_from\n",
      "  5.03s DETAIL: load main: G.edges_to\n",
      "  5.17s LOGFILE=/Users/dirk/surfdrive/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  5.17s PARSING ANNOTATION FILES\n",
      "  5.21s INFO: parsing complements.xml\n",
      "  5.23s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "       291 good   annots   and     0 faulty ones\n",
      "      1164 good   features and     0 faulty ones\n",
      "       291 distinct xml identifiers\n",
      "\n",
      "  5.23s MODELING RESULT FILES\n",
      "  5.23s INFO: CONNECTIVITY\n",
      "  5.39s WRITING RESULT FILES for a\n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_grammatical [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_lexical [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_semantic [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_valence [node] \n",
      "  5.39s DETAIL: write annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  5.40s DETAIL: write annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  5.40s DETAIL: write annox: F.etcbc4_ph_phono [node] \n",
      "  6.16s DETAIL: write annox: F.etcbc4_ph_phono_sep [node] \n",
      "  6.35s END   COMPILE a: complements\n",
      "  6.35s USING annox DATA COMPILED AT: 2016-03-23T11-37-32\n",
      "  6.35s DETAIL: keep main: G.node_anchor_min\n",
      "  6.35s DETAIL: keep main: G.node_anchor_max\n",
      "  6.35s DETAIL: keep main: G.node_sort\n",
      "  6.35s DETAIL: keep main: G.node_sort_inv\n",
      "  6.35s DETAIL: keep main: G.edges_from\n",
      "  6.35s DETAIL: keep main: G.edges_to\n",
      "  6.35s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  6.36s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  6.36s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ph_phono [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ph_phono_sep [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_db_otype [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_lex_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ph_phono [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ph_phono_sep [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_book [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_chapter [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_verse [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_grammatical [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_lexical [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_semantic [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_valence [node] \n",
      "  6.37s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  7.47s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  7.62s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  7.87s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  8.18s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_grammatical [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_lexical [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_semantic [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_valence [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_db_oid [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_db_otype [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_function [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_lex [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_sp [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_vs [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_sft_verse [node] \n",
      "  8.45s INFO: LOADING PREPARED data: please wait ... \n",
      "  8.45s DETAIL: keep prep: G.node_sort\n",
      "  8.45s DETAIL: keep prep: G.node_sort_inv\n",
      "  8.46s DETAIL: keep prep: L.node_up\n",
      "  8.46s DETAIL: keep prep: L.node_down\n",
      "  8.46s DETAIL: keep prep: V.verses\n",
      "  8.46s DETAIL: keep prep: V.books_la\n",
      "  8.46s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    11s INFO: LOADED PREPARED data\n",
      "    11s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-23T11-37-36\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "        function\n",
    "    ''' + ' '.join(specfields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
