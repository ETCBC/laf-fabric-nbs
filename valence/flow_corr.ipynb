{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext (author's copy with deviant page numbering)](https://shebanq.ancient-data.org/static/docs/methods/2014_Dyk_jnsl.pdf)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.6.2\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os, collections\n",
    "from copy import deepcopy\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s USING annox DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "  7.48s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  7.48s INFO: LOADING PREPARED data: please wait ... \n",
      "  7.48s prep prep: G.node_sort\n",
      "  7.59s prep prep: G.node_sort_inv\n",
      "  8.15s prep prep: L.node_up\n",
      "    12s prep prep: L.node_down\n",
      "    18s prep prep: V.verses\n",
      "    18s prep prep: V.books_la\n",
      "    18s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    21s INFO: LOADED PREPARED data\n",
      "    21s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-06-15T14-45-19\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, 'lexicon', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex uvf prs nametype ls\n",
    "        function rela\n",
    "        chapter verse\n",
    "    ''','''\n",
    "        mother\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        msg('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this phrase is part of another phrase and does not merit its own function value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.34s {'function': {'Voct', 'PrcS', 'PreC', 'Nega', 'Ques', 'Time', 'PreS', 'PrAd', 'Modi', 'Cmpl', 'Rela', 'PreO', 'NCoS', 'Supp', 'Loca', 'BoundErr', 'EPPr', 'ExsS', 'NCop', 'Subj', 'Conj', 'Frnt', 'IntS', 'Exst', 'Objc', 'Intj', 'PtcO', 'Pred', 'Adju', 'ModS'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "inf('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.35s Finding occurrences ...\n",
      "  3.03s Done\n",
      "Total:     73679 verb occurrences in 70131 clauses\n",
      "Selected:  16209 verb occurrences in 16053 clauses\n",
      "<BR 556   occurrences\n",
      "<FH 2629  occurrences\n",
      "<LH 890   occurrences\n",
      "BR> 54    occurrences\n",
      "BW> 2570  occurrences\n",
      "CJT 85    occurrences\n",
      "CWB 1056  occurrences\n",
      "FJM 609   occurrences\n",
      "HLK 1554  occurrences\n",
      "JRD 377   occurrences\n",
      "JY> 1069  occurrences\n",
      "NF> 656   occurrences\n",
      "NPL 445   occurrences\n",
      "NTN 2017  occurrences\n",
      "NWS 159   occurrences\n",
      "PQD 303   occurrences\n",
      "QR> 883   occurrences\n",
      "SWR 297   occurrences\n"
     ]
    }
   ],
   "source": [
    "inf('Finding occurrences ...')\n",
    "occs = collections.defaultdict(list)           # dictionary of all verb occurrence nodes per verb lexeme\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of all verb occurrence nodes per clause node\n",
    "clause_verb_selected = collections.defaultdict(list) # idem but for the occurrences of selected verbs\n",
    "\n",
    "nw = 0\n",
    "nws = 0\n",
    "for n in F.otype.s('word'):\n",
    "    if F.sp.v(n) != 'verb': continue\n",
    "    nw += 1\n",
    "    lex = F.lex.v(n).rstrip('/=[')\n",
    "    occs[lex].append(n)\n",
    "    cn = L.u('clause', n)\n",
    "    clause_verb[cn].append(n)\n",
    "    if lex in verbs:\n",
    "        nws += 1\n",
    "        clause_verb_selected[cn].append(n)\n",
    "\n",
    "inf('Done')\n",
    "inf('Total:    {:>6} verb occurrences in {} clauses'.format(nw, len(clause_verb)), withtime=False)\n",
    "inf('Selected: {:>6} verb occurrences in {} clauses'.format(nws, len(clause_verb_selected)), withtime=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    inf('{} {:<5} occurrences'.format(verb, len(occs[verb])), withtime=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.55s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BWa_etcbc4b.csv\n",
      "  3.87s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NTN_etcbc4b.csv\n",
      "  3.98s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NFa_etcbc4b.csv\n",
      "  4.04s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/PQD_etcbc4b.csv\n",
      "  4.11s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NPL_etcbc4b.csv\n",
      "  4.12s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BRa_etcbc4b.csv\n",
      "  4.33s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/HLK_etcbc4b.csv\n",
      "  4.43s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/FJM_etcbc4b.csv\n",
      "  4.45s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NWS_etcbc4b.csv\n",
      "  4.54s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oBR_etcbc4b.csv\n",
      "  4.56s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CJT_etcbc4b.csv\n",
      "  4.69s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/QRa_etcbc4b.csv\n",
      "  4.85s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JYa_etcbc4b.csv\n",
      "  4.99s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oLH_etcbc4b.csv\n",
      "  5.05s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/SWR_etcbc4b.csv\n",
      "  5.42s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oFH_etcbc4b.csv\n",
      "  5.57s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CWB_etcbc4b.csv\n",
      "  5.64s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JRD_etcbc4b.csv\n",
      "  5.69s 52110  phrases seen 1  time(s)\n",
      "  5.69s 181    phrases seen 2  time(s)\n",
      "  5.69s 9      phrases seen 3  time(s)\n",
      "  5.69s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = T.passage(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        row = [cln, wn, passage_label, lnx, vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            row.extend((pn, pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5.78s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oBR_etcbc4b.csv\n",
      "  5.78s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      "  5.78s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.78s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "  5.78s BR>: Found     4 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "  5.78s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.78s OK: all corrected nodes where phrase nodes\n",
      "  5.78s OK: all corrected values are legal\n",
      "  5.79s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "  5.82s BW>: Found    59 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "  5.82s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.82s OK: all corrected nodes where phrase nodes\n",
      "  5.82s OK: all corrected values are legal\n",
      "  5.82s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "  5.83s CJT: Found    62 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "  5.83s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.83s OK: all corrected nodes where phrase nodes\n",
      "  5.83s OK: all corrected values are legal\n",
      "  5.83s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "  5.85s CWB: Found   107 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "  5.85s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.85s OK: all corrected nodes where phrase nodes\n",
      "  5.85s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5.85s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.85s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "  5.87s HLK: Found   266 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "  5.87s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.87s OK: all corrected nodes where phrase nodes\n",
      "  5.87s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5.87s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JRD_etcbc4b.csv\n",
      "  5.87s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JYa_etcbc4b.csv\n",
      "  5.87s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      "  5.87s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.87s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "  5.91s NTN: Found   410 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "  5.91s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.91s OK: all corrected nodes where phrase nodes\n",
      "  5.91s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5.91s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      "  5.91s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.91s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "  5.92s QR>: Found   413 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "  5.92s OK: Corrected phrases did not receive multiple corrections\n",
      "  5.92s OK: all corrected nodes where phrase nodes\n",
      "  5.92s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5.92s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.92s Found 413 corrections in the phrase function\n",
      "  5.94s 26090  phrases seen 1  time(s)\n",
      "  5.94s 33     phrases seen 2  time(s)\n",
      "  5.94s Total phrases seen: 26123\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            msg('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            inf('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        inf('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                msg('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            inf('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            inf('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            msg('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            inf('OK: all corrected values are legal')\n",
    "    inf('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.95s Enrich field specification OK\n",
      "valence = {adjunct, complement, core}\n",
      "grammatical = {*, _promoted_direct_object, copula, copula+subject, direct_object, indirect_object, predication, predication+_promoted_direct_object, predication+direct_object, predication+indirect_object, predication+principal_direct_object, predication+subject, principal_direct_object, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, location, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "grammatical\n",
    "    *\n",
    "    subject\n",
    "    _promoted_direct_object\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    indirect_object\n",
    "    copula\n",
    "    copula+subject\n",
    "    predication\n",
    "    predication+subject\n",
    "    predication+_promoted_direct_object\n",
    "    predication+principal_direct_object\n",
    "    predication+direct_object\n",
    "    predication+indirect_object\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "if None in enrich_fields:\n",
    "    msg('Invalid enrich field specification')\n",
    "else:\n",
    "    inf('Enrich field specification OK')\n",
    "for ef in enrich_fields:\n",
    "    inf('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specs = '''\n",
    "Adju\tAdjunct\tadjunct\tNA\t\t\n",
    "Cmpl\tComplement\tcomplement\t*\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula+subject\t\t\n",
    "Exst\tExistence\tcore\tcopula\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tsubject\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tsubject\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula+subject\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\tNA\n",
    "Objc\tObject\tcomplement\tdirect_object\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tpredication+subject\t\t\n",
    "PreC\tPredicate complement\tcore\tpredication\t\t\n",
    "Pred\tPredicate\tcore\tpredication\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tpredication+direct_object\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tpredication+subject\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tpredication+direct_object\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\tNA\n",
    "Subj\tSubject\tcore\tsubject\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\tNA'''.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.99s Defaults OK (124 good)\n"
     ]
    }
   ],
   "source": [
    "transform = {}\n",
    "for line in specs:\n",
    "    x = line.split('\\t') \n",
    "    transform[x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "for e in error_values['function']:\n",
    "    transform[e] = dict(zip(enrich_fields, ['NA']*4))\n",
    "\n",
    "errors = 0\n",
    "good = 0\n",
    "for f in transform:\n",
    "    for e in enrich_fields:\n",
    "        val = transform[f][e]\n",
    "        if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "            msg('Defaults for `{}`: wrong `{}` value: \"{}\"'.format(f, e, val))\n",
    "            errors += 1\n",
    "        else: good += 1\n",
    "if errors:\n",
    "    msg('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    inf('Defaults OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence         grammatical                    lexical         semantic       \n",
      "Adju    : adjunct         NA                                                            \n",
      "BoundErr: NA              NA                             NA              NA             \n",
      "Cmpl    : complement      *                                                             \n",
      "Conj    : NA              NA                             NA              NA             \n",
      "EPPr    : NA              copula                                                        \n",
      "ExsS    : core            copula+subject                                                \n",
      "Exst    : core            copula                                                        \n",
      "Frnt    : NA              NA                             NA              NA             \n",
      "IntS    : core            subject                                                       \n",
      "Intj    : NA              NA                             NA              NA             \n",
      "Loca    : adjunct         NA                             location        location       \n",
      "ModS    : core            subject                                                       \n",
      "Modi    : NA              NA                             NA              NA             \n",
      "NCoS    : core            copula+subject                                                \n",
      "NCop    : core            copula                                                        \n",
      "Nega    : NA              NA                             NA              NA             \n",
      "Objc    : complement      direct_object                                                 \n",
      "PrAd    : adjunct         NA                                                            \n",
      "PrcS    : core            predication+subject                                           \n",
      "PreC    : core            predication                                                   \n",
      "PreO    : core            predication+direct_object                                     \n",
      "PreS    : core            predication+subject                                           \n",
      "Pred    : core            predication                                                   \n",
      "PtcO    : core            predication+direct_object                                     \n",
      "Ques    : NA              NA                             NA              NA             \n",
      "Rela    : NA              NA                             NA              NA             \n",
      "Subj    : core            subject                                                       \n",
      "Supp    : adjunct         NA                                             benefactive    \n",
      "Time    : adjunct         NA                             time            time           \n",
      "Unkn    : NA              NA                             NA              NA             \n",
      "Voct    : NA              NA                             NA              NA             \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: {:<15} {:<30} {:<15} {:<15}'\n",
    "inf(ltpl.format('func', *enrich_fields), withtime=False)\n",
    "for f in sorted(transform):\n",
    "    sfs = transform[f]\n",
    "    inf(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*.\n",
    "\n",
    "We will also analyse the direct and indirect objects more precisely and implement heuristics to make a distinction between complements (locative) and indirect objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the direct objects\n",
    "\n",
    "In the target clauses we will find the direct object(s).\n",
    "If there is more than one, we will compute which is the principal one.\n",
    "The others are secundary ones.\n",
    "If there is only one direct object, it may or may not be the principal direct object.\n",
    "Initially we assume that a single object is the principal one.\n",
    "\n",
    "An object can be a phrase or a clause. \n",
    "Initially, we will not mark object phrases as principal direct objects.\n",
    "\n",
    "### Implied objects\n",
    "\n",
    "There are many cases where there is a direct object without it being marked as such in the data.\n",
    "Those are cases where there are no objective, unambiguous signals for a direct object.\n",
    "We call them *implied objects*. Examples: \n",
    "\n",
    "* the relativum in relative clauses\n",
    "* complements starting with MN (from) or L (to)\n",
    "\n",
    "In the case of implied objects we have to guess.\n",
    "Initially we assume that there are no implied objects.\n",
    "\n",
    "Later, when we inspect individual cases, we can mark principal objects and implied objects manually\n",
    "for those cases where these rules do not suffice.\n",
    "\n",
    "### Finding the principal direct object\n",
    "\n",
    "When there are multiple direct objects, we use the rules formulated by (Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) to determine which one is the principal one. The rules are stated below where we make some remarks about how we apply them to our data.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "When looking for principal direct objects, we restrict ourselves to direct objects at the phrase level, either being complete phrases, or pronominal suffixes within phrases. The following rules express a preference for the principal direct object. In a given context, we select the direct object that is preferred by applying those rules as the principal direct object. We only apply these rules if there are at least two direct objects.\n",
    "If there is only one direct object, it is chosen as principal object if and only if it is not a clause.\n",
    "\n",
    "#### Rule 1: pronominal suffixes > preferred above marked objects > unmarked objects\n",
    "\n",
    "In a given clause, we collect all phrases with function ``PreO`` or ``PtcO``. \n",
    "If this collection is non-empty, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise, we proceed as follows.\n",
    "\n",
    "We collect all the phrases with function ``Objc``.\n",
    "If this collection is empty, there will not be a principal object.\n",
    "Otherwise, we split it up in marked and unmarked object phrases.\n",
    "\n",
    "An object phrase is *marked* if and only if it contains, somewhere, the object marker ``>T``.\n",
    "If there are marked object phrases, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 2: determined phrases > undetermined phrases\n",
    "\n",
    "We only arrive here if there are multiple ``Objc`` phrases, neither of which is marked.\n",
    "In this case, we take the textually first one (by rule 3) which has the value ``det`` for its feature ``det``, if there is one, and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 3: earlier phrases > later phrases (by textual order)\n",
    "\n",
    "This rule is implicitly applied if one of the rules before yielded more than one candidate for the principal object. Furthermore, we arrive here if the previous rules have not selected any principal direct object, while we do have more than one ``Objc`` phrase.\n",
    "\n",
    "In this case, we pick the textually first ``Objc`` phrase.\n",
    "\n",
    "### Complements as Objects\n",
    "\n",
    "In some cases, a complement functions as objects, such as in [Genesis 21:13](https://shebanq.ancient-data.org/hebrew/text?nget=v&chapter=21&book=Genesis&qw=n&tp=txt_tb1&version=4b&mr=m) *I make him (into) a people*.\n",
    "\n",
    "Candidates are those complements that: \n",
    "\n",
    "* start with either preposition ``L`` or ``K`` and\n",
    "* the ``L`` or ``K`` in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\n",
    "\n",
    "We are not sure whether these conditions are sufficient to warrant a direct object in all cases.\n",
    "So we generated the preliminary grammatical label ``_promoted_direct_object`` in these cases.\n",
    "\n",
    "In the review phase of the enrichment sheets, these cases must be resolved by changing this label to ``direct_object`` or ``NA``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.22s Finding direct objects and determining the principal one\n",
      "  7.56s Done\n",
      "  7.56s Clauses with  3 objects                 :     3\n",
      "  7.56s Clauses with  2 objects                 :  1110\n",
      "  7.56s Clauses with  1 objects                 : 26211\n",
      "  7.56s Clauses with  0 objects                 : 42807\n",
      "  7.57s Clauses with  2 complements as objects  :    34\n",
      "  7.57s Clauses with  1 complements as objects  :  3957\n",
      "Clauses with a principal object         : 26062\n",
      "Clauses with a direct object            : 28440\n",
      "Clauses with a promoted object          :  3991\n",
      "Total number of clauses                 : 70131\n"
     ]
    }
   ],
   "source": [
    "inf('Finding direct objects and determining the principal one')\n",
    "directobjects = set()\n",
    "directobjects_c = {}\n",
    "pdirectobjects = set()\n",
    "pdirectobjects_c = {}\n",
    "promotions_c = {}\n",
    "promotions = set()\n",
    "mobjects = collections.Counter() # count how many clauses have m objects (for each m)\n",
    "cmobjects = collections.Counter() # count how many clauses have m promotion candidates\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d('word', p)\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "        \n",
    "for c in clause_verb:\n",
    "    dobjects = {}\n",
    "    dobjects_set = set()\n",
    "    nobjects = 0\n",
    "    prom = set()\n",
    "    \n",
    "    for p in L.d('phrase', c):\n",
    "        pf = pf_corr.get(p, None) or F.function.v(p)\n",
    "        if pf in objectfuncs:\n",
    "            dobjects.setdefault('p_'+pf, set()).add(p)\n",
    "            nobjects += 1\n",
    "            dobjects_set.add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d('word', p)\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                prom.add(p)\n",
    "    nprom = len(prom)\n",
    "    if nprom:\n",
    "        cmobjects[nprom] += 1\n",
    "        promotions_c[c] = prom\n",
    "        promotions |= prom\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d('clause', L.u('sentence', c)):\n",
    "        cr = F.rela.v(ac)\n",
    "        if cr in {'Objc'} and list(C.mother.v(ac))[0] == c:\n",
    "            dobjects.setdefault('c_'+cr, set()).add(ac)\n",
    "            nobjects += 1\n",
    "            dobjects_set.add(ac)\n",
    "    mobjects[nobjects] += 1\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    dobjects_order = sorted(dobjects_set, key=NK)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            theobject = list(dobjects_set)[0]\n",
    "            if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes\n",
    "        principal_candidates = dobjects.get('p_PreO', set()) | dobjects.get('p_PtcO', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sorted(principal_candidates, key=NK)[0]\n",
    "            break\n",
    "        principal_candidates = dobjects.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) > 0:\n",
    "                principal_object = sorted(principal_candidates, key=NK)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sorted(objects_marked, key=NK)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sorted(objects_unmarked, key=NK)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        pdirectobjects_c[c] = principal_object\n",
    "        pdirectobjects.add(principal_object)\n",
    "\n",
    "    if len(dobjects_set):\n",
    "        directobjects_c[c] = dobjects_set\n",
    "        directobjects |= dobjects_set\n",
    "\n",
    "inf('Done') \n",
    "\n",
    "for (label, n) in sorted(mobjects.items(), key=lambda y: -y[0]):\n",
    "    inf('{:<40}: {:>5}'.format('Clauses with {:>2} objects'.format(label), n))\n",
    "for (label, n) in sorted(cmobjects.items(), key=lambda y: -y[0]):\n",
    "    inf('{:<40}: {:>5}'.format('Clauses with {:>2} complements as objects'.format(label), n))\n",
    "\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a principal object', len(pdirectobjects)), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a direct object', len(directobjects)), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Clauses with a promoted object', sum(cmobjects.values())), withtime=False)\n",
    "inf('{:<40}: {:>5}'.format('Total number of clauses', len(clause_verb)), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "#### Cues for a locative complement\n",
    "\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``# h_loc`` how many H-locales are carried on words in the complement\n",
    "* ``body_part`` is 2 if the phrase starts with the preposition ``L`` followed by a body part, else 0\n",
    "* ``locativity`` ($loc$) a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``#topo``, ``# prep_b``, ``# h_loc`` and ``body_part``.\n",
    "\n",
    "#### Cues for an indirect object\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` or ``>L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` or ``>L`` plus proper name or person reference word occur in the complement\n",
    "* ``indirect object`` ($ind$) a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "\n",
    "#### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "The outcome is $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*)\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$\n",
    "\n",
    "In words:\n",
    "\n",
    "* if there are positive signals for L or I and none for the other, we choose the one for which there are positive signals;\n",
    "* if there are positive signals for both L and I, we follow the majority count, but only if the difference is at least two;\n",
    "* in all other cases we leave it at C: not necessarilty locative and not necessarily indirect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.68s Determinig kind of complements\n",
      "  8.71s Done\n",
      "Phrases of kind C :  17191\n",
      "Phrases of kind L :  12057\n",
      "Phrases of kind I :   7857\n",
      "Total complements :  37105\n",
      "Total phrases     : 214555\n"
     ]
    }
   ],
   "source": [
    "inf('Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d('phrase', c):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, None) or F.function.v(p)\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d('word', p)\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "inf('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    inf('Phrases of kind {:<2}: {:>6}'.format(label, n), withtime=False)\n",
    "inf('Total complements : {:>6}'.format(ncomplements), withtime=False)\n",
    "inf('Total phrases     : {:>6}'.format(nphrases), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 1 and F.lex.v(words[1] in personal_lexemes)\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    1: '''direct_object => principal_direct_object''',\n",
    "    2: '''non-object => principal_direct_object''',\n",
    "    3: '''non-object => direct_object''',\n",
    "    4: '''direct-object superfluously promoted to direct object''',\n",
    "    5: '''non-object => _promoted_direct_object''',\n",
    "    6: '''complement => indirect_object''',\n",
    "    7: '''complement => location''',\n",
    "    8: '''predicate+complement => predicate+indirect_object''',\n",
    "    9: '''predicate+complement => location''',\n",
    "}\n",
    "\n",
    "def generic_logic(pn, values):\n",
    "    gl = None\n",
    "    if pn in pdirectobjects:\n",
    "        oldv = values['grammatical']\n",
    "        if 'direct' in oldv:\n",
    "            newv = oldv.replace('direct', 'principal_direct')\n",
    "            gl = 1\n",
    "        else:\n",
    "            if 'predication' in oldv:\n",
    "                newv = oldv+'+principal_direct_object'\n",
    "            else:\n",
    "                newv = 'principal_direct_object'\n",
    "            gl = 2\n",
    "        values['grammatical'] = newv\n",
    "    elif pn in directobjects:\n",
    "        oldv = values['grammatical']\n",
    "        if 'direct' not in oldv:\n",
    "            if 'predication' in oldv:\n",
    "                newv = oldv+'+direct_object'\n",
    "            else:\n",
    "                newv = 'direct_object'\n",
    "            gl = 3\n",
    "            values['grammatical'] = newv\n",
    "    elif pn in promotions:\n",
    "        oldv = values['grammatical']\n",
    "        if 'direct' in oldv:\n",
    "            gl = 4\n",
    "        else:\n",
    "            if 'predication' in oldv:\n",
    "                newvv = oldv+'+_promoted_direct_object'\n",
    "            else:\n",
    "                newv = '_promoted_direct_object'\n",
    "            gl = 5\n",
    "        values['grammatical'] = newv\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 6\n",
    "                else:\n",
    "                    values['valence'] = 'adjunct'\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 7\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'predication+indirect_object'\n",
    "                    gl = 8\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 9\n",
    "    return gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases : value, which is true iff that feature has that value for the phrase in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_logic = {\n",
    "    'CJT': [\n",
    "        (\n",
    "            ('semantic', 'benefactive'), \n",
    "            ('function:Adju', has_L, is_lex_personal),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('function:Cmpl', has_H_locale),\n",
    "        ),\n",
    "        (\n",
    "            ('lexical', 'location'),\n",
    "            ('semantic', 'location'),\n",
    "            ('function:Cmpl', is_lex_local),\n",
    "        ),\n",
    "    ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1m 20s CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      " 1m 20s CJT-2 if function   = Cmpl     AND has_H_locale   \n",
      "      lexical    => location       \n",
      "\n",
      " 1m 20s CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      " 1m 20s All 3 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    result = '{}-{} '.format(vl, i+1)\n",
    "    pref_len = len(result)\n",
    "    result += 'if {}\\n'.format(' AND '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        result += '{}{:<10} => {:<15}\\n'.format(' '* pref_len, *sfa)\n",
    "    return result\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for vl in sorted(enrich_logic):\n",
    "        for items in enrich_logic[vl]:\n",
    "            rule_index[vl].append((items[-1], items[0:-1]))\n",
    "        for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "            inf(rule_as_str(vl, i))\n",
    "            nrules += 1\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                if sf not in enrich_fields:\n",
    "                    msg('\"{}\" not a valid enrich field'.format(sf), withtime=False)\n",
    "                    errors += 1\n",
    "                elif sfval not in enrich_fields[sf]:\n",
    "                    msg('`{}`: \"{}\" not a valid enrich field value'.format(sf, sfval), withtime=False)\n",
    "                    errors += 1\n",
    "            for c in conditions:\n",
    "                if type(c) == str:\n",
    "                    x = c.split(':')\n",
    "                    if len(x) != 2:\n",
    "                        msg('Wrong feature condition {}'.format(c))\n",
    "                        errors += 1\n",
    "                    else:\n",
    "                        (feat, val) = x\n",
    "                        if feat not in legal_values:\n",
    "                            msg('Feature `{}` not in use'.format(feat))\n",
    "                            errors += 1\n",
    "                        elif val not in legal_values[feat]:\n",
    "                            msg('Feature `{}`: not a valid value \"{}\"'.format(feat, val))\n",
    "                            errors += 1\n",
    "    if errors:\n",
    "        msg('There were {} errors in {} rules'.format(errors, nrules))\n",
    "    else:\n",
    "        inf('All {} rules OK'.format(nrules))\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generic_cases = {}\n",
    "applied_cases = {}\n",
    "\n",
    "def apply_logic(vl, pn, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic(pn, values)\n",
    "    if gr:\n",
    "        generic_cases.setdefault(gr, []).append(pn)\n",
    "    verb_rules = enrich_logic.get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                this_ok = F.item[feature].v(pn) == value\n",
    "            else:\n",
    "                this_ok = condition(vl, pn)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            applied_cases.setdefault((vl, i), []).append(pn)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "pnode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "link\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "phrase_text\n",
      "function\n",
      "valence\n",
      "grammatical\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    link\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    clause_text    \n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    phrase_text\n",
    "    function\n",
    "'''.strip().split() + list(enrich_fields)\n",
    "\n",
    "field_names = []\n",
    "for f in COMMON_FIELDS: field_names.append(f)\n",
    "for i in range(max((len(CLAUSE_FIELDS), len(PHRASE_FIELDS)))):\n",
    "    pf = PHRASE_FIELDS[i] if i < len(PHRASE_FIELDS) else '--'\n",
    "    field_names.append(pf)\n",
    "    \n",
    "fillrows = len(CLAUSE_FIELDS) - len(PHRASE_FIELDS)\n",
    "cfillrows = 0 if fillrows >= 0 else -fillrows\n",
    "pfillrows = fillrows if fillrows >= 0 else 0\n",
    "inf('\\n'.join(field_names), withtime=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1m 23s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BWa_etcbc4b.csv (10817 rows)\n",
      " 1m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NTN_etcbc4b.csv (9659 rows)\n",
      " 1m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NFa_etcbc4b.csv (2826 rows)\n",
      " 1m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/PQD_etcbc4b.csv (1269 rows)\n",
      " 1m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NPL_etcbc4b.csv (1915 rows)\n",
      " 1m 24s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BRa_etcbc4b.csv (219 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/HLK_etcbc4b.csv (5672 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/FJM_etcbc4b.csv (2857 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NWS_etcbc4b.csv (613 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oBR_etcbc4b.csv (2309 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CJT_etcbc4b.csv (375 rows)\n",
      " 1m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/QRa_etcbc4b.csv (3662 rows)\n",
      " 1m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JYa_etcbc4b.csv (4495 rows)\n",
      " 1m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oLH_etcbc4b.csv (3821 rows)\n",
      " 1m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/SWR_etcbc4b.csv (1259 rows)\n",
      " 1m 27s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oFH_etcbc4b.csv (11048 rows)\n",
      " 1m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CWB_etcbc4b.csv (4238 rows)\n",
      " 1m 29s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JRD_etcbc4b.csv (1553 rows)\n",
      " 1m 29s Done\n",
      "2 rules applied\n",
      "direct_object => principal_direct_object\n",
      "\t6473 phrases: 605755, 606136, 606853, 606875, 607210, 610073, 610589, 611505, 612463, 612947\n",
      "\n",
      "non-object => _promoted_direct_object\n",
      "\t1468 phrases: 606137, 611592, 613369, 613419, 613636, 623449, 634755, 634816, 634825, 636629\n",
      "\n",
      "complement => indirect_object\n",
      "\t1675 phrases: 606715, 606885, 606980, 607018, 607211, 607224, 608957, 609021, 609045, 609736\n",
      "\n",
      "complement => location\n",
      "\t4096 phrases: 606803, 606854, 607721, 607724, 607780, 608092, 608166, 608238, 608280, 608470\n",
      "\n",
      "predicate+complement => predicate+indirect_object\n",
      "\t   4 phrases: 853847, 691074, 654844, 704297\n",
      "\n",
      "predicate+complement => location\n",
      "\t  20 phrases: 762650, 773835, 774139, 774162, 784987, 670130, 646365, 654810, 654899, 737240\n",
      "\n",
      "13736 generic applications in total\n",
      "CJT-3 if function   = Cmpl     AND is_lex_local   \n",
      "      lexical    => location       \n",
      "      semantic   => location       \n",
      "\n",
      "\t   7 phrases: 606396, 619338, 630956, 654145, 776266, 789542, 797377\n",
      "\n",
      "CJT-1 if function   = Adju     AND has_L           AND is_lex_personal\n",
      "      semantic   => benefactive    \n",
      "\n",
      "\t   5 phrases: 615130, 630648, 712015, 794512, 797440\n",
      "\n",
      "12 specific applications in total\n",
      " 1m 29s 52110  phrases seen 1  time(s)\n",
      " 1m 29s 181    phrases seen 2  time(s)\n",
      " 1m 29s 9      phrases seen 3  time(s)\n",
      " 1m 29s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cl = L.u('clause', wn)\n",
    "        if cl in clauses_seen: continue\n",
    "        clauses_seen.add(cl)\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, lnx, vl, vstem, vt)\n",
    "        clause_fields = (ct,)\n",
    "        rows.append(common_fields + clause_fields + (('',)*cfillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            phrases_seen[pn] += 1\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vt)\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = pf_corr.get(pn, None) or F.function.v(pn)\n",
    "            phrase_fields = (pt, pf) + apply_logic(vl, pn, transform[pf])            \n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated enrichment sheet for verb {} ({} rows)'.format(filename, len(rows)))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "inf('Done')\n",
    "inf('{} rules applied'.format(len(applied_cases)), withtime=False)\n",
    "totaln = 0\n",
    "for rule_id in generic_cases:\n",
    "    cases = generic_cases[rule_id]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    inf('{}\\n\\t{:>4} phrases: {}\\n'.format(\n",
    "        grule_as_str[rule_id], n, ', '.join(str(c) for c in cases[0:10]),\n",
    "    ), withtime=False)\n",
    "inf('{} generic applications in total'.format(totaln), withtime=False)\n",
    "totaln = 0\n",
    "for rule_id in applied_cases:\n",
    "    cases = applied_cases[rule_id]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    inf('{}\\n\\t{:>4} phrases: {}\\n'.format(\n",
    "        rule_as_str(*rule_id), n, ', '.join(str(c) for c in cases[0:10]),\n",
    "    ), withtime=False)\n",
    "inf('{} specific applications in total'.format(totaln), withtime=False)\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showcase(pn):\n",
    "    inf('''{} {}\\n{}'''.format(\n",
    "        pf_corr.get(pn, None) or F.function.v(pn),\n",
    "        T.words(L.d('word', pn), fmt='ec'), \n",
    "        T.text(\n",
    "            book=F.book.v(L.u('book', pn)), \n",
    "            chapter=int(F.chapter.v(L.u('chapter', pn))),\n",
    "            verse=int(F.verse.v(L.u('verse', pn))), \n",
    "            fmt='ec', lang='la',\n",
    "        ),\n",
    "    ), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreC PQWDJ HLWJ LM#PXTM \n",
      "Numeri 26:57\tW>LH PQWDJ HLWJ LM#PXTM LGR#WN M#PXT HGR#NJ LQHT M#PXT HQHTJ LMRRJ M#PXT HMRRJ00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase(654844)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t154354, 322818, 154964, 75702\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t90243, 93571, 29637, 284965, 289859, 136745, 257293, 289871, 154354, 154964, 9525, 257016, 284989, 93598\n",
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t26118, 26127, 146447, 187920, 197138, 272406, 95257, 184350, 398368, 289826, 201253, 24616, 78897, 401459, 100410, 32829, 100413, 198208, 5698, 200258, 100938, 24653, 141902, 112207, 186960, 24658, 196690, 28764, 34400, 298594, 248931, 132198, 162918, 12402, 5747, 146044, 396927, 153216, 134792, 151176, 188042, 97419, 426120, 257165, 136338, 21656, 162970, 200349, 214687, 24740, 257192, 158378, 100527, 25777, 160434, 214707, 4789, 4793, 272569, 139963, 90812, 249020, 38595, 113861, 138448, 8920, 282841, 19166, 20703, 26850, 43235, 145127, 8424, 8937, 170729, 397032, 254703, 154354, 200948, 426230, 176376, 79609, 165626, 206075, 208636, 27391, 269569, 106246, 157447, 26380, 149785, 170782, 211232, 126758, 26414, 27438, 246062, 109363, 172340, 249140, 398134, 64828, 26431, 16704, 4929, 168771, 154964, 132955, 393569, 47460, 157541, 47466, 100206, 37232, 269170, 23415, 410999, 23933, 24448, 78208, 133518, 25999, 191381, 12698, 19355, 24476, 170909, 18344, 157608, 267689, 244660, 256952, 8633, 63419, 167359, 175553, 138694, 110536, 175561, 108490, 111051, 143820, 37324, 192973, 264137, 5586, 99795, 11732, 170963, 20438, 218583, 269285, 25062, 110576, 26099, 184315, 256511\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u('clause', w)\n",
    "        ps = L.d('phrase', c)\n",
    "        phs = {p for p in ps if len({w for w in L.d('word', p) if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, None) or F.function.v(p)) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        inf('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), withtime=False)\n",
    "        if show_results: inf('\\t{}'.format(', '.join(str(x) for x in hl[f])), withtime=False)\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oBR_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oFH_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oLH_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BRa_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BWa_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CJT_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CWB_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/FJM_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/HLK_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JRD_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JYa_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NFa_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NPL_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NTN_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NWS_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/PQD_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/QRa_etcbc4b.csv\n",
      " 4m 58s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4m 58s OK: The used blank enrichment sheets have legal values\n",
      " 4m 58s OK: The used blank enrichment sheets are consistent\n",
      " 4m 58s OK: The used filled enrichment sheets have legal values\n",
      " 4m 58s OK: The used filled enrichment sheets are consistent\n",
      " 4m 58s OK: all enriched nodes where phrase nodes\n",
      " 4m 58s OK: all enriched nodes occurred in the blank sheet\n",
      " 4m 58s OK: there are 416 manual correction/enrichment annotations\n",
      "COR                                    => 2_Kings 22:6 726393 NA,NA,NA,NA\n",
      "\tLEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "\t\tW:JIT.:NW. >OTOW L:<OF;J HAM.:L@>K@H LEX@R@CIJm W:LAB.ONIJm W:LAG.OD:RIJm \n",
      "COR                                    => Genesis 1:27 605442 complement,principal_direct_object,,\n",
      "\tZ@K@R W.N:Q;B@H \n",
      "\t\tZ@K@R W.N:Q;B@H B.@R@> >OT@m00\n",
      "\n",
      "COR                                    => Genesis 5:2 606420 complement,direct_object,,\n",
      "\tZ@K@R W.N:Q;B@H \n",
      "\t\tZ@K@R W.N:Q;B@H B.:R@>@m \n",
      "COR                                    => Isaiah 4:5 728419 adjunct,NA,location,location\n",
      "\t<AL K.@L&M:KOWn HAR&YIJ.OWn W:<AL&MIQ:R@>EH@ \n",
      "\t\tW.B@R@> J:HW@H <AL K.@L&M:KOWn HAR&YIJ.OWn W:<AL&MIQ:R@>EH@ <@N@n 05 JOWM@m \n",
      "COR                                    => Psalms 89:48 799849 complement,principal_direct_object,,\n",
      "\tC.@W:> \n",
      "\t\t<AL&MAH&C.@W:> B.@R@>T@ K@L&B.:N;J&>@D@m00\n",
      "\n",
      "COR                                    => Genesis 45:25 621386 NA,NA,NA,NA\n",
      "\t>EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "\t\tWAJ.@BO>W. >EREy K.:NA<An >EL&JA<:AQOB >:ABIJHEm00\n",
      "\n",
      "COR                                    => Exodus 14:16 627778 adjunct,*,location,location\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tW:J@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H00\n",
      "\n",
      "COR                                    => Exodus 14:22 627852 adjunct,*,location,location\n",
      "\tB.:TOWk: HAJ.@m \n",
      "\t\tWAJ.@BO>W. B:N;J&JIF:R@>;L B.:TOWk: HAJ.@m B.AJ.AB.@C@H \n",
      "COR                                    => Leviticus 4:23 637127 NA,NA,NA,NA\n",
      "\tF:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "\t\tW:H;BIJ> >ET&Q@R:B.@NOW F:<IJR <IZ.IJm Z@K@R T.@MIJm00\n",
      "\n",
      "COR                                    => Leviticus 4:28 637188 NA,NA,NA,NA\n",
      "\tF:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H \n",
      "\t\tW:H;BIJ> Q@R:B.@NOW F:<IJRAT <IZ.IJm T.:MIJM@H N:Q;B@H <AL&XAV.@>TOW \n",
      "... AND 406 ANNOTATIONS MORE\n",
      " 4m 59s 52110  phrases seen 1  time(s)\n",
      " 4m 59s 181    phrases seen 2  time(s)\n",
      " 4m 59s 9      phrases seen 3  time(s)\n",
      " 4m 59s Total phrases seen: 52300\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def read_enrich(rootdir): # rootdir will not be used, data is computed from sheets\n",
    "    pf_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_phrase = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                msg('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            with open(filename) as f:\n",
    "                header = f.__next__()\n",
    "                for line in f:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    pn = int(fields[2])\n",
    "                    if pn < 0: continue\n",
    "                    phrases_seen[pn] += 1\n",
    "                    vvals = tuple(fields[-4:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][pn].append((verb, f, v))\n",
    "                    vresults[is_filled][pn] = vvals\n",
    "                    if pn in pf_enriched[is_filled]:\n",
    "                        if pn not in repeated[is_filled]:\n",
    "                            repeated[is_filled][pn] = [pf_enriched[is_filled][pn]]\n",
    "                        repeated[is_filled][pn].append((verb, vvals))\n",
    "                    else:\n",
    "                        pf_enriched[is_filled][pn] = (verb, vvals)\n",
    "                    if F.otype.v(pn) != 'phrase': \n",
    "                        non_phrase[pn].append(verb)\n",
    "            for pn in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if pn not in vresults[False]:\n",
    "                    wrong_node[pn].append(verb)\n",
    "            for pn in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                f_corr = pn in pf_corr                               # manual correction in phrase function\n",
    "                s_manual = pn in vresults[True] and vresults[False][pn] != vresults[True][pn] # real change\n",
    "                these_results = vresults[True][pn] if s_manual else vresults[False][pn]\n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((pn,)+these_results+(f_corr, s_manual))\n",
    "                results.append((pn,)+these_results+(f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                msg('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), withtime=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    msg('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), withtime=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        else:\n",
    "            inf('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        msg('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), withtime=False)\n",
    "                        for vals in overview:\n",
    "                            msg('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), withtime=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        inf('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), withtime=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        if nerrors == 0:\n",
    "            inf('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Enrichments have been applied to non-phrase nodes:')\n",
    "        for x in sorted(non_phrase)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} Node {} is not a phrase but a {}'.format(\n",
    "                non_phrase[x], T.passage(x), x, F.otype.v(x),\n",
    "            ), withtime=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        msg('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} node {}'.format(\n",
    "                non_phrase[x], T.passage(x), x,\n",
    "            ), withtime=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        inf('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_corr, s_manual) = r\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "            inf('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                T.passage(x), x, ','.join(vals), px, cx\n",
    "            ), withtime=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: inf('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        msg('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-06'},\n",
    "    [\n",
    "        (None, 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in list(enrich_fields.keys())+['f_correction', 's_manual']\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Annox complements\n",
    "We load the s into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in ``enrich_fields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.00s DETAIL: load main: X. [node]  -> \n",
      "  1.48s DETAIL: load main: X. [e]  -> \n",
      "  3.87s DETAIL: load main: G.node_anchor_min\n",
      "  3.97s DETAIL: load main: G.node_anchor_max\n",
      "  4.07s DETAIL: load main: G.node_sort\n",
      "  4.17s DETAIL: load main: G.node_sort_inv\n",
      "  4.75s DETAIL: load main: G.edges_from\n",
      "  4.87s DETAIL: load main: G.edges_to\n",
      "  5.01s LOGFILE=/Users/dirk/laf/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  5.01s PARSING ANNOTATION FILES\n",
      "  5.04s INFO: parsing complements.xml\n",
      "  7.57s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "     52300 good   annots   and     0 faulty ones\n",
      "    313800 good   features and     0 faulty ones\n",
      "     52300 distinct xml identifiers\n",
      "\n",
      "  7.57s MODELING RESULT FILES\n",
      "  7.57s INFO: CONNECTIVITY\n",
      "  7.76s WRITING RESULT FILES for a\n",
      "  7.76s DETAIL: write annox: F.JanetDyk_ft_f_correction [node] \n",
      "  7.84s DETAIL: write annox: F.JanetDyk_ft_grammatical [node] \n",
      "  7.91s DETAIL: write annox: F.JanetDyk_ft_lexical [node] \n",
      "  7.96s DETAIL: write annox: F.JanetDyk_ft_s_manual [node] \n",
      "  8.03s DETAIL: write annox: F.JanetDyk_ft_semantic [node] \n",
      "  8.07s DETAIL: write annox: F.JanetDyk_ft_valence [node] \n",
      "  8.15s DETAIL: write annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  8.15s DETAIL: write annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  8.15s DETAIL: write annox: F.etcbc4_ph_phono [node] \n",
      "  8.92s DETAIL: write annox: F.etcbc4_ph_phono_sep [node] \n",
      "  9.10s END   COMPILE a: complements\n",
      "  9.10s USING annox DATA COMPILED AT: 2016-06-15T14-50-36\n",
      "  9.10s DETAIL: keep main: G.node_anchor_min\n",
      "  9.10s DETAIL: keep main: G.node_anchor_max\n",
      "  9.10s DETAIL: keep main: G.node_sort\n",
      "  9.11s DETAIL: keep main: G.node_sort_inv\n",
      "  9.11s DETAIL: keep main: G.edges_from\n",
      "  9.11s DETAIL: keep main: G.edges_to\n",
      "  9.11s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  9.11s DETAIL: keep main: F.etcbc4_ft_lex [node] \n",
      "  9.11s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  9.11s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_g_lex [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_g_word [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ph_phono [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_ph_phono_sep [node] \n",
      "  9.11s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  9.11s DETAIL: clear annox: F.etcbc4_db_otype [node] \n",
      "  9.11s DETAIL: clear annox: F.etcbc4_ft_g_lex [node] \n",
      "  9.11s DETAIL: clear annox: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ft_g_word [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ft_lex [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ft_lex_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ph_phono [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_ph_phono_sep [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_sft_book [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_sft_chapter [node] \n",
      "  9.12s DETAIL: clear annox: F.etcbc4_sft_verse [node] \n",
      "  9.12s DETAIL: load main: F.JanetDyk_ft_f_correction [node] \n",
      "  9.12s DETAIL: load main: F.JanetDyk_ft_grammatical [node] \n",
      "  9.12s DETAIL: load main: F.JanetDyk_ft_lexical [node] \n",
      "  9.12s DETAIL: load main: F.JanetDyk_ft_s_manual [node] \n",
      "  9.12s DETAIL: load main: F.JanetDyk_ft_semantic [node] \n",
      "  9.13s DETAIL: load main: F.JanetDyk_ft_valence [node] \n",
      "  9.13s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "    10s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "    11s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "    11s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "    11s DETAIL: load annox: F.JanetDyk_ft_f_correction [node] \n",
      "    11s DETAIL: load annox: F.JanetDyk_ft_grammatical [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_lexical [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_s_manual [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_semantic [node] \n",
      "    12s DETAIL: load annox: F.JanetDyk_ft_valence [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_db_oid [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_db_otype [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_function [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_lex [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_sp [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_ft_vs [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n",
      "    12s DETAIL: load annox: F.etcbc4_sft_verse [node] \n",
      "    12s INFO: LOADING PREPARED data: please wait ... \n",
      "    12s DETAIL: keep prep: G.node_sort\n",
      "    12s DETAIL: keep prep: G.node_sort_inv\n",
      "    12s DETAIL: keep prep: L.node_up\n",
      "    12s DETAIL: keep prep: L.node_down\n",
      "    12s DETAIL: keep prep: V.verses\n",
      "    12s DETAIL: keep prep: V.books_la\n",
      "    12s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    16s INFO: LOADED PREPARED data\n",
      "    16s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-06-15T14-50-43\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "        function s_manual f_correction\n",
    "    ''' + ' '.join(enrich_fields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.94s collecting phrases ...\n",
      "  2.80s  10000 phrases in  2910 clauses ...\n",
      "  3.55s  20000 phrases in  5916 clauses ...\n",
      "  4.27s  30000 phrases in  9055 clauses ...\n",
      "  5.00s  40000 phrases in 12168 clauses ...\n",
      "  5.74s  50000 phrases in 15374 clauses ...\n",
      "  5.95s  52300 phrases in 16053 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = outfile('all.csv')\n",
    "NALLFIELDS = 10\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "inf('collecting phrases ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'valence',\n",
    "    'grammatical',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'phrase text',\n",
    "    'phrase node',\n",
    "))\n",
    "i = 0\n",
    "j = 0\n",
    "c = 0\n",
    "CHUNK_SIZE = 10000\n",
    "for cn in sorted(clause_verb_selected):\n",
    "    c += 1\n",
    "    verbs = sorted(clause_verb_selected[cn])\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        T.passage(cn),\n",
    "        ' '.join(F.lex.v(verb) for verb in verbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.words(L.d('word', cn), fmt='ec').replace('\\n', ' '),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d('phrase', cn):\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            inf('{:>6} phrases in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'True' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'True' else '',\n",
    "            T.passage(pn),\n",
    "            '',\n",
    "            F.valence.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.words(L.d('word', pn), fmt='ec').replace('\\n', ' '),\n",
    "            pn,\n",
    "        ))\n",
    "f.close()\n",
    "inf('{:>6} phrases in {:>5} clauses done'.format(i, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
