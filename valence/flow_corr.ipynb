{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.25\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os, collections\n",
    "from copy import deepcopy\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  5.90s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  5.90s INFO: LOADING PREPARED data: please wait ... \n",
      "  5.90s prep prep: G.node_sort\n",
      "  6.00s prep prep: G.node_sort_inv\n",
      "  6.52s prep prep: L.node_up\n",
      "  9.82s prep prep: L.node_down\n",
      "    16s prep prep: V.verses\n",
      "    16s prep prep: V.books_la\n",
      "    16s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    18s INFO: LOADED PREPARED data\n",
      "    18s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-04-06T11-51-22\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, '--', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex uvf\n",
    "        function\n",
    "        chapter verse\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "annox_basedir = API['data_dir']\n",
    "annox_subdir = 'cpl'\n",
    "annox_dir = '{}/{}'.format(annox_basedir, annox_subdir)\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        msg('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "legal_values['function'] = {F.function.v(p) for p in F.otype.s('phrase')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    BoundErr='this phrase is part of another phrase and does not merit its own function value',\n",
    ")\n",
    "\n",
    "legal_values['function'] |= set(error_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 22m 54s Finding occurrences\n",
      " 1h 22m 56s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BR 556   occurrences\n",
      "<FH 2629  occurrences\n",
      "<LH 890   occurrences\n",
      "BR> 54    occurrences\n",
      "BW> 2570  occurrences\n",
      "CJT 85    occurrences\n",
      "CWB 1056  occurrences\n",
      "FJM 609   occurrences\n",
      "HLK 1554  occurrences\n",
      "JRD 377   occurrences\n",
      "JY> 1069  occurrences\n",
      "NF> 656   occurrences\n",
      "NPL 445   occurrences\n",
      "NTN 2017  occurrences\n",
      "NWS 159   occurrences\n",
      "PQD 303   occurrences\n",
      "QR> 883   occurrences\n",
      "SWR 297   occurrences\n"
     ]
    }
   ],
   "source": [
    "msg('Finding occurrences')\n",
    "occs = collections.defaultdict(list)\n",
    "for n in F.otype.s('word'):\n",
    "    if F.sp.v(n) != 'verb': continue\n",
    "    lex = F.lex.v(n).rstrip('/=[')\n",
    "    occs[lex].append(n)\n",
    "msg('Done')\n",
    "for verb in sorted(verbs):\n",
    "    print('{} {:<5} occurrences'.format(verb, len(occs[verb])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1h 23m 12s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oFH_etcbc4b.csv\n",
      " 1h 23m 12s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/HLK_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NPL_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CJT_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CWB_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JYa_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NFa_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/FJM_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/PQD_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oBR_etcbc4b.csv\n",
      " 1h 23m 13s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NWS_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BWa_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oLH_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/SWR_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JRD_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BRa_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/QRa_etcbc4b.csv\n",
      " 1h 23m 14s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NTN_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = '{} {}:{}'.format(T.book_name(bn, lang='en'), ch, vs)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        row = [cln, wn, passage_label, lnx, vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            row.extend((pn, pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oBR_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "    40s BR>: Found     2 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n",
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "    40s BW>: Found    57 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n",
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "    40s CJT: Found    58 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n",
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "    40s CWB: Found   108 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "    40s HLK: Found   156 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JRD_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JYa_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NPL_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    40s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "    40s QR>: Found   159 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "    40s OK: Corrected phrases did not receive multiple corrections\n",
      "    40s OK: all corrected nodes where phrase nodes\n",
      "    40s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    40s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            msg('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            inf('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        inf('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                msg('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            inf('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            inf('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            msg('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            inf('OK: all corrected values are legal')\n",
    "        \n",
    "read_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    44s Enrich field specification OK\n",
      "valence = {adjunct, complement, core}\n",
      "grammatical = {*, copula, copula+subject, object, predication, predication+object, predication+subject, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, location, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "grammatical\n",
    "    *\n",
    "    subject\n",
    "    object\n",
    "    copula\n",
    "    copula+subject\n",
    "    predication\n",
    "    predication+subject\n",
    "    predication+object\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "if None in enrich_fields:\n",
    "    msg('Invalid enrich field specification')\n",
    "else:\n",
    "    inf('Enrich field specification OK')\n",
    "for ef in enrich_fields:\n",
    "    print('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specs = '''\n",
    "Adju\tAdjunct\tadjunct\tNA\t\t\n",
    "Cmpl\tComplement\tcomplement\t*\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula+subject\t\t\n",
    "Exst\tExistence\tcore\tcopula\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tsubject\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tsubject\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula+subject\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\tNA\n",
    "Objc\tObject\tcomplement\tobject\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tpredication+subject\t\t\n",
    "PreC\tPredicate complement\tcore\tpredication\t\t\n",
    "Pred\tPredicate\tcore\tpredication\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tpredication+object\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tpredication+subject\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tpredication+object\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\tNA\n",
    "Subj\tSubject\tcore\tsubject\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\tNA'''.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    48s Defaults OK (124 good)\n"
     ]
    }
   ],
   "source": [
    "transform = {}\n",
    "for line in specs:\n",
    "    x = line.split('\\t') \n",
    "    transform[x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "for e in error_values:\n",
    "    transform[e] = dict(zip(enrich_fields, ['NA']*4))\n",
    "\n",
    "errors = 0\n",
    "good = 0\n",
    "for f in transform:\n",
    "    for e in enrich_fields:\n",
    "        val = transform[f][e]\n",
    "        if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "            msg('Defaults for `{}`: wrong `{}` value: \"{}\"'.format(f, e, val))\n",
    "            errors += 1\n",
    "        else: good += 1\n",
    "if errors:\n",
    "    msg('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    inf('Defaults OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence         grammatical          lexical         semantic       \n",
      "Adju    : adjunct         NA                                                  \n",
      "BoundErr: NA              NA                   NA              NA             \n",
      "Cmpl    : complement      *                                                   \n",
      "Conj    : NA              NA                   NA              NA             \n",
      "EPPr    : NA              copula                                              \n",
      "ExsS    : core            copula+subject                                      \n",
      "Exst    : core            copula                                              \n",
      "Frnt    : NA              NA                   NA              NA             \n",
      "IntS    : core            subject                                             \n",
      "Intj    : NA              NA                   NA              NA             \n",
      "Loca    : adjunct         NA                   location        location       \n",
      "ModS    : core            subject                                             \n",
      "Modi    : NA              NA                   NA              NA             \n",
      "NCoS    : core            copula+subject                                      \n",
      "NCop    : core            copula                                              \n",
      "Nega    : NA              NA                   NA              NA             \n",
      "Objc    : complement      object                                              \n",
      "PrAd    : adjunct         NA                                                  \n",
      "PrcS    : core            predication+subject                                 \n",
      "PreC    : core            predication                                         \n",
      "PreO    : core            predication+object                                  \n",
      "PreS    : core            predication+subject                                 \n",
      "Pred    : core            predication                                         \n",
      "PtcO    : core            predication+object                                  \n",
      "Ques    : NA              NA                   NA              NA             \n",
      "Rela    : NA              NA                   NA              NA             \n",
      "Subj    : core            subject                                             \n",
      "Supp    : adjunct         NA                                   benefactive    \n",
      "Time    : adjunct         NA                   time            time           \n",
      "Unkn    : NA              NA                   NA              NA             \n",
      "Voct    : NA              NA                   NA              NA             \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: {:<15} {:<20} {:<15} {:<15}'\n",
    "print(ltpl.format('func', *enrich_fields))\n",
    "for f in sorted(transform):\n",
    "    sfs = transform[f]\n",
    "    print(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 1 and F.lex.v(words[1] in personal_lexemes)\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_logic = {\n",
    "    'CJT': [\n",
    "        (('semantic', 'benefactive'), ('function:Adju', has_L, is_lex_personal)),\n",
    "        (('lexical', 'location'), ('function:Cmpl', has_H_locale)),\n",
    "        (('lexical', 'location'), ('function:Cmpl', is_lex_local)),\n",
    "    ],    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT-1 semantic   => benefactive     if function   = Adju     AND has_L           AND is_lex_personal\n",
      "CJT-2 lexical    => location        if function   = Cmpl     AND has_H_locale   \n",
      "CJT-3 lexical    => location        if function   = Cmpl     AND is_lex_local   \n",
      "15m 42s All 3 rules OK\n"
     ]
    }
   ],
   "source": [
    "def rule_as_str(vl, i, sf, sfval, conditions):\n",
    "    return '{}-{} {:<10} => {:<15} if {}'.format(\n",
    "                    vl, i+1, sf, sfval,\n",
    "                    ' AND '.join(\n",
    "                        '{:<10} = {:<8}'.format(\n",
    "                                *c.split(':')\n",
    "                            ) if type(c) is str else '{:<15}'.format(\n",
    "                                c.__name__\n",
    "                            ) for c in conditions,\n",
    "                    ),\n",
    "    )\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    rules = 0\n",
    "    for vl in sorted(enrich_logic):\n",
    "        for (i, ((sf, sfval), conditions)) in enumerate(enrich_logic[vl]):\n",
    "            rules += 1\n",
    "            inf(rule_as_str(vl, i, sf, sfval, conditions), withtime=False)\n",
    "            if sf not in enrich_fields:\n",
    "                msg('\"{}\" not a valid enrich field'.format(sf), withtime=False)\n",
    "                errors += 1\n",
    "            elif sfval not in enrich_fields[sf]:\n",
    "                msg('`{}`: \"{}\" not a valid enrich field value'.format(sf, sfval), withtime=False)\n",
    "                errors += 1\n",
    "            for c in conditions:\n",
    "                if type(c) == str:\n",
    "                    x = c.split(':')\n",
    "                    if len(x) != 2:\n",
    "                        msg('Wrong feature condition {}'.format(c))\n",
    "                        errors += 1\n",
    "                    else:\n",
    "                        (feat, val) = x\n",
    "                        if feat not in legal_values:\n",
    "                            msg('Feature `{}` not in use'.format(feat))\n",
    "                            errors += 1\n",
    "                        elif val not in legal_values[feat]:\n",
    "                            msg('Feature `{}`: not a valid value \"{}\"'.format(feat, val))\n",
    "                            errors += 1\n",
    "    if errors:\n",
    "        msg('There were {} errors in {} rules'.format(errors, rules))\n",
    "    else:\n",
    "        inf('All {} rules OK'.format(rules))\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "applied_cases = {}\n",
    "\n",
    "def apply_logic(vl, pn, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    verb_rules = enrich_logic.get(vl, [])\n",
    "    for (i, ((sf, sfval), conditions)) in enumerate(verb_rules):\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                this_ok = F.item[feature].v(pn) == value\n",
    "            else:\n",
    "                this_ok = condition(vl, pn)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            values[sf] = sfval\n",
    "            applied_cases.setdefault(rule_as_str(vl, i, sf, sfval, conditions), []).append(pn)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "pnode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "link\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "phrase_text\n",
      "function\n",
      "valence\n",
      "grammatical\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    pnode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    link\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    clause_text    \n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    phrase_text\n",
    "    function\n",
    "'''.strip().split() + list(enrich_fields)\n",
    "\n",
    "field_names = []\n",
    "for f in COMMON_FIELDS: field_names.append(f)\n",
    "for i in range(max((len(CLAUSE_FIELDS), len(PHRASE_FIELDS)))):\n",
    "    pf = PHRASE_FIELDS[i] if i < len(PHRASE_FIELDS) else '--'\n",
    "    field_names.append(pf)\n",
    "    \n",
    "fillrows = len(CLAUSE_FIELDS) - len(PHRASE_FIELDS)\n",
    "cfillrows = 0 if fillrows >= 0 else -fillrows\n",
    "pfillrows = fillrows if fillrows >= 0 else 0\n",
    "print('\\n'.join(field_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39m 25s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oFH_etcbc4b.csv (11048 rows)\n",
      "39m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oBR_etcbc4b.csv (2279 rows)\n",
      "39m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/HLK_etcbc4b.csv (5672 rows)\n",
      "39m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NPL_etcbc4b.csv (1915 rows)\n",
      "39m 26s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NWS_etcbc4b.csv (613 rows)\n",
      "39m 27s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BWa_etcbc4b.csv (10817 rows)\n",
      "39m 27s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CJT_etcbc4b.csv (375 rows)\n",
      "39m 27s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/CWB_etcbc4b.csv (4161 rows)\n",
      "39m 27s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JYa_etcbc4b.csv (4495 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/oLH_etcbc4b.csv (3821 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/SWR_etcbc4b.csv (1259 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/FJM_etcbc4b.csv (2857 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/JRD_etcbc4b.csv (1553 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/BRa_etcbc4b.csv (196 rows)\n",
      "39m 28s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/QRa_etcbc4b.csv (3272 rows)\n",
      "39m 29s Generated enrichment sheet for verb /Users/dirk/Dropbox/SYNVAR/enrich_blank/NTN_etcbc4b.csv (9659 rows)\n",
      "39m 29s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rules applied\n",
      "CJT-1 semantic   => benefactive     if function   = Adju     AND has_L           AND is_lex_personal\n",
      "\t  10 phrases: 615130, 630648, 712015, 794512, 797440, 615130, 630648, 712015, 794512, 797440\n",
      "CJT-3 lexical    => location        if function   = Cmpl     AND is_lex_local   \n",
      "\t  14 phrases: 606396, 619338, 630956, 654145, 776266, 789542, 797377, 606396, 619338, 630956\n",
      "24 applications in total\n"
     ]
    }
   ],
   "source": [
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cl = L.u('clause', wn)\n",
    "        if cl in clauses_seen: continue\n",
    "        clauses_seen.add(cl)\n",
    "        cn = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, lnx, vl, vstem, vt)\n",
    "        clause_fields = (ct,)\n",
    "        rows.append(common_fields + clause_fields + (('',)*cfillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vt)\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = pf_corr.get(pn, None) or F.function.v(pn)\n",
    "            phrase_fields = (pt, pf) + apply_logic(vl, pn, transform[pf])            \n",
    "            rows.append(common_fields + phrase_fields + (('',)*pfillrows))\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated enrichment sheet for verb {} ({} rows)'.format(filename, len(rows)))\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "msg('Done')\n",
    "print('{} rules applied'.format(len(applied_cases)))\n",
    "totaln = 0\n",
    "for rule in applied_cases:\n",
    "    cases = applied_cases[rule]\n",
    "    n = len(cases)\n",
    "    totaln += n\n",
    "    print('{}\\n\\t{:>4} phrases: {}'.format(rule, n, ', '.join(str(c) for c in cases[0:10])))\n",
    "print('{} applications in total'.format(totaln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t154354, 322818, 154964, 75702\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t90243, 93571, 29637, 284965, 289859, 136745, 257293, 289871, 154354, 154964, 9525, 257016, 284989, 93598\n",
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t26118, 26127, 146447, 187920, 197138, 272406, 95257, 184350, 398368, 289826, 201253, 24616, 78897, 401459, 100410, 32829, 100413, 198208, 5698, 200258, 100938, 24653, 141902, 112207, 186960, 24658, 196690, 28764, 34400, 298594, 248931, 132198, 162918, 12402, 5747, 146044, 396927, 153216, 134792, 151176, 188042, 97419, 426120, 257165, 136338, 21656, 162970, 200349, 214687, 24740, 257192, 158378, 100527, 25777, 160434, 214707, 4789, 4793, 272569, 139963, 90812, 249020, 38595, 113861, 138448, 8920, 282841, 19166, 20703, 26850, 43235, 145127, 8424, 8937, 170729, 397032, 254703, 154354, 200948, 426230, 176376, 79609, 165626, 206075, 208636, 27391, 269569, 106246, 157447, 26380, 149785, 170782, 211232, 126758, 26414, 27438, 246062, 109363, 172340, 249140, 398134, 64828, 26431, 16704, 4929, 168771, 154964, 132955, 393569, 47460, 157541, 47466, 100206, 37232, 269170, 23415, 410999, 23933, 24448, 78208, 133518, 25999, 191381, 12698, 19355, 24476, 170909, 18344, 157608, 267689, 244660, 256952, 8633, 63419, 167359, 175553, 138694, 110536, 175561, 108490, 111051, 143820, 37324, 192973, 264137, 5586, 99795, 11732, 170963, 20438, 218583, 269285, 25062, 110576, 26099, 184315, 256511\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u('clause', w)\n",
    "        ps = L.d('phrase', c)\n",
    "        phs = {p for p in ps if len({w for w in L.d('word', p) if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or F.function.v(p) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        print('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])))\n",
    "        if show_results: print('\\t{}'.format(', '.join(str(x) for x in hl[f])))\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    30s OK: Enriched phrases did not receive multiple enrichments\n",
      "    30s OK: all enriched nodes where phrase nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO file /Users/dirk/surfdrive/laf-fabric-data/cpl/BRa_enriched_etcbc4b.csv\n",
      "CJT: Found   291 enrichments in /Users/dirk/surfdrive/laf-fabric-data/cpl/CJT_enriched_etcbc4b.csv\n",
      "NO file /Users/dirk/surfdrive/laf-fabric-data/cpl/QRa_enriched_etcbc4b.csv\n",
      "[(605977, 'NA', 'NA', 'NA', 'NA'), (605978, 'complement', 'object', '', ''), (605979, 'core', 'predication', '', ''), (605980, 'complement', '*', '', ''), (606391, 'NA', 'NA', 'NA', 'NA'), (606392, 'core', 'predication', '', ''), (606393, 'adjunct', 'NA', '', ''), (606394, 'core', 'subject', '', ''), (606395, 'complement', 'object', '', ''), (606396, 'complement', '*', '', '')]\n"
     ]
    }
   ],
   "source": [
    "pf_enriched = set()\n",
    "repeated = collections.defaultdict(list)\n",
    "non_phrase = set()\n",
    "\n",
    "def read_enrich(rootdir):\n",
    "    results = []\n",
    "    for verb in sorted(verbs):\n",
    "        filename = '{}/{}'.format(rootdir, vfile(verb, 'enriched'))\n",
    "        if not os.path.exists(filename):\n",
    "            print('NO file {}'.format(filename))\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                pn = int(fields[2])\n",
    "                if pn < 0: continue\n",
    "                vvals = tuple(fields[-4:])\n",
    "                results.append((pn,)+vvals)\n",
    "                if pn in pf_enriched:\n",
    "                    repeated[pn] += vvals\n",
    "                else:\n",
    "                    pf_enriched.add(pn)\n",
    "                if F.otype.v(pn) != 'phrase': \n",
    "                    non_phrase.add(pn)\n",
    "\n",
    "        print('{}: Found {:>5} enrichments in {}'.format(verb, len(results), filename))\n",
    "    if len(repeated):\n",
    "        msg('ERROR: Some phrases have been enriched multiple times!')\n",
    "        for x in sorted(repeated):\n",
    "            print('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "    else:\n",
    "        msg('OK: Enriched phrases did not receive multiple enrichments')\n",
    "    if len(non_phrase):\n",
    "        msg('ERROR: Enrichments have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "    else:\n",
    "        msg('OK: all enriched nodes where phrase nodes')\n",
    "    print(results[0:10])\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-03'},\n",
    "    [\n",
    "        ('cpl', 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in enrich_fields\n",
    "        ))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Annox complements\n",
    "We load the s into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in ``enrich_fields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.00s DETAIL: load main: X. [node]  -> \n",
      "  1.44s DETAIL: load main: X. [e]  -> \n",
      "  4.00s DETAIL: load main: G.node_anchor_min\n",
      "  4.11s DETAIL: load main: G.node_anchor_max\n",
      "  4.22s DETAIL: load main: G.node_sort\n",
      "  4.33s DETAIL: load main: G.node_sort_inv\n",
      "  4.90s DETAIL: load main: G.edges_from\n",
      "  5.03s DETAIL: load main: G.edges_to\n",
      "  5.17s LOGFILE=/Users/dirk/surfdrive/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  5.17s PARSING ANNOTATION FILES\n",
      "  5.21s INFO: parsing complements.xml\n",
      "  5.23s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "       291 good   annots   and     0 faulty ones\n",
      "      1164 good   features and     0 faulty ones\n",
      "       291 distinct xml identifiers\n",
      "\n",
      "  5.23s MODELING RESULT FILES\n",
      "  5.23s INFO: CONNECTIVITY\n",
      "  5.39s WRITING RESULT FILES for a\n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_grammatical [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_lexical [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_semantic [node] \n",
      "  5.39s DETAIL: write annox: F.JanetDyk_ft_valence [node] \n",
      "  5.39s DETAIL: write annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  5.40s DETAIL: write annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  5.40s DETAIL: write annox: F.etcbc4_ph_phono [node] \n",
      "  6.16s DETAIL: write annox: F.etcbc4_ph_phono_sep [node] \n",
      "  6.35s END   COMPILE a: complements\n",
      "  6.35s USING annox DATA COMPILED AT: 2016-03-23T11-37-32\n",
      "  6.35s DETAIL: keep main: G.node_anchor_min\n",
      "  6.35s DETAIL: keep main: G.node_anchor_max\n",
      "  6.35s DETAIL: keep main: G.node_sort\n",
      "  6.35s DETAIL: keep main: G.node_sort_inv\n",
      "  6.35s DETAIL: keep main: G.edges_from\n",
      "  6.35s DETAIL: keep main: G.edges_to\n",
      "  6.35s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  6.36s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  6.36s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ph_phono [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_ph_phono_sep [node] \n",
      "  6.36s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_db_otype [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_lex_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ph_phono [node] \n",
      "  6.36s DETAIL: clear annox: F.etcbc4_ph_phono_sep [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_book [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_chapter [node] \n",
      "  6.37s DETAIL: clear annox: F.etcbc4_sft_verse [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_grammatical [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_lexical [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_semantic [node] \n",
      "  6.37s DETAIL: load main: F.JanetDyk_ft_valence [node] \n",
      "  6.37s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  7.47s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  7.62s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  7.87s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  8.18s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_grammatical [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_lexical [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_semantic [node] \n",
      "  8.44s DETAIL: load annox: F.JanetDyk_ft_valence [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_db_oid [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_db_otype [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_function [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_lex [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_sp [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_ft_vs [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_sft_chapter [node] \n",
      "  8.45s DETAIL: load annox: F.etcbc4_sft_verse [node] \n",
      "  8.45s INFO: LOADING PREPARED data: please wait ... \n",
      "  8.45s DETAIL: keep prep: G.node_sort\n",
      "  8.45s DETAIL: keep prep: G.node_sort_inv\n",
      "  8.46s DETAIL: keep prep: L.node_up\n",
      "  8.46s DETAIL: keep prep: L.node_down\n",
      "  8.46s DETAIL: keep prep: V.verses\n",
      "  8.46s DETAIL: keep prep: V.books_la\n",
      "  8.46s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    11s INFO: LOADED PREPARED data\n",
      "    11s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-23T11-37-36\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "        function\n",
    "    ''' + ' '.join(enrich_fields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
