{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/etcbc4easy-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label.\n",
    "\n",
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* transform the set of filled in correction sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "\n",
    "# 1. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.21\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n",
      "25m 49s END\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import collections\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  4.23s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  4.23s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  4.24s INFO: LOADING PREPARED data: please wait ... \n",
      "  4.24s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  4.79s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  4.79s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  6.01s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  6.01s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  6.01s INFO: LOADED PREPARED data\n",
      "  6.01s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-11T07-02-00\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, '--', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        function\n",
    "        chapter verse\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 1m 40s Finding occurrences\n",
      " 1m 42s Done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR> 48    occurrences\n",
      "CJT 85    occurrences\n",
      "QR> 743   occurrences\n"
     ]
    }
   ],
   "source": [
    "msg('Finding occurrences')\n",
    "occs = collections.defaultdict(list)\n",
    "for n in F.otype.s('word'):\n",
    "    lex = F.lex.v(n)\n",
    "    if lex.endswith('['):\n",
    "        lex = lex[0:-1]\n",
    "        occs[lex].append(n)\n",
    "msg('Done')\n",
    "for verb in sorted(verbs):\n",
    "    print('{} {:<5} occurrences'.format(verb, len(occs[verb])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1.2 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 8m 21s Generated correction sheet for verb CJT_blank_etcbc4b.csv\n",
      " 8m 21s Generated correction sheet for verb QRa_blank_etcbc4b.csv\n",
      " 8m 21s Generated correction sheet for verb BRa_blank_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "def vfile(verb, kind): return '{}_{}_{}{}.csv'.format(verb.replace('>','a').replace('<', 'o'), kind, source, version)\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        verb    \n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        passage_label = '{} {}:{}'.format(T.book_name(bn, lang='en'), F.chapter.v(vn), F.verse.v(vn))\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        row = [cln, wn, passage_label, vt]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            row.extend((pn, pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'blank')\n",
    "    row_file = outfile(filename)\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    msg('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 1.3 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corr(rootdir):\n",
    "    results = []\n",
    "    for verb in verbs:\n",
    "        filename = '{}/{}'.format(rootdir, vfile(verb, 'corrected'))\n",
    "        if not os.path.exists(filename):\n",
    "            print('NO file {}'.format(filename))\n",
    "            continue\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[4*i], fields[4*i+3])\n",
    "                    pc = pc.strip()\n",
    "                    if pc != '': results.append((int(pn), pc))\n",
    "        print('{}: Found {:>5} corrections in {}'.format(verb, len(results), filename))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we turn that data into LAF annotations. Every correction is stored in a new feature, with name `function`, label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT: Found     6 corrections in /Users/dirk/laf-fabric-data/cpl/CJT_corrected_etcbc4b.csv\n",
      "NO file /Users/dirk/laf-fabric-data/cpl/QRa_corrected_etcbc4b.csv\n",
      "NO file /Users/dirk/laf-fabric-data/cpl/BRa_corrected_etcbc4b.csv\n"
     ]
    }
   ],
   "source": [
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement corrections', 'date': '2016-02'},\n",
    "    [\n",
    "        ('cpl', 'complements', read_corr, (\n",
    "            ('JanetDyk', 'ft', 'function'),\n",
    "        ))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Check the corrections\n",
    "\n",
    "We load the corrections into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "We perform a few basic consistency checks.\n",
    "\n",
    "# 2.1 Annox complements\n",
    "Load the API again, but note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s USING annox DATA COMPILED AT: 2016-03-11T06-31-06\n",
      "  0.99s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.00s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.00s INFO: LOADING PREPARED data: please wait ... \n",
      "  1.00s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  1.56s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  1.56s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  2.94s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  2.94s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  2.95s INFO: LOADED PREPARED data\n",
      "  2.95s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-03-11T06-34-18\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        JanetDyk:ft.function etcbc4:ft.function\n",
    "    ''',\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='NORMAL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Checks\n",
    "\n",
    "We make sure that every correction applies to a node that corresponds to a phrase, and that no correction applies to the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21m 35s Checking corrections\n",
      "21m 38s Found 3 types of corrections\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmpl  => Loca      3 x\n",
      "Adju  => Benf      2 x\n",
      "Supp  => Adju-Benf     1 x\n",
      "NO ERRORS DETECTED\n"
     ]
    }
   ],
   "source": [
    "msg('Checking corrections')\n",
    "corr = collections.Counter()\n",
    "errors = collections.Counter()\n",
    "corrected_nodes = set()\n",
    "for n in NN():\n",
    "    c = F.JanetDyk_ft_function.v(n)\n",
    "    o = F.etcbc4_ft_function.v(n)\n",
    "    if c == None: continue\n",
    "    corr[(o, c)] += 1\n",
    "    if F.otype.v(n) != 'phrase': errors['Correction applied to non-phrase object'] += 1\n",
    "    elif n in corrected_nodes: errors['Phrase with multiple corrections'] += 1\n",
    "    corrected_nodes.add(n)\n",
    "    \n",
    "msg('Found {} types of corrections'.format(len(corr)))\n",
    "for ((o, c), n) in sorted(corr.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print('{:<5} => {:<5} {:>5} x'.format(o, c, n))\n",
    "if not errors:\n",
    "    print('NO ERRORS DETECTED')\n",
    "else:\n",
    "    print('THERE ARE ERRORS:')\n",
    "    for (e, n) in sorted(errors.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print('{:>5} x: {}'.format(n, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
