{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-small.png\"/></a>\n",
    "<a href=\"https://shebanq.ancient-data.org\" target=\"_blank\"><img align=\"left\"src=\"images/shebanq_logo_small.png\"/></a>\n",
    "<a href=\"http://dx.doi.org/10.17026/dans-z6y-skyh\" target=\"_blank\"><img align=\"left\"src=\"images/DANS-logo_small.png\"/></a>\n",
    "<a href=\"https://www.dbg.de/index.php?L=1\" target=\"_blank\"><img align=\"right\" src=\"images/DBG-small.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"right\" src=\"images/VU-ETCBC-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2016-03-14\n",
    "\n",
    "# 1. Datamodel: \n",
    "\n",
    "## 1.1 ETCBC data in the Emdros model\n",
    "\n",
    "See the [otype](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/otype.html) feature.\n",
    "\n",
    "And then the [overview](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/0_overview.html) of features.\n",
    "\n",
    "[SHEBANQ](https://shebanq.ancient-data.org) is your friend, especially the **Help** there.\n",
    "\n",
    "## 1.2 Let's go LAF\n",
    "\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/LAF.png\"/></a>\n",
    "\n",
    "## 1.3 Let's forget LAF\n",
    "\n",
    "**LAF-Fabric**\n",
    "* has read all the LAF-XML\n",
    "* has built a datastructure (graph)\n",
    "* has saved the data structure on disk\n",
    "* will load the relevant parts for you quickly\n",
    "\n",
    "# 2. API\n",
    "\n",
    "We'll start the API, but first we have to import the necessary modules.\n",
    "``sys, collections, re`` are not necessary for LAF-Fabric, but may come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.22\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, collections, re\n",
    "\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LafFabric` is a class offered by the `laf.fabric` module, and have created just one object of that class, and stored it in the variable `fabric`.\n",
    "\n",
    "Note the links to the documentation.\n",
    "\n",
    "LAF-Fabric can work with several data sources and versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source='etcbc'\n",
    "version='4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading data\n",
    "\n",
    "The `load` method is a function that listens to your data requirements, and manages to keep in memory exactly what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.00s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s DETAIL: COMPILING a: UP TO DATE\n",
      "  0.00s USING annox DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "  0.01s DETAIL: keep main: G.node_anchor_min\n",
      "  0.01s DETAIL: keep main: G.node_anchor_max\n",
      "  0.01s DETAIL: keep main: G.node_sort\n",
      "  0.01s DETAIL: keep main: G.node_sort_inv\n",
      "  0.01s DETAIL: keep main: G.edges_from\n",
      "  0.01s DETAIL: keep main: G.edges_to\n",
      "  0.01s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  0.01s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  0.01s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  0.01s DETAIL: keep annox: F.etcbc4_db_otype [node] \n",
      "  0.01s DETAIL: keep annox: F.etcbc4_sft_chapter [node] \n",
      "  0.01s DETAIL: keep annox: F.etcbc4_sft_verse [node] \n",
      "  0.01s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  0.01s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  0.01s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  0.02s DETAIL: clear main: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  0.02s DETAIL: clear main: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  0.02s DETAIL: clear main: F.etcbc4_ph_phono [node] \n",
      "  0.02s DETAIL: clear main: F.etcbc4_ph_phono_sep [node] \n",
      "  0.02s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_ft_lex_utf8 [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_ph_phono [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_ph_phono_sep [node] \n",
      "  0.02s DETAIL: clear annox: F.etcbc4_sft_book [node] \n",
      "  0.02s DETAIL: load main: F.etcbc4_ft_g_word [node] \n",
      "  0.44s DETAIL: load main: F.etcbc4_ft_language [node] \n",
      "  0.66s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  0.88s DETAIL: load main: F.etcbc4_ft_ls [node] \n",
      "  1.16s DETAIL: load main: F.etcbc4_ft_pdp [node] \n",
      "  1.53s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  1.90s DETAIL: load main: F.etcbc4_lex_gloss [node] \n",
      "  1.90s DETAIL: load main: F.etcbc4_lex_nametype [node] \n",
      "  1.90s DETAIL: load main: F.etcbc4_ft_mother [e] \n",
      "  1.99s DETAIL: load main: C.etcbc4_ft_mother -> \n",
      "  2.18s DETAIL: load main: C.etcbc4_ft_mother <- \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_g_word [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_language [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_lex [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_ls [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_pdp [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_ft_sp [node] \n",
      "  2.30s DETAIL: load annox: F.etcbc4_lex_gloss [node] \n",
      "  2.66s DETAIL: load annox: F.etcbc4_lex_nametype [node] \n",
      "  2.89s DETAIL: load annox: F.etcbc4_ft_mother [e] \n",
      "  2.89s DETAIL: load annox: C.etcbc4_ft_mother -> \n",
      "  2.89s DETAIL: load annox: C.etcbc4_ft_mother <- \n",
      "  2.89s INFO: LOADING PREPARED data: please wait ... \n",
      "  2.89s DETAIL: keep prep: G.node_sort\n",
      "  2.89s DETAIL: keep prep: G.node_sort_inv\n",
      "  2.90s DETAIL: keep prep: L.node_up\n",
      "  2.90s DETAIL: keep prep: L.node_down\n",
      "  2.90s DETAIL: keep prep: V.verses\n",
      "  2.90s DETAIL: keep prep: V.books_la\n",
      "  2.90s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  5.54s INFO: LOADED PREPARED data\n",
      "  5.54s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK workshop AT 2016-03-15T10-12-53\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'lexicon', 'workshop', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        otype\n",
    "        lex g_word\n",
    "        sp pdp nametype ls gloss language\n",
    "        chapter verse\n",
    "    ''','mother'),\n",
    "    \"prepare\": prepare,\n",
    "    \"primary\": False,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 ETCBC additions\n",
    "\n",
    "The `laf` modules know nothing about Hebrew data, nor about ETCBC data features.\n",
    "\n",
    "The `etcbc` modules bring in specific knowledge about how the ETCBC data has been modeled in LAF. It knows\n",
    "* it knows that *sentences* contain *clauses* contain *phrases*\n",
    "* it can order all nodes in a logical order\n",
    "* it can find parts and wholes\n",
    "* it can print text content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tasks\n",
    "\n",
    "## 3.1 Exploration\n",
    "\n",
    "### 3.1.1 Show the first 20 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1367497 book\n",
      "1367536 chapter\n",
      "1413645 verse\n",
      "1125793 sentence\n",
      "1189379 sentence_atom\n",
      "426568 clause\n",
      "514579 clause_atom\n",
      "1368465 half_verse\n",
      "605133 phrase\n",
      "858294 phrase_atom\n",
      "0 word\n",
      "1 word\n",
      "605134 phrase\n",
      "858295 phrase_atom\n",
      "2 word\n",
      "605135 phrase\n",
      "858296 phrase_atom\n",
      "3 word\n",
      "1368466 half_verse\n",
      "605136 phrase\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for n in NN():\n",
    "    print('{} {}'.format(n, F.otype.v(n)))\n",
    "    i += 1\n",
    "    if i >= 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Count all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7.19s Counting\n",
      "  7.47s Done. 1436858 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1436858\n"
     ]
    }
   ],
   "source": [
    "msg('Counting')\n",
    "\n",
    "i = 0\n",
    "for n in NN(): i += 1\n",
    "print(i)\n",
    "\n",
    "msg('Done. {} nodes'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Count the nodes per object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    11s Counting per object type\n",
      "    12s Done. 12 distinct object types\n"
     ]
    }
   ],
   "source": [
    "msg('Counting per object type')\n",
    "\n",
    "counts = collections.Counter()\n",
    "for n in NN(): counts[F.otype.v(n)] += 1\n",
    "\n",
    "msg('Done. {} distinct object types'.format(len(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how many nodes per object type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence has 63586 nodes\n",
      "clause has 88011 nodes\n",
      "subphrase has 113764 nodes\n",
      "verse has 23213 nodes\n",
      "half_verse has 45180 nodes\n",
      "phrase_atom has 267499 nodes\n",
      "clause_atom has 90554 nodes\n",
      "sentence_atom has 64354 nodes\n",
      "book has 39 nodes\n",
      "phrase has 253161 nodes\n",
      "word has 426568 nodes\n",
      "chapter has 929 nodes\n"
     ]
    }
   ],
   "source": [
    "for tp in counts: print('{} has {} nodes'.format(tp, counts[tp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now more pretty, sorted by most numerous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                :  426568 nodes\n",
      "phrase_atom         :  267499 nodes\n",
      "phrase              :  253161 nodes\n",
      "subphrase           :  113764 nodes\n",
      "clause_atom         :   90554 nodes\n",
      "clause              :   88011 nodes\n",
      "sentence_atom       :   64354 nodes\n",
      "sentence            :   63586 nodes\n",
      "half_verse          :   45180 nodes\n",
      "verse               :   23213 nodes\n",
      "chapter             :     929 nodes\n",
      "book                :      39 nodes\n"
     ]
    }
   ],
   "source": [
    "for (tp, n) in sorted(counts.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print('{:<20}: {:>7} nodes'.format(tp, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Find a passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413645\n"
     ]
    }
   ],
   "source": [
    "# for convenience, we use swahili bible book names\n",
    "my_node = T.node_of('Mwanzo', 1, 1, lang='sw')\n",
    "\n",
    "print(my_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, here are the avalaible languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am': ('amharic', 'ኣማርኛ'),\n",
       " 'ar': ('arabic', 'العَرَبِية'),\n",
       " 'bn': ('bengali', 'বাংলা'),\n",
       " 'da': ('danish', 'Dansk'),\n",
       " 'de': ('german', 'Deutsch'),\n",
       " 'el': ('greek', 'Ελληνικά'),\n",
       " 'en': ('english', 'English'),\n",
       " 'es': ('spanish', 'Español'),\n",
       " 'fa': ('farsi', 'فارسی'),\n",
       " 'fr': ('french', 'Français'),\n",
       " 'he': ('hebrew', 'עברית'),\n",
       " 'hi': ('hindi', 'हिन्दी'),\n",
       " 'id': ('indonesian', 'Bahasa Indonesia'),\n",
       " 'ja': ('japanese', '日本語'),\n",
       " 'ko': ('korean', '한국어'),\n",
       " 'la': ('latin', 'Latina'),\n",
       " 'nl': ('dutch', 'Nederlands'),\n",
       " 'pa': ('punjabi', 'ਪੰਜਾਬੀ'),\n",
       " 'pt': ('portuguese', 'Português'),\n",
       " 'ru': ('russian', 'Русский'),\n",
       " 'sw': ('swahili', 'Kiswahili'),\n",
       " 'syc': ('syriac', 'ܠܫܢܐ ܣܘܪܝܝܐ'),\n",
       " 'tr': ('turkish', 'Türkçe'),\n",
       " 'ur': ('urdu', 'اُردُو'),\n",
       " 'yo': ('yoruba', 'èdè Yorùbá'),\n",
       " 'zh': ('chinese', '中文')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the word nodes of that passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "my_words = L.d('word', my_node)\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(T.words(my_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can get it in other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בראשית ברא אלהימ את השמימ ואת הארצ׃\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(T.words(my_words, fmt='hc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get it in all available formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hp',\n",
       "              ('hebrew primary',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('ha',\n",
       "              ('hebrew accent',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('hv',\n",
       "              ('hebrew vowel',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('hc',\n",
       "              ('hebrew cons',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('ea',\n",
       "              ('trans accent',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('ev',\n",
       "              ('trans vowel',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('ec',\n",
       "              ('trans cons',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('pf',\n",
       "              ('phono full',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>)),\n",
       "             ('ps',\n",
       "              ('phono simple',\n",
       "               <function etcbc.text.Text.__init__.<locals>.<lambda>>))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.formats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\n",
      "\n",
      "בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\n",
      "\n",
      "בְּרֵאשִׁית בָּרָא אֱלֹהִים אֵת הַשָּׁמַיִם וְאֵת הָאָרֶץ׃\n",
      "\n",
      "בראשית ברא אלהימ את השמימ ואת הארצ׃\n",
      "\n",
      "B.:R;>CI73JT B.@R@74> >:ELOHI92Jm >;71T HAC.@MA73JIm W:>;71T H@>@35REy00\n",
      "\n",
      "B.:R;>CIJT B.@R@> >:ELOHIJm >;T HAC.@MAJIm W:>;T H@>@REy00\n",
      "\n",
      "BR>#JT BR> >LHJM >T H#MJM W>T H>RY00\n",
      "\n",
      "bᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ .\n",
      "\n",
      "brēšîṯ bårå ʔlōhîm ʔēṯ haššåmayim wʔēṯ håʔåreṣ .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in T.formats(): print('{}'.format(T.words(my_words, fmt=f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With a bit more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hp=hebrew primary בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\n",
      "\n",
      "ha=hebrew accent בְּרֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים אֵ֥ת הַשָּׁמַ֖יִם וְאֵ֥ת הָאָֽרֶץ׃\n",
      "\n",
      "hv=hebrew vowel בְּרֵאשִׁית בָּרָא אֱלֹהִים אֵת הַשָּׁמַיִם וְאֵת הָאָרֶץ׃\n",
      "\n",
      "hc=hebrew cons בראשית ברא אלהימ את השמימ ואת הארצ׃\n",
      "\n",
      "ea=trans accent B.:R;>CI73JT B.@R@74> >:ELOHI92Jm >;71T HAC.@MA73JIm W:>;71T H@>@35REy00\n",
      "\n",
      "ev=trans vowel B.:R;>CIJT B.@R@> >:ELOHIJm >;T HAC.@MAJIm W:>;T H@>@REy00\n",
      "\n",
      "ec=trans cons BR>#JT BR> >LHJM >T H#MJM W>T H>RY00\n",
      "\n",
      "pf=phono full bᵊrēšˌîṯ bārˈā ʔᵉlōhˈîm ʔˌēṯ haššāmˌayim wᵊʔˌēṯ hāʔˈāreṣ .\n",
      "\n",
      "ps=phono simple brēšîṯ bårå ʔlōhîm ʔēṯ haššåmayim wʔēṯ håʔåreṣ .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (f, (desc, method)) in T.formats().items(): print('{}={} {}'.format(f, desc, T.words(my_words, fmt=f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced tasks\n",
    "\n",
    "## 4.1 Adding annotations.\n",
    "\n",
    "See notebook valence/flow_corr.\n",
    "\n",
    "## 4.2 R\n",
    "\n",
    "See notebooks shebanq/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.3 Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 5m 28s collecting relationships\n",
      " 5m 31s Done. 181219\n"
     ]
    }
   ],
   "source": [
    "msg('collecting relationships')\n",
    "mothers = {}\n",
    "for source in NN():\n",
    "    targets = set(C.mother.v(source))\n",
    "    if targets: mothers[source] = targets\n",
    "msg('Done. {}'.format(len(mothers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12m 00s Creating a set of mother nodes\n",
      "12m 00s Done. 143097 mother nodes\n"
     ]
    }
   ],
   "source": [
    "msg('Creating a set of mother nodes')\n",
    "mother_nodes = set()\n",
    "for mset in mothers.values(): mother_nodes |= mset\n",
    "msg('Done. {} mother nodes'.format(len(mother_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mother_count = collections.Counter()\n",
    "for m in mother_nodes:\n",
    "    mother_count[F.otype.v(m)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'clause': 13248,\n",
       "         'clause_atom': 55223,\n",
       "         'phrase': 5807,\n",
       "         'phrase_atom': 10168,\n",
       "         'subphrase': 21118,\n",
       "         'word': 37533})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mother_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mothers(nodeset):\n",
    "    mother_nodes = set()\n",
    "    for n in nodeset:\n",
    "        mother_nodes |= mothers.get(n, set())\n",
    "    return mother_nodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143097"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_mothers(set(NN())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43722"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_mothers(mother_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ancestors(nodeset):\n",
    "    my_mothers = get_mothers(nodeset)\n",
    "    my_ancestors = my_mothers | get_ancestors(my_mothers)\n",
    "    return my_ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def longest_chain(nodeset):\n",
    "    mset = set()\n",
    "    for n in nodeset:\n",
    "        mset |= mothers.get(n, set())\n",
    "    return 0 if not mset else 1 + max({longest_chain({n}) for n in mset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43m 25s Computing longest chain\n",
      "43m 27s Done: 46\n"
     ]
    }
   ],
   "source": [
    "msg('Computing longest chain')\n",
    "lc = longest_chain(set(NN()))\n",
    "msg('Done: {}'.format(lc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lexical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413645"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_node = T.node_of('Mwanzo', 1, 1, lang='sw')\n",
    "focus_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_nodes = L.d('word', focus_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "field_names = '''\n",
    "    g_word\n",
    "    sp\n",
    "    pdp\n",
    "    ls\n",
    "    nametype\n",
    "    language\n",
    "    lex\n",
    "'''.strip().split()\n",
    "row_template = ('{}\\t' * (len(field_names) - 1))+'{}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fh = outfile('first_words.tsv')\n",
    "fh.write(row_template.format(*field_names))\n",
    "for wn in word_nodes:\n",
    "    fh.write(row_template.format(\n",
    "        F.g_word.v(wn),\n",
    "        F.sp.v(wn),\n",
    "        F.pdp.v(wn),\n",
    "        F.item['ls'].v(wn),\n",
    "        F.nametype.v(wn),\n",
    "        F.language.v(wn),\n",
    "    ))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fh = outfile('first_words.tsv')\n",
    "fh.write(row_template.format(*field_names))\n",
    "for wn in word_nodes:\n",
    "    fh.write(row_template.format(*[F.item[feat].v(wn) for feat in field_names]))\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
